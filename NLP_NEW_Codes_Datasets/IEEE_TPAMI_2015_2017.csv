Authors,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Link,Abstract,Author Keywords
"Alterman M., Schechner Y.Y., Swirski Y.","Triangulation in random refractive distortions",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","3", 7448905,"603","616",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012898446&doi=10.1109%2fTPAMI.2016.2551740&partnerID=40&md5=106e7c24884b82d7509a31f091138a6b","Random refraction occurs in turbulence and through a wavy water-air interface. It creates distortion that changes in space, time and with viewpoint. Localizing objects in three dimensions (3D) despite this random distortion is important to some predators and also to submariners avoiding the salient use of periscopes. We take a multiview approach to this task. Refracted distortion statistics induce a probabilistic relation between any pixel location and a line of sight in space. Measurements of an object's random projection from multiple views and times lead to a likelihood function of the object's 3D location. The likelihood leads to estimates of the 3D location and its uncertainty. Furthermore, multiview images acquired simultaneously in a wide stereo baseline have uncorrelated distortions. This helps reduce the acquisition time needed for localization. The method is demonstrated in stereoscopic video sequences, both in a lab and a swimming pool. © 1979-2012 IEEE.","likelihood; probability; stereo; triangulation; Underwater"
"Tao M.W., Srinivasan P.P., Hadap S., Rusinkiewicz S., Malik J., Ramamoorthi R.","Shape estimation from shading, defocus, and correspondence using light-field angular coherence",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","3", 7452621,"546","560",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012867205&doi=10.1109%2fTPAMI.2016.2554121&partnerID=40&md5=593299056effbd12f86c5bb9e10eb53c","Light-field cameras are quickly becoming commodity items, with consumer and industrial applications. They capture many nearby views simultaneously using a single image with a micro-lens array, thereby providing a wealth of cues for depth recovery: Defocus, correspondence, and shading. In particular, apart from conventional image shading, one can refocus images after acquisition, and shift one's viewpoint within the sub-apertures of the main lens, effectively obtaining multiple views. We present a principled algorithm for dense depth estimation that combines defocus and correspondence metrics. We then extend our analysis to the additional cue of shading, using it to refine fine details in the shape. By exploiting an all-in-focus image, in which pixels are expected to exhibit angular coherence, we define an optimization framework that integrates photo consistency, depth consistency, and shading consistency. We show that combining all three sources of information: Defocus, correspondence, and shading, outperforms state-of-the-art light-field depth estimation algorithms in multiple scenarios. © 1979-2012 IEEE.","3D reconstruction; depth cues; Light fields; reflection components separation; shape from shading; specular-free image"
"Wang B., Wang G., Chan K.L., Wang L.","Tracklet association by online target-specific metric learning and coherent dynamics estimation",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","3", 7448472,"589","602",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012905120&doi=10.1109%2fTPAMI.2016.2551245&partnerID=40&md5=56b84033749169269135630660a11773","In this paper, we present a novel method based on online target-specific metric learning and coherent dynamics estimation for tracklet (track fragment) association by network flow optimization in long-term multi-person tracking. Our proposed framework aims to exploit appearance and motion cues to prevent identity switches during tracking and to recover missed detections. Furthermore, target-specific metrics (appearance cue) and motion dynamics (motion cue) are proposed to be learned and estimated online, i.e., during the tracking process. Our approach is effective even when such cues fail to identify or follow the target due to occlusions or object-to-object interactions. We also propose to learn the weights of these two tracking cues to handle the difficult situations, such as severe occlusions and object-to-object interactions effectively. Our method has been validated on several public datasets and the experimental results show that it outperforms several state-of-the-art tracking methods. © 1979-2012 IEEE.","motion dynamics; Multi-object tracking; network flow optimization; target-specific metric learning; tracklet association"
"Elhayek A., De Aguiar E., Jain A., Thompson J., Pishchulin L., Andriluka M., Bregler C., Schiele B., Theobalt C.","MARCOnI - ConvNet-Based MARker-less motion capture in outdoor and indoor scenes",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","3", 7457717,"501","514",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012890112&doi=10.1109%2fTPAMI.2016.2557779&partnerID=40&md5=ad55f37c9a399821cace660000fa49a8","Marker-less motion capture has seen great progress, but most state-of-the-art approaches fail to reliably track articulated human body motion with a very low number of cameras, let alone when applied in outdoor scenes with general background. In this paper, we propose a method for accurate marker-less capture of articulated skeleton motion of several subjects in general scenes, indoors and outdoors, even from input filmed with as few as two cameras. The new algorithm combines the strengths of a discriminative image-based joint detection method with a model-based generative motion tracking algorithm through an unified pose optimization energy. The discriminative part-based pose detection method is implemented using Convolutional Networks (ConvNet) and estimates unary potentials for each joint of a kinematic skeleton model. These unary potentials serve as the basis of a probabilistic extraction of pose constraints for tracking by using weighted sampling from a pose posterior that is guided by the model. In the final energy, we combine these constraints with an appearance-based model-to-image similarity term. Poses can be computed very efficiently using iterative local optimization, since joint detection with a trained ConvNet is fast, and since our formulation yields a combined pose estimation energy with analytic derivatives. In combination, this enables to track full articulated joint angles at state-of-the-art accuracy and temporal stability with a very low number of cameras. Our method is efficient and lends itself to implementation on parallel computing hardware, such as GPUs. We test our method extensively and show its advantages over related work on many indoor and outdoor data sets captured by ourselves, as well as data sets made available to the community by other research labs. The availability of good evaluation data sets is paramount for scientific progress, and many existing test data sets focus on controlled indoor settings, do not feature much variety in the scenes, and often lack a large corpus of data with ground truth annotation. We therefore further contribute with a new extensive test data set called MPI-MARCOnI for indoor and outdoor marker-less motion capture that features 12 scenes of varying complexity and varying camera count, and that features ground truth reference data from different modalities, ranging from manual joint annotations to marker-based motion capture results. Our new method is tested on these data, and the data set will be made available to the community. © 1979-2012 IEEE.","convolutional neural networks; marker-less motion capture; Motion capture; multi-model dataset"
"Biggio B., Fumera G., Marcialis G.L., Roli F.","Statistical meta-analysis of presentation attacks for secure multibiometric systems",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","3", 7458874,"561","575",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012933450&doi=10.1109%2fTPAMI.2016.2558154&partnerID=40&md5=1aebf35b4163c0245b19d26cde394d6e","Prior work has shown that multibiometric systems are vulnerable to presentation attacks, assuming that their matching score distribution is identical to that of genuine users, without fabricating any fake trait. We have recently shown that this assumption is not representative of current fingerprint and face presentation attacks, leading one to overestimate the vulnerability of multibiometric systems, and to design less effective fusion rules. In this paper, we overcome these limitations by proposing a statistical meta-model of face and fingerprint presentation attacks that characterizes a wider family of fake score distributions, including distributions of known and, potentially, unknown attacks. This allows us to perform a thorough security evaluation of multibiometric systems against presentation attacks, quantifying how their vulnerability may vary also under attacks that are different from those considered during design, through an uncertainty analysis. We empirically show that our approach can reliably predict the performance of multibiometric systems even under never-before-seen face and fingerprint presentation attacks, and that the secure fusion rules designed using our approach can exhibit an improved trade-off between the performance in the absence and in the presence of attack. We finally argue that our method can be extended to other biometrics besides faces and fingerprints. © 1979-2012 IEEE.","presentation attacks; secure multibiometric fusion; security evaluation; Statistical meta-analysis; uncertainty analysis"
"Yang J., Yang M.-H.","Top-down visual saliency via joint CRF and dictionary learning",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","3", 7442536,"576","588",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012878987&doi=10.1109%2fTPAMI.2016.2547384&partnerID=40&md5=ea195206d8539eb6b4ae073859d77ed5","Top-down visual saliency is an important module of visual attention. In this work, we propose a novel top-down saliency model that jointly learns a Conditional Random Field (CRF) and a visual dictionary. The proposed model incorporates a layered structure from top to bottom: CRF, sparse coding and image patches. With sparse coding as an intermediate layer, CRF is learned in a feature-adaptive manner; meanwhile with CRF as the output layer, the dictionary is learned under structured supervision. For efficient and effective joint learning, we develop a max-margin approach via a stochastic gradient descent algorithm. Experimental results on the Graz-02 and PASCAL VOC datasets show that our model performs favorably against state-of-the-art top-down saliency methods for target object localization. In addition, the dictionary update significantly improves the performance of our model. We demonstrate the merits of the proposed top-down saliency model by applying it to prioritizing object proposals for detection and predicting human fixations. © 1979-2012 IEEE.","dictionary learning and conditional random fields; fixation prediction; top-down visual saliency; Visual saliency"
"Trigeorgis G., Bousmalis K., Zafeiriou S., Schuller B.W.","A deep matrix factorization method for learning attribute representations",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","3", 7453156,"417","429",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012894180&doi=10.1109%2fTPAMI.2016.2554555&partnerID=40&md5=b2b6d6a722634f7a9cb01c70acdba4ac","Semi-Non-negative Matrix Factorization is a technique that learns a low-dimensional representation of a dataset that lends itself to a clustering interpretation. It is possible that the mapping between this new representation and our original data matrix contains rather complex hierarchical information with implicit lower-level hidden attributes, that classical one level clustering methodologies cannot interpret. In this work we propose a novel model, Deep Semi-NMF, that is able to learn such hidden representations that allow themselves to an interpretation of clustering according to different, unknown attributes of a given dataset. We also present a semi-supervised version of the algorithm, named Deep WSF, that allows the use of (partial) prior information for each of the known attributes of a dataset, that allows the model to be used on datasets with mixed attribute knowledge. Finally, we show that our models are able to learn low-dimensional representations that are better suited for clustering, but also classification, outperforming Semi-Non-negative Matrix Factorization, but also other state-of-the-art methodologies variants. © 1979-2012 IEEE.","deep semi-NMF; Deep WSF; face classification; face clustering; matrix factorization; Semi-NMF; semi-supervised learning; unsupervised feature learning; WSF"
"Martin-Clemente R., Zarzoso V.","On the link between L1-PCA and ICA",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","3", 7497466,"515","528",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012923018&doi=10.1109%2fTPAMI.2016.2557797&partnerID=40&md5=3ab21ca95039550d8a80ccd1d0010477","Principal component analysis (PCA) based on L1-norm maximization is an emerging technique that has drawn growing interest in the signal processing and machine learning research communities, especially due to its robustness to outliers. The present work proves that L1-norm PCA can perform independent component analysis (ICA) under the whitening assumption. However, when the source probability distributions fulfil certain conditions, the L1-norm criterion needs to be minimized rather than maximized, which can be accomplished by simple modifications on existing optimal algorithms for L1-PCA. If the sources have symmetric distributions, we show in addition that L1-PCA is linked to kurtosis optimization. A number of numerical experiments illustrate the theoretical results and analyze the comparative performance of different algorithms for ICA via L1-PCA. Although our analysis is asymptotic in the sample size, this equivalence opens interesting new perspectives for performing ICA using optimal algorithms for L1-PCA with guaranteed global convergence while inheriting the increased robustness to outliers of the L1-norm criterion. © 1979-2012 IEEE.","feature evaluation and selection; Feature extraction or construction; feature representation; independent component analysis; interactive data exploration and discovery; L1-norm; multivariate statistics; principal component analysis"
"Luu K., Savvides M., Bui T.D., Suen C.Y.","Compressed submanifold multifactor analysis",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","3", 7452654,"444","456",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012940910&doi=10.1109%2fTPAMI.2016.2554107&partnerID=40&md5=9ff6a2a37c4b3dcd3cfb0875491d349f","Although widely used, Multilinear PCA (MPCA), one of the leading multilinear analysis methods, still suffers from four major drawbacks. First, it is very sensitive to outliers and noise. Second, it is unable to cope with missing values. Third, it is computationally expensive since MPCA deals with large multi-dimensional datasets. Finally, it is unable to maintain the local geometrical structures due to the averaging process. This paper proposes a novel approach named Compressed Submanifold Multifactor Analysis (CSMA) to solve the four problems mentioned above. Our approach can deal with the problem of missing values and outliers via SVD-L1. The Random Projection method is used to obtain the fast low-rank approximation of a given multifactor dataset. In addition, it is able to preserve the geometry of the original data. Our CSMA method can be used efficiently for multiple purposes, e.g., noise and outlier removal, estimation of missing values, biometric applications. We show that CSMA method can achieve good results and is very efficient in the inpainting problem as compared to [1] , [2]. Our method also achieves higher face recognition rates compared to LRTC, SPMA, MPCA and some other methods, i.e., PCA, LDA and LPP, on three challenging face databases, i.e., CMU-MPIE, CMU-PIE and Extended YALE-B. © 1979-2012 IEEE.","compressed sensing; multifactor analysis; random projection; Tensor analysis; ℓ-norm optimization"
"Mitra A., Biswas S., Bhattacharyya C.","Bayesian modeling of temporal coherence in videos for entity discovery and summarization",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","3", 7457669,"430","443",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012863535&doi=10.1109%2fTPAMI.2016.2557785&partnerID=40&md5=18b831589c30e371ad06f29faa66fa67","A video is understood by users in terms of entities present in it. Entity Discovery is the task of building appearance model for each entity (e.g., a person), and finding all its occurrences in the video. We represent a video as a sequence of tracklets, each spanning 10-20 frames, and associated with one entity. We pose Entity Discovery as tracklet clustering, and approach it by leveraging Temporal Coherence (TC): The property that temporally neighboring tracklets are likely to be associated with the same entity. Our major contributions are the first Bayesian nonparametric models for TC at tracklet-level. We extend Chinese Restaurant Process (CRP) to TC-CRP, and further to Temporally Coherent Chinese Restaurant Franchise (TC-CRF) to jointly model entities and temporal segments using mixture components and sparse distributions. For discovering persons in TV serial videos without meta-data like scripts, these methods show considerable improvement over state-of-the-art approaches to tracklet clustering in terms of clustering accuracy, cluster purity and entity coverage. The proposed methods can perform online tracklet clustering on streaming videos unlike existing approaches, and can automatically reject false tracklets. Finally we discuss entity-driven video summarization- where temporal segments of the video are selected based on the discovered entities, to create a semantically meaningful summary. © 1979-2012 IEEE.","Bayesian nonparametrics; Chinese restaurant process; entity discovery; entity-driven video summarization; temporal coherence; temporal segmentation; tracklet clustering"
"Lu Z., Fu Z., Xiang T., Han P., Wang L., Gao X.","Learning from weak and noisy labels for semantic segmentation",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","3", 7450177,"486","500",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012895343&doi=10.1109%2fTPAMI.2016.2552172&partnerID=40&md5=a6969200cd6422992f9c9073754bde2d","A weakly supervised semantic segmentation (WSSS) method aims to learn a segmentation model from weak (image-level) as opposed to strong (pixel-level) labels. By avoiding the tedious pixel-level annotation process, it can exploit the unlimited supply of user-tagged images from media-sharing sites such as Flickr for large scale applications. However, these 'free' tags/labels are often noisy and few existing works address the problem of learning with both weak and noisy labels. In this work, we cast the WSSS problem into a label noise reduction problem. Specifically, after segmenting each image into a set of superpixels, the weak and potentially noisy image-level labels are propagated to the superpixel level resulting in highly noisy labels; the key to semantic segmentation is thus to identify and correct the superpixel noisy labels. To this end, a novel L1-optimisation based sparse learning model is formulated to directly and explicitly detect noisy labels. To solve the L1-optimisation problem, we further develop an efficient learning algorithm by introducing an intermediate labelling variable. Extensive experiments on three benchmark datasets show that our method yields state-of-the-art results given noise-free labels, whilst significantly outperforming the existing methods when the weak labels are also noisy. © 1979-2012 IEEE.","label noise reduction; Semantic segmentation; sparse learning; weakly supervised learning"
"Takigawa I., Mamitsuka H.","Generalized sparse learning of linear models over the complete subgraph feature set",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","3", 7469410,"617","624",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012922523&doi=10.1109%2fTPAMI.2016.2567399&partnerID=40&md5=63e5b86f774ccd182182e20b6ea158b2","Supervised learning over graphs is an intrinsically difficult problem: Simultaneous learning of relevant features from the complete subgraph feature set, in which enumerating all subgraph features occurring in given graphs is practically intractable due to combinatorial explosion. We show that 1) existing graph supervised learning studies, such as Adaboost, LPBoost, and LARS/LASSO, can be viewed as variations of a branch-and-bound algorithm with simple bounds, which we call Morishita-Kudo bounds; 2) We present a direct sparse optimization algorithm for generalized problems with arbitrary twice-differentiable loss functions, to which Morishita-Kudo bounds cannot be directly applied; 3) We experimentally showed that i) our direct optimization method improves the convergence rate and stability, and ii) L1-penalized logistic regression (L1-LogReg) by our method identifies a smaller subgraph set, keeping the competitive performance, iii) the learned subgraphs by L1-LogReg are more size-balanced than competing methods, which are biased to small-sized subgraphs. © 1979-2012 IEEE.","block coordinate gradient descent; graph mining; simultaneous feature learning; sparsity-inducing regularization; Supervised learning for graphs"
"Xu Y., Carlinet E., Geraud T., Najman L.","Hierarchical segmentation using tree-based shape spaces",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","3", 7452658,"457","469",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012862654&doi=10.1109%2fTPAMI.2016.2554550&partnerID=40&md5=5613428ebf1098b36dc0fea69356b628","Current trends in image segmentation are to compute a hierarchy of image segmentations from fine to coarse. A classical approach to obtain a single meaningful image partition from a given hierarchy is to cut it in an optimal way, following the seminal approach of the scale-set theory. While interesting in many cases, the resulting segmentation, being a non-horizontal cut, is limited by the structure of the hierarchy. In this paper, we propose a novel approach that acts by transforming an input hierarchy into a new saliency map. It relies on the notion of shape space: A graph representation of a set of regions extracted from the image. Each region is characterized with an attribute describing it. We weigh the boundaries of a subset of meaningful regions (local minima) in the shape space by extinction values based on the attribute. This extinction-based saliency map represents a new hierarchy of segmentations highlighting regions having some specific characteristics. Each threshold of this map represents a segmentation which is generally different from any cut of the original hierarchy. This new approach thus enlarges the set of possible partition results that can be extracted from a given hierarchy. Qualitative and quantitative illustrations demonstrate the usefulness of the proposed method. © 1979-2012 IEEE.","binary partition tree; Graph; hierarchical segmentation; hierarchy; image segmentation; minimum spanning tree; object spotting; saliency map; shape space; tree of shapes; α-tree"
"Chu W.-S., De La Torre F., Cohn J.F.","Selective transfer machine for personalized facial expression analysis",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","3", 7442563,"529","545",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012888646&doi=10.1109%2fTPAMI.2016.2547397&partnerID=40&md5=d829a2b0ef08171c5c2d21865f4fab68","Automatic facial action unit (AU) and expression detection from videos is a long-standing problem. The problem is challenging in part because classifiers must generalize to previously unknown subjects that differ markedly in behavior and facial morphology (e.g., heavy versus delicate brows, smooth versus deeply etched wrinkles) from those on which the classifiers are trained. While some progress has been achieved through improvements in choices of features and classifiers, the challenge occasioned by individual differences among people remains. Person-specific classifiers would be a possible solution but for a paucity of training data. Sufficient training data for person-specific classifiers typically is unavailable. This paper addresses the problem of how to personalize a generic classifier without additional labels from the test subject. We propose a transductive learning method, which we refer to as a Selective Transfer Machine (STM), to personalize a generic classifier by attenuating person-specific mismatches. STM achieves this effect by simultaneously learning a classifier and re-weighting the training samples that are most relevant to the test subject. We compared STM to both generic classifiers and cross-domain learning methods on four benchmarks: CK+ [44] , GEMEP-FERA [67] , RU-FACS [4] and GFT [57]. STM outperformed generic classifiers in all. © 1979-2012 IEEE.","domain adaptation; Facial expression analysis; personalization; support vector machine (SVM); transfer learning"
"Wang P., Shen C., Van Den Hengel A., Torr P.H.S.","Large-scale binary quadratic optimization using semidefinite relaxation and applications",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","3", 7431988,"470","485",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012916533&doi=10.1109%2fTPAMI.2016.2541146&partnerID=40&md5=db4c77ed29f9a5c542ac647d13643566","In computer vision, many problems can be formulated as binary quadratic programs (BQPs), which are in general NP hard. Finding a solution when the problem is of large size to be of practical interest typically requires relaxation. Semidefinite relaxation usually yields tight bounds, but its computational complexity is high. In this work, we present a semidefinite programming (SDP) formulation for BQPs, with two desirable properties. First, it produces similar bounds to the standard SDP formulation. Second, compared with the conventional SDP formulation, the proposed SDP formulation leads to a considerably more efficient and scalable dual optimization approach. We then propose two solvers, namely, quasi-Newton and smoothing Newton methods, for the simplified dual problem. Both of them are significantly more efficient than standard interior-point methods. Empirically the smoothing Newton solver is faster than the quasi-Newton solver for dense or medium-sized problems, while the quasi-Newton solver is preferable for large sparse/structured problems. © 1979-2012 IEEE.","Binary quadratic optimization; Markov random fields; semidefinite programming"
"Peng C., Gao X., Wang N., Li J.","Graphical Representation for Heterogeneous Face Recognition",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","2", 7434636,"301","312",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009853908&doi=10.1109%2fTPAMI.2016.2542816&partnerID=40&md5=5da3154535093698111effc4c201c117","Heterogeneous face recognition (HFR) refers to matching face images acquired from different sources (i.e., different sensors or different wavelengths) for identification. HFR plays an important role in both biometrics research and industry. In spite of promising progresses achieved in recent years, HFR is still a challenging problem due to the difficulty to represent two heterogeneous images in a homogeneous manner. Existing HFR methods either represent an image ignoring the spatial information, or rely on a transformation procedure which complicates the recognition task. Considering these problems, we propose a novel graphical representation based HFR method (G-HFR) in this paper. Markov networks are employed to represent heterogeneous image patches separately, which takes the spatial compatibility between neighboring image patches into consideration. A coupled representation similarity metric (CRSM) is designed to measure the similarity between obtained graphical representations. Extensive experiments conducted on multiple HFR scenarios (viewed sketch, forensic sketch, near infrared image, and thermal infrared image) show that the proposed method outperforms state-of-the-art methods. © 1979-2012 IEEE.","forensic sketch; graphical representation; Heterogeneous face recognition; infrared image; thermal image"
"Qu H.-B., Wang J.-Q., Li B., Yu M.","Probabilistic Model for Robust Affine and Non-Rigid Point Set Matching",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","2", 7439870,"371","384",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009892652&doi=10.1109%2fTPAMI.2016.2545659&partnerID=40&md5=cc03b6f39a552230302e7336f8196eaa","In this work, we propose a combinative strategy based on regression and clustering for solving point set matching problems under a Bayesian framework, in which the regression estimates the transformation from the model to the sceneand the clustering establishes the correspondence between two point sets. The point set matching model is illustrated by a hierarchical directed graph, and the matching uncertainties are approximated by a coarse-to-fine variational inference algorithm. Furthermore, two Gaussian mixtures are proposed for the estimation of heteroscedastic noise and spurious outliers, and an isotropic or anisotropic covariance can be imposed on each mixture in terms of the transformed model points. The experimental results show that the proposed approach achieves comparable performance to state-of-the-art matching or registration algorithms in terms of both robustness and accuracy. © 1979-2012 IEEE.","affine transformation; Gaussian mixture model; graphical model; non-rigid registration; Point set matching; robust estimation; variational inference"
"Koniusz P., Yan F., Gosselin P.-H., Mikolajczyk K.","Higher-Order Occurrence Pooling for Bags-of-Words: Visual Concept Detection",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","2", 7439823,"313","326",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009910420&doi=10.1109%2fTPAMI.2016.2545667&partnerID=40&md5=b367b95a1dcfa4bfe7a1dd0eb349f9ea","In object recognition, the Bag-of-Words model assumes: i) extraction of local descriptors from images, ii) embedding the descriptors by a coder to a given visual vocabulary space which results in mid-level features, iii) extracting statistics from mid-level features with a pooling operator that aggregates occurrences of visual words in images into signatures, which we refer to as First-order Occurrence Pooling. This paper investigates higher-order pooling that aggregates over co-occurrences of visual words. We derive Bag-of-Words with Higher-order Occurrence Pooling based on linearisation of Minor Polynomial Kernel, and extend this model to work with various pooling operators. This approach is then effectively used for fusion of various descriptor types. Moreover, we introduce Higher-order Occurrence Pooling performed directly on local image descriptors as well as a novel pooling operator that reduces the correlation in the image signatures. Finally, First-, Second-, and Third-order Occurrence Pooling are evaluated given various coders and pooling operators on several widely used benchmarks. The proposed methods are compared to other approaches such as Fisher Vector Encoding and demonstrate improved results. © 1979-2012 IEEE.","Bag-of-words; co-occurrence; first-order; mid-level features; pooling operator; second-order; sparse coding"
"Zhao R., Oyang W., Wang X.","Person Re-Identification by Saliency Learning",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","2", 7437489,"356","370",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009935490&doi=10.1109%2fTPAMI.2016.2544310&partnerID=40&md5=57a95435c8b8df5c3e46b6907b01007c","Human eyes can recognize person identities based on small salient regions, i.e., person saliency is distinctive and reliable in pedestrian matching across disjoint camera views. However, such valuable information is often hidden when computing similarities of pedestrian images with existing approaches. Inspired by our user study result of human perception on person saliency, we propose a novel perspective for person re-identification based on learning person saliency and matching saliency distribution. The proposed saliency learning and matching framework consists of four steps: (1) To handle misalignment caused by drastic viewpoint change and pose variations, we apply adjacency constrained patch matching to build dense correspondence between image pairs. (2) We propose two alternative methods, i.e., K-Nearest Neighbors and One-class SVM, to estimate a saliency score for each image patch, through which distinctive features stand out without using identity labels in the training procedure. (3) saliency matching is proposed based on patch matching. Matching patches with inconsistent saliency brings penalty, and images of the same identity are recognized by minimizing the saliency matching cost. (4) Furthermore, saliency matching is tightly integrated with patch matching in a unified structural RankSVM learning framework. The effectiveness of our approach is validated on the four public datasets. Our approach outperforms the state-of-the-art person re-identification methods on all these datasets. © 1979-2012 IEEE.","patch matching; Person re-identification; person saliency; video surveillance"
"Bok Y., Jeon H.-G., Kweon I.S.","Geometric Calibration of Micro-Lens-Based Light Field Cameras Using Line Features",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","2", A2,"287","300",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009739454&doi=10.1109%2fTPAMI.2016.2541145&partnerID=40&md5=35bc2f4643564abffa7bb82cbba184b3","We present a novel method for the geometric calibration of micro-lens-based light field cameras. Accurate geometric calibration is the basis of various applications. Instead of using sub-Aperture images, we directly utilize raw images for calibration. We select appropriate regions in raw images and extract line features from micro-lens images in those regions. For the entire process, we formulate a new projection model of a micro-lens-based light field camera, which contains a smaller number of parameters than previous models. The model is transformed into a linear form using line features. We compute the initial solution of both the intrinsic and the extrinsic parameters by a linear computation and refine them via non-linear optimization. Experimental results demonstrate the accuracy of the correspondences between rays and pixels in raw images, as estimated by the proposed method. © 1979-2012 IEEE.","calibration; Computational photography and camera; light field cameras; plenoptic"
"Theologou P., Pratikakis I., Theoharis T.","Unsupervised Spectral Mesh Segmentation Driven by Heterogeneous Graphs",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","2", A5,"397","410",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009752514&doi=10.1109%2fTPAMI.2016.2544311&partnerID=40&md5=b62557f9fc0ae89982080fb527e14500","A fully automatic mesh segmentation scheme using heterogeneous graphs is presented. We introduce a spectral framework where local geometry affinities are coupled with surface patch affinities. A heterogeneous graph is constructed combining two distinct graphs: A weighted graph based on adjacency of patches of an initial over-segmentation, and the weighted dual mesh graph. The partitioning relies on processing each eigenvector of the heterogeneous graph Laplacian individually, taking into account the nodal set and nodal domain theory. Experiments on standard datasets show that the proposed unsupervised approach outperforms the state-of-The-Art unsupervised methodologies and is comparable to the best supervised approaches. © 1979-2012 IEEE.","3D mesh segmentation; Mesh processing; spectral analysis"
"Kviatkovsky I., Gabel M., Rivlin E., Shimshoni I.","On the Equivalence of the LC-KSVD and the D-KSVD Algorithms",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","2", 7439860,"411","416",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009875677&doi=10.1109%2fTPAMI.2016.2545661&partnerID=40&md5=2aac2e23c4261e3ceab8b14fa97a33fe","Sparse and redundant representations, where signals are modeled as a combination of a few atoms from an overcomplete dictionary, is increasingly used in many image processing applications, such as denoising, super resolution, and classification. One common problem is learning a 'good' dictionary for different tasks. In the classification task the aim is to learn a dictionary that also takes training labels into account, and indeed there exist several approaches to this problem. One well-known technique is D-KSVD, which jointly learns a dictionary and a linear classifier using the K-SVD algorithm. LC-KSVD is a recent variation intended to further improve on this idea by adding an explicit label consistency term to the optimization problem, so that different classes are represented by different dictionary atoms. In this work we prove that, under identical initialization conditions, LC-KSVD with uniform atom allocation is in fact a reformulation of D-KSVD: given the regularization parameters of LC-KSVD, we give a closed-form expression for the equivalent D-KSVD regularization parameter, assuming the LC-KSVD's initialization scheme is used. We confirm this by reproducing several of the original LC-KSVD experiments. © 1979-2012 IEEE.","Discriminative dictionary learning; discriminative K-SVD; equivalence proof; label consistent K-SVD"
"Gorelick L., Veksler O., Boykov Y., Nieuwenhuis C.","Convexity Shape Prior for Binary Segmentation",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","2", 7442548,"258","271",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009913089&doi=10.1109%2fTPAMI.2016.2547399&partnerID=40&md5=99e79a95b4ebaf42c3cf9da32ca21e99","Convexity is a known important cue in human vision. We propose shape convexity as a new high-order regularization constraint for binary image segmentation. In the context of discrete optimization, object convexity is represented as a sum of three-clique potentials penalizing any 1-0-1 configuration on all straight lines. We show that these non-submodular potentials can be efficiently optimized using an iterative trust region approach. At each iteration the energy is linearly approximated and globally optimized within a small trust region around the current solution. While the quadratic number of all three-cliques is prohibitively high, we design a dynamic programming technique for evaluating and approximating these cliques in linear time. We also derive a second order approximation model that is more accurate but computationally intensive. We discuss limitations of our local optimization and propose gradual non-submodularization scheme that alleviates some limitations. Our experiments demonstrate general usefulness of the proposed convexity shape prior on synthetic and real image segmentation examples. Unlike standard second-order length regularization, our convexity prior does not have shrinking bias, and is robust to changes in scale and parameter selection. © 1979-2012 IEEE.","convexity shape prior; graph cuts; high-order functionals; Segmentation; trust region"
"Wu Z., Huang Y., Wang L., Wang X., Tan T.","A Comprehensive Study on Cross-View Gait Based Human Identification with Deep CNNs",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","2", 7439821,"209","226",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009863681&doi=10.1109%2fTPAMI.2016.2545669&partnerID=40&md5=70cccdcb36b1ae709d06c84d440243c5","This paper studies an approach to gait based human identification via similarity learning by deep convolutional neural networks (CNNs). With a pretty small group of labeled multi-view human walking videos, we can train deep networks to recognize the most discriminative changes of gait patterns which suggest the change of human identity. To the best of our knowledge, this is the first work based on deep CNNs for gait recognition in the literature. Here, we provide an extensive empirical evaluation in terms of various scenarios, namely, cross-view and cross-walking-condition, with different preprocessing approaches and network architectures. The method is first evaluated on the challenging CASIA-B dataset in terms of cross-view gait recognition. Experimental results show that it outperforms the previous state-of-the-art methods by a significant margin. In particular, our method shows advantages when the cross-view angle is large, i.e., no less than 36 degree. And the average recognition rate can reach 94 percent, much better than the previous best result (less than 65 percent). The method is further evaluated on the OU-ISIR gait dataset to test its generalization ability to larger data. OU-ISIR is currently the largest dataset available in the literature for gait recognition, with 4,007 subjects. On this dataset, the average accuracy of our method under identical view conditions is above 98 percent, and the one for cross-view scenarios is above 91 percent. Finally, the method also performs the best on the USF gait dataset, whose gait sequences are imaged in a real outdoor scene. These results show great potential of this method for practical applications. © 1979-2012 IEEE.","CNN; cross-view; Deep learning; gait; human identification"
"Xiao Y., Liu B., Hao Z.","A Sphere-Description-Based Approach for Multiple-Instance Learning",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","2", A1,"242","257",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009775068&doi=10.1109%2fTPAMI.2016.2539952&partnerID=40&md5=bc3aec254d79aa026316b7a56796eb01","Multiple-instance learning (MIL) is a generalization of supervised learning which addresses the classification of bags. Similar to traditional supervised learning, most of the existing MIL work is proposed based on the assumption that a representative training set is available for a proper learning of the classifier. That is to say, the training data can appropriately describe the distribution of positive and negative data in the testing set. However, this assumption may not be always satisfied. In real-world MIL applications, the negative data in the training set may not sufficiently represent the distribution of negative data in the testing set. Hence, how to learn an appropriate MIL classifier when a representative training set is not available becomes a key challenge for real-world MIL applications. To deal with this problem, we propose a novel Sphere-Description-Based approach for Multiple-Instance Learning (SDB-MIL). SDB-MIL learns an optimal sphere by determining a large margin among the instances, and meanwhile ensuring that each positive bag has at least one instance inside the sphere and all negative bags are outside the sphere. Enclosing at least one instance from each positive bag in the sphere enables a more desirable MIL classifier when the negative data in the training set cannot sufficiently represent the distribution of negative data in the testing set. Substantial experiments on the benchmark and real-world MIL datasets show that SDB-MIL obtains statistically better classification performance than the MIL methods compared. © 1979-2012 IEEE.","classification; Multiple-instance learning"
"Liu T., Tao D., Song M., Maybank S.J.","Algorithm-Dependent Generalization Bounds for Multi-Task Learning",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","2", A4,"227","241",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009732680&doi=10.1109%2fTPAMI.2016.2544314&partnerID=40&md5=3246e4a3ddf9e1e301d3ad7f97e73e8c","Often, tasks are collected for multi-Task learning (MTL) because they share similar feature structures. Based on this observation, in this paper, we present novel algorithm-dependent generalization bounds for MTL by exploiting the notion of algorithmic stability. We focus on the performance of one particular task and the average performance over multiple tasks by analyzing the generalization ability of a common parameter that is shared in MTL. When focusing on one particular task, with the help of a mild assumption on the feature structures, we interpret the function of the other tasks as a regularizer that produces a specific inductive bias. The algorithm for learning the common parameter, as well as the predictor, is thereby uniformly stable with respect to the domain of the particular task and has a generalization bound with a fast convergence rate of order mathcal {O}(1/n), where is the sample size of the particular task. When focusing on the average performance over multiple tasks, we prove that a similar inductive bias exists under certain conditions on the feature structures. Thus, the corresponding algorithm for learning the common parameter is also uniformly stable with respect to the domains of the multiple tasks, and its generalization bound is of the order mathcal {O}(1/T), where T is the number of tasks. These theoretical analyses naturally show that the similarity of feature structures in MTL will lead to specific regularizations for predicting, which enables the learning algorithms to generalize fast and correctly from a few examples. © 1979-2012 IEEE.","generalization; inductive bias; learning theory; learning to learn; Multi-Task learning; regularization; stability"
"Saurer O., Vasseur P., Boutteau R., Demonceaux C., Pollefeys M., Fraundorfer F.","Homography Based Egomotion Estimation with a Common Direction",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","2", 7439820,"327","341",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009885116&doi=10.1109%2fTPAMI.2016.2545663&partnerID=40&md5=413719683df919f417839aa3dc22c493","In this paper, we explore the different minimal solutions for egomotion estimation of a camera based on homography knowing the gravity vector between calibrated images. These solutions depend on the prior knowledge about the reference plane used by the homography. We then demonstrate that the number of matched points can vary from two to three and that a direct closed-form solution or a Gröbner basis based solution can be derived according to this plane. Many experimental results on synthetic and real sequences in indoor and outdoor environments show the efficiency and the robustness of our approach compared to standard methods. © 1979-2012 IEEE.","Computer vision; egomotion estimation; homography estimation; structure-from-motion"
"Chen D., Cao X., Wipf D., Wen F., Sun J.","An Efficient Joint Formulation for Bayesian Face Verification",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","1", 7415949,"32","46",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003905072&doi=10.1109%2fTPAMI.2016.2533383&partnerID=40&md5=0c8923bd079a3539d00d03769c1c4565","This paper revisits the classical Bayesian face recognition algorithm from Baback Moghaddam et al. and proposes enhancements tailored to face verification, the problem of predicting whether or not a pair of facial images share the same identity. Like a variety of face verification algorithms, the original Bayesian face model only considers the appearance difference between two faces rather than the raw images themselves. However, we argue that such a fixed and blind projection may prematurely reduce the separability between classes. Consequently, we model two facial images jointly with an appropriate prior that considers intra-and extra-personal variations over the image pairs. This joint formulation is trained using a principled EM algorithm, while testing involves only efficient closed-formed computations that are suitable for real-time practical deployment. Supporting theoretical analyses investigate computational complexity, scale-invariance properties, and convergence issues. We also detail important relationships with existing algorithms, such as probabilistic linear discriminant analysis and metric learning. Finally, on extensive experimental evaluations, the proposed model is superior to the classical Bayesian face algorithm and many alternative state-of-the-art supervised approaches, achieving the best test accuracy on three challenging datasets, Labeled Face in Wild, Multi-PIE, and YouTube Faces, all with unparalleled computational efficiency. © 2016 IEEE.","Bayesian face recognition; EM algorithm; face verification"
"Hu W., Gao J., Xing J., Zhang C., Maybank S.","Semi-Supervised Tensor-Based Graph Embedding Learning and Its Application to Visual Discriminant Tracking",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","1", 7429797,"172","188",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006077971&doi=10.1109%2fTPAMI.2016.2539944&partnerID=40&md5=9d65720adb60cc1117b99ed3bf55639d","An appearance model adaptable to changes in object appearance is critical in visual object tracking. In this paper, we treat an image patch as a two-order tensor which preserves the original image structure. We design two graphs for characterizing the intrinsic local geometrical structure of the tensor samples of the object and the background. Graph embedding is used to reduce the dimensions of the tensors while preserving the structure of the graphs. Then, a discriminant embedding space is constructed. We prove two propositions for finding the transformation matrices which are used to map the original tensor samples to the tensor-based graph embedding space. In order to encode more discriminant information in the embedding space, we propose a transfer-learning-based semi-supervised strategy to iteratively adjust the embedding space into which discriminative information obtained from earlier times is transferred. We apply the proposed semi-supervised tensor-based graph embedding learning algorithm to visual tracking. The new tracking algorithm captures an object's appearance characteristics during tracking and uses a particle filter to estimate the optimal object state. Experimental results on the CVPR 2013 benchmark dataset demonstrate the effectiveness of the proposed tracking algorithm. © 2016 IEEE.","Discriminant tracking; graph embedding space; semi-supervised learning; tensor samples"
"Pont-Tuset J., Arbelaez P., Barron J.T., Marques F., Malik J.","Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","1", 7423791,"128","140",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003890183&doi=10.1109%2fTPAMI.2016.2537320&partnerID=40&md5=aba7d7f792b9223ffecc4b1b758f332f","We propose a unified approach for bottom-up hierarchical image segmentation and object proposal generation for recognition, called Multiscale Combinatorial Grouping (MCG). For this purpose, we first develop a fast normalized cuts algorithm. We then propose a high-performance hierarchical segmenter that makes effective use of multiscale information. Finally, we propose a grouping strategy that combines our multiscale regions into highly-accurate object proposals by exploring efficiently their combinatorial space. We also present Single-scale Combinatorial Grouping (SCG), a faster version of MCG that produces competitive proposals in under five seconds per image. We conduct an extensive and comprehensive empirical validation on the BSDS500, SegVOC12, SBD, and COCO datasets, showing that MCG produces state-of-the-art contours, hierarchical regions, and object proposals. © 2016 IEEE.","Image segmentation; Normalized cuts; object proposals"
"Xiong C., Johnson D.M., Corso J.J.","Active Clustering with Model-Based Uncertainty Reduction",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","1", 7429793,"5","17",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006058368&doi=10.1109%2fTPAMI.2016.2539965&partnerID=40&md5=e6d347cebd93b0df7dbb6ee748800cd5","Semi-supervised clustering seeks to augment traditional clustering methods by incorporating side information provided via human expertise in order to increase the semantic meaningfulness of the resulting clusters. However, most current methods are passive in the sense that the side information is provided beforehand and selected randomly. This may require a large number of constraints, some of which could be redundant, unnecessary, or even detrimental to the clustering results. Thus in order to scale such semi-supervised algorithms to larger problems it is desirable to pursue an active clustering method - i.e., an algorithm that maximizes the effectiveness of the available human labor by only requesting human input where it will have the greatest impact. Here, we propose a novel online framework for active semi-supervised spectral clustering that selects pairwise constraints as clustering proceeds, based on the principle of uncertainty reduction. Using a first-order Taylor expansion, we decompose the expected uncertainty reduction problem into a gradient and a step-scale, computed via an application of matrix perturbation theory and cluster-assignment entropy, respectively. The resulting model is used to estimate the uncertainty reduction potential of each sample in the dataset. We then present the human user with pairwise queries with respect to only the best candidate sample. We evaluate our method using three different image datasets (faces, leaves and dogs), a set of common UCI machine learning datasets and a gene dataset. The results validate our decomposition formulation and show that our method is consistently superior to existing state-of-the-art techniques, as well as being robust to noise and to unknown numbers of clusters. © 2016 IEEE.","Active clustering; image clustering; semi-supervised clustering; uncertainty reduction"
"Chen D., Yuan Z., Hua G., Wang J., Zheng N.","Multi-Timescale Collaborative Tracking",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","1", 7429794,"141","155",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005950044&doi=10.1109%2fTPAMI.2016.2539956&partnerID=40&md5=feff3489a64b7f6cdbb1944878e64f44","We present the multi-timescale collaborative tracker for single object tracking. The tracker simultaneously utilizes different types of 'forces', namely attraction, repulsion and support, to take advantage of their complementary strengths. We model the three forces via three components that are learned from the sample sets with different timescales. The long-term descriptive component attracts the target sample, while the medium-term discriminative component repulses the target from the background. They are collaborated in the appearance model to benefit each other. The short-term regressive component combines the votes of the auxiliary samples to predict the target's position, forming the context-aware motion model. The appearance model and the motion model collaboratively determine the target state, and the optimal state is estimated by a novel coarse-to-fine search strategy. We have conducted an extensive set of experiments on the standard 50 video benchmark. The results confirm the effectiveness of each component and their collaboration, outperforming current state-of-the-art methods. © 2016 IEEE.","collaboration; context; descriptive; discriminative; multi-timescale; regressive; Visual tracking"
"Amit Kumar K.C., Jacques L., De Vleeschouwer C.","Discriminative and Efficient Label Propagation on Complementary Graphs for Multi-Object Tracking",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","1", 7415947,"61","64",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85004191368&doi=10.1109%2fTPAMI.2016.2533391&partnerID=40&md5=9adcb36dfc7b8fa63432fdc2db4029eb","Given a set of detections, detected at each time instant independently, we investigate how to associate them across time. This is done by propagating labels on a set of graphs, each graph capturing how either the spatio-temporal or the appearance cues promote the assignment of identical or distinct labels to a pair of detections. The graph construction is motivated by a locally linear embedding of the detection features. Interestingly, the neighborhood of a node in appearance graph is defined to include all the nodes for which the appearance feature is available (even if they are temporally distant). This gives our framework the uncommon ability to exploit the appearance features that are available only sporadically. Once the graphs have been defined, multi-object tracking is formulated as the problem of finding a label assignment that is consistent with the constraints captured each graph, which results into a difference of convex (DC) program. We propose to decompose the global objective function into node-wise sub-problems. This not only allows a computationally efficient solution, but also supports an incremental and scalable construction of the graph, thereby making the framework applicable to large graphs and practical tracking scenarios. Moreover, it opens the possibility of parallel implementation. © 2016 IEEE.","Computer vision; Graph labeling; label propagation; multi-object tracking; sporadic features"
"Cinbis R.G., Verbeek J., Schmid C.","Weakly Supervised Object Localization with Multi-Fold Multiple Instance Learning",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","1", 7420739,"189","203",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003782026&doi=10.1109%2fTPAMI.2016.2535231&partnerID=40&md5=b3c3daca7055d7998b248bf48a74a8cb","Object category localization is a challenging problem in computer vision. Standard supervised training requires bounding box annotations of object instances. This time-consuming annotation process is sidestepped in weakly supervised learning. In this case, the supervised information is restricted to binary labels that indicate the absence/presence of object instances in the image, without their locations. We follow a multiple-instance learning approach that iteratively trains the detector and infers the object locations in the positive training images. Our main contribution is a multi-fold multiple instance learning procedure, which prevents training from prematurely locking onto erroneous object locations. This procedure is particularly important when using high-dimensional representations, such as Fisher vectors and convolutional neural network features. We also propose a window refinement method, which improves the localization accuracy by incorporating an objectness prior. We present a detailed experimental evaluation using the PASCAL VOC 2007 dataset, which verifies the effectiveness of our approach. © 2016 IEEE.","object detection; Weakly supervised learning"
"Yang J., Luo L., Qian J., Tai Y., Zhang F., Xu Y.","Nuclear Norm Based Matrix Regression with Applications to Face Recognition with Occlusion and Illumination Changes",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","1", 7420697,"156","171",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003781883&doi=10.1109%2fTPAMI.2016.2535218&partnerID=40&md5=272425adf946f8c0b23b842c52b59549","Recently, regression analysis has become a popular tool for face recognition. Most existing regression methods use the one-dimensional, pixel-based error model, which characterizes the representation error individually, pixel by pixel, and thus neglects the two-dimensional structure of the error image. We observe that occlusion and illumination changes generally lead, approximately, to a low-rank error image. In order to make use of this low-rank structural information, this paper presents a two-dimensional image-matrix-based error model, namely, nuclear norm based matrix regression (NMR), for face representation and classification. NMR uses the minimal nuclear norm of representation error image as a criterion, and the alternating direction method of multipliers (ADMM) to calculate the regression coefficients. We further develop a fast ADMM algorithm to solve the approximate NMR model and show it has a quadratic rate of convergence. We experiment using five popular face image databases: the Extended Yale B, AR, EURECOM, Multi-PIE and FRGC. Experimental results demonstrate the performance advantage of NMR over the state-of-the-art regression-based methods for face recognition in the presence of occlusion and illumination variations. © 2016 IEEE.","alternating direction method of multipliers (ADMM); Face recognition; Nuclear norm; robust regression; sparse representation"
"Sharma G., Jurie F., Schmid C.","Expanded Parts Model for Semantic Description of Humans in Still Images",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","1", 7423799,"87","101",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006065015&doi=10.1109%2fTPAMI.2016.2537325&partnerID=40&md5=4f3c37e1a25c4366e05549c5aed39d0f","We introduce an Expanded Parts Model (EPM) for recognizing human attributes (e.g., young, short hair, wearing suits) and actions (e.g., running, jumping) in still images. An EPM is a collection of part templates which are learnt discriminatively to explain specific scale-space regions in the images (in human centric coordinates). This is in contrast to current models which consist of a relatively few (i.e., a mixture of) 'average' templates. EPM uses only a subset of the parts to score an image and scores the image sparsely in space, i.e., it ignores redundant and random background in an image. To learn our model, we propose an algorithm which automatically mines parts and learns corresponding discriminative templates together with their respective locations from a large number of candidate parts. We validate our method on three recent challenging datasets of human attributes and actions. We obtain convincing qualitative and state-of-the-art quantitative results on the three datasets. © 2016 IEEE.","actions; attributes; Human analysis; image classification; semantic description"
"Premachandran V., Tarlow D., Yuille A.L., Batra D.","Empirical Minimum Bayes Risk Prediction",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","1", 7425216,"75","86",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005992358&doi=10.1109%2fTPAMI.2016.2537807&partnerID=40&md5=685dce74f5f365b51780fd91f8707e72","When building vision systems that predict structured objects such as image segmentations or human poses, a crucial concern is performance under task-specific evaluation measures (e.g., Jaccard Index or Average Precision). An ongoing research challenge is to optimize predictions so as to maximize performance on such complex measures. In this work, we present a simple meta-algorithm that is surprisingly effective - Empirical Min Bayes Risk. EMBR takes as input a pre-trained model that would normally be the final product and learns three additional parameters so as to optimize performance on the complex instance-level high-order task-specific measure. We demonstrate EMBR in several domains, taking existing state-of-the-art algorithms and improving performance up to 8 percent, simply by learning three extra parameters. Our code is publicly available and the results presented in this paper can be replicated from our code-release. © 2016 IEEE.","Diverse predictions; DivMBest; human pose estimation; image segmentation; object segmentation"
"Liu A.-A., Su Y.-T., Nie W.-Z., Kankanhalli M.","Hierarchical Clustering Multi-Task Learning for Joint Human Action Grouping and Recognition",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","1", 7423818,"102","114",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005949742&doi=10.1109%2fTPAMI.2016.2537337&partnerID=40&md5=b5c8ca79c1a4b9636250017175e0402c","This paper proposes a hierarchical clustering multi-task learning (HC-MTL) method for joint human action grouping and recognition. Specifically, we formulate the objective function into the group-wise least square loss regularized by low rank and sparsity with respect to two latent variables, model parameters and grouping information, for joint optimization. To handle this non-convex optimization, we decompose it into two sub-tasks, multi-task learning and task relatedness discovery. First, we convert this non-convex objective function into the convex formulation by fixing the latent grouping information. This new objective function focuses on multi-task learning by strengthening the shared-action relationship and action-specific feature learning. Second, we leverage the learned model parameters for the task relatedness measure and clustering. In this way, HC-MTL can attain both optimal action models and group discovery by alternating iteratively. The proposed method is validated on three kinds of challenging datasets, including six realistic action datasets (Hollywood2, YouTube, UCF Sports, UCF50, HMDB51 & UCF101), two constrained datasets (KTH & TJU), and two multi-view datasets (MV-TJU & IXMAS). The extensive experimental results show that: 1) HC-MTL can produce competing performances to the state of the arts for action recognition and grouping; 2) HC-MTL can overcome the difficulty in heuristic action grouping simply based on human knowledge; 3) HC-MTL can avoid the possible inconsistency between the subjective action grouping depending on human knowledge and objective action grouping based on the feature subspace distributions of multiple actions. Comparison with the popular clustered multi-task learning further reveals that the discovered latent relatedness by HC-MTL aids inducing the group-wise multi-task learning and boosts the performance. To the best of our knowledge, ours is the first work that breaks the assumption that all actions are either independent for individual learning or correlated for joint modeling and proposes HC-MTL for automated, joint action grouping and modeling. © 2016 IEEE.","Action recognition; multi-task learning; task grouping; task relatedness measure"
"Liu G., Liu Q., Li P.","Blessing of Dimensionality: Recovering Mixture Data via Dictionary Pursuit",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","1", 7429796,"47","60",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006054740&doi=10.1109%2fTPAMI.2016.2539946&partnerID=40&md5=cc0b156ac7cba2c54bffe2039a030888","This paper studies the problem of recovering the authentic samples that lie on a union of multiple subspaces from their corrupted observations. Due to the high-dimensional and massive nature of today's data-driven community, it is arguable that the target matrix (i.e., authentic sample matrix) to recover is often low-rank. In this case, the recently established Robust Principal Component Analysis (RPCA) method already provides us a convenient way to solve the problem of recovering mixture data. However, in general, RPCA is not good enough because the incoherent condition assumed by RPCA is not so consistent with the mixture structure of multiple subspaces. Namely, when the subspace number grows, the row-coherence of data keeps heightening and, accordingly, RPCA degrades. To overcome the challenges arising from mixture data, we suggest to consider LRR in this paper. We elucidate that LRR can well handle mixture data, as long as its dictionary is configured appropriately. More precisely, we mathematically prove that LRR can weaken the dependence on the row-coherence, provided that the dictionary is well-conditioned and has a rank of not too high. In particular, if the dictionary itself is sufficiently low-rank, then the dependence on the row-coherence can be completely removed. These provide some elementary principles for dictionary learning and naturally lead to a practical algorithm for recovering mixture data. Our experiments on randomly generated matrices and real motion sequences show promising results. © 2016 IEEE.","dictionary learning; incoherent condition; low-rank representation; matrix factorization; subspace clustering"
"Liang X., Xu C., Shen X., Yang J., Tang J., Lin L., Yan S.","Human Parsing with Contextualized Convolutional Neural Network",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","1", 7423822,"115","127",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005950673&doi=10.1109%2fTPAMI.2016.2537339&partnerID=40&md5=36733600a9f0bc807bed2cfb39870d6d","In this work, we address the human parsing task with a novel Contextualized Convolutional Neural Network (Co-CNN) architecture, which well integrates the cross-layer context, global image-level context, semantic edge context, within-super-pixel context and cross-super-pixel neighborhood context into a unified network. Given an input human image, Co-CNN produces the pixel-wise categorization in an end-to-end way. First, the cross-layer context is captured by our basic local-to-global-to-local structure, which hierarchically combines the global semantic information and the local fine details across different convolutional layers. Second, the global image-level label prediction is used as an auxiliary objective in the intermediate layer of the Co-CNN, and its outputs are further used for guiding the feature learning in subsequent convolutional layers to leverage the global image-level context. Third, semantic edge context is further incorporated into Co-CNN, where the high-level semantic boundaries are leveraged to guide pixel-wise labeling. Finally, to further utilize the local super-pixel contexts, the within-super-pixel smoothing and cross-super-pixel neighbourhood voting are formulated as natural sub-components of the Co-CNN to achieve the local label consistency in both training and testing process. Comprehensive evaluations on two public datasets well demonstrate the significant superiority of our Co-CNN over other state-of-the-arts for human parsing. In particular, the F-1 score on the large dataset [1] reaches 81.72 percent by Co-CNN, significantly higher than 62.81 percent and 64.38 percent by the state-of-the-art algorithms, M-CNN [2] and ATR [1], respectively. By utilizing our newly collected large dataset for training, our Co-CNN can achieve 85.36 percent in F-1 score. © 2016 IEEE.","context modeling; fully convolutional network; Human parsing; semantic labeling"
"Kwon J., Lee K.M.","Adaptive Visual Tracking with Minimum Uncertainty Gap Estimation",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","1", 7423782,"18","31",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003845113&doi=10.1109%2fTPAMI.2016.2537330&partnerID=40&md5=85afa1e9b1b4c1e7061ab1e6c9cdeacd","A novel tracking algorithm is proposed, which robustly tracks a target by finding the state that minimizes the likelihood uncertainty. Likelihood uncertainty is estimated by determining the gap between the lower and upper bounds of likelihood. By minimizing the gap between the two bounds, the proposed method identifies the confident and reliable state of the target. In this study, the state that provides the Minimum Uncertainty Gap (MUG) between likelihood bounds is shown to be more reliable than the state that provides the maximum likelihood only, especially when severe illumination changes, occlusions, and pose variations occur. A rigorous derivation of the lower and upper bounds of the likelihood for the visual tracking problem is provided to address this issue. Additionally, an efficient inference algorithm that uses Interacting Markov Chain Monte Carlo (IMCMC) approach is presented to find the best state that maximizes the average of the lower and upper bounds of likelihood while minimizing the gap between the two bounds. We extend our method to update the target model adaptively. To update the model, the current observation is combined with a previous target model with the adaptive weight, which is calculated according to the goodness of the current observation. The goodness of the observation is measured using the proposed uncertainty gap estimation of likelihood. Experimental results demonstrate that the proposed method robustly tracks the target in realistic videos and outperforms conventional tracking methods. © 2016 IEEE.","Adaptive model update; lower and upper bounds of likelihood; minimum uncertainty gap; Object tracking"
"Leifman G., Shtrom E., Tal A.","Surface Regions of Interest for Viewpoint Selection",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","12", 7393832,"2544","2556",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995642117&doi=10.1109%2fTPAMI.2016.2522437&partnerID=40&md5=12bc10faf9f5f2fc1882a53c2d98c33f","While the detection of the interesting regions in images has been extensively studied, relatively few papers have addressed surfaces. This paper proposes an algorithm for detecting the regions of interest of surfaces. It looks for regions that are distinct both locally and globally and accounts for the distance to the foci of attention. It is also shown how this algorithm can be adopted to saliency detection in point clouds. Many applications can utilize these regions. In this paper we explore one such application-viewpoint selection. The most informative views are those that collectively provide the most descriptive presentation of the surface. We show that our results compete favorably with the state-of-the-art results. © 2016 IEEE.","Point clouds; Saliency detection; surfaces"
"Rahmani H., Mahmood A., Huynh D., Mian A.","Histogram of Oriented Principal Components for Cross-View Action Recognition",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","12", 7415989,"2430","2443",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995584051&doi=10.1109%2fTPAMI.2016.2533389&partnerID=40&md5=fafeefe9ae6d7280615b94d4b7f69653","Existing techniques for 3D action recognition are sensitive to viewpoint variations because they extract features from depth images which are viewpoint dependent. In contrast, we directly process pointclouds for cross-view action recognition from unknown and unseen views. We propose the histogram of oriented principal components (HOPC) descriptor that is robust to noise, viewpoint, scale and action speed variations. At a 3D point, HOPC is computed by projecting the three scaled eigenvectors of the pointcloud within its local spatio-temporal support volume onto the vertices of a regular dodecahedron. HOPC is also used for the detection of spatio-temporal keypoints (STK) in 3D pointcloud sequences so that view-invariant STK descriptors (or Local HOPC descriptors) at these key locations only are used for action recognition. We also propose a global descriptor computed from the normalized spatio-temporal distribution of STKs in 4-D, which we refer to as STK-D. We have evaluated the performance of our proposed descriptors against nine existing techniques on two cross-view and three single-view human action recognition datasets. The experimental results show that our techniques provide significant improvement over state-of-the-art methods. © 2016 IEEE.","pointcloud; Spatio-temporal keypoint; view invariance"
"Venkataraman V., Turaga P.","Shape Distributions of Nonlinear Dynamical Systems for Video-Based Inference",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","12", 7415965,"2531","2543",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995596456&doi=10.1109%2fTPAMI.2016.2533388&partnerID=40&md5=44969909f60adb01ca0ff12471d3a483","This paper presents a shape-theoretic framework for dynamical analysis of nonlinear dynamical systems which appear frequently in several video-based inference tasks. Traditional approaches to dynamical modeling have included linear and nonlinear methods with their respective drawbacks. A novel approach we propose is the use of descriptors of the shape of the dynamical attractor as a feature representation of nature of dynamics. The proposed framework has two main advantages over traditional approaches: a) representation of the dynamical system is derived directly from the observational data, without any inherent assumptions, and b) the proposed features show stability under different time-series lengths where traditional dynamical invariants fail. We illustrate our idea using nonlinear dynamical models such as Lorenz and Rossler systems, where our feature representations (shape distribution) support our hypothesis that the local shape of the reconstructed phase space can be used as a discriminative feature. Our experimental analyses on these models also indicate that the proposed framework show stability for different time-series lengths, which is useful when the available number of samples are small/variable. The specific applications of interest in this paper are: 1) activity recognition using motion capture and RGBD sensors, 2) activity quality assessment for applications in stroke rehabilitation, and 3) dynamical scene classification. We provide experimental validation through action and gesture recognition experiments on motion capture and Kinect datasets. In all these scenarios, we show experimental evidence of the favorable properties of the proposed representation. © 2016 IEEE.","action and gesture recognition; Action modeling; chaos theory; Dynamical scene analysis; largest Lyapunov exponent; movement quality assessment; shape distribution"
"Gebru I.D., Alameda-Pineda X., Forbes F., Horaud R.","EM Algorithms for Weighted-Data Clustering with Application to Audio-Visual Scene Analysis",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","12", 7393841,"2402","2415",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995554241&doi=10.1109%2fTPAMI.2016.2522425&partnerID=40&md5=bc9cda87162f43553e82c422babe789d","Data clustering has received a lot of attention and numerous methods, algorithms and software packages are available. Among these techniques, parametric finite-mixture models play a central role due to their interesting mathematical properties and to the existence of maximum-likelihood estimators based on expectation-maximization (EM). In this paper we propose a new mixture model that associates a weight with each observed point. We introduce the weighted-data Gaussian mixture and we derive two EM algorithms. The first one considers a fixed weight for each observation. The second one treats each weight as a random variable following a gamma distribution. We propose a model selection method based on a minimum message length criterion, provide a weight initialization strategy, and validate the proposed algorithms by comparing them with several state of the art parametric and non-parametric clustering techniques. We also demonstrate the effectiveness and robustness of the proposed clustering technique in the presence of heterogeneous data, namely audio-visual scene analysis. © 2016 IEEE.","audio-visual fusion; expectation-maximization; Finite mixtures; minimum message length; model selection; outlier detection; robust clustering; Speaker localization; weighted-data clustering"
"Turetken E., Benmansour F., Andres B., Glowacki P., Pfister H., Fua P.","Reconstructing Curvilinear Networks Using Path Classifiers and Integer Programming",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","12", 7405348,"2515","2530",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995575880&doi=10.1109%2fTPAMI.2016.2519025&partnerID=40&md5=a1c854a4f282d9d46e67450e4ce947fc","We propose a novel approach to automated delineation of curvilinear structures that form complex and potentially loopy networks. By representing the image data as a graph of potential paths, we first show how to weight these paths using discriminatively-trained classifiers that are both robust and generic enough to be applied to very different imaging modalities. We then present an Integer Programming approach to finding the optimal subset of paths, subject to structural and topological constraints that eliminate implausible solutions. Unlike earlier approaches that assume a tree topology for the networks, ours explicitly models the fact that the networks may contain loops, and can reconstruct both cyclic and acyclic ones. We demonstrate the effectiveness of our approach on a variety of challenging datasets including aerial images of road networks and micrographs of neural arbors, and show that it outperforms state-of-the-art techniques. © 2016 IEEE.","automated reconstruction; Curvilinear networks; curvilinear structures; integer programming; Minimum arborescence; path classification; tubular structures"
"Liu R., Zhong G., Cao J., Lin Z., Shan S., Luo Z.","Learning to Diffuse: A New Perspective to Design PDEs for Visual Analysis",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","12", 7393839,"2457","2471",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995549526&doi=10.1109%2fTPAMI.2016.2522415&partnerID=40&md5=d99cc844ff90263ac213e1bb6bfeefdd","Partial differential equations (PDEs) have been used to formulate image processing for several decades. Generally, a PDE system consists of two components: the governing equation and the boundary condition. In most previous work, both of them are generally designed by people using mathematical skills. However, in real world visual analysis tasks, such predefined and fixed-form PDEs may not be able to describe the complex structure of the visual data. More importantly, it is hard to incorporate the labeling information and the discriminative distribution priors into these PDEs. To address above issues, we propose a new PDE framework, named learning to diffuse (LTD), to adaptively design the governing equation and the boundary condition of a diffusion PDE system for various vision tasks on different types of visual data. To our best knowledge, the problems considered in this paper (i.e., saliency detection and object tracking) have never been addressed by PDE models before. Experimental results on various challenging benchmark databases show the superiority of LTD against existing state-of-the-art methods for all the tested visual analysis tasks. © 2016 IEEE.","Object tracking; PDE governed combinatorial optimization; saliency detection; submodularity; Visual diffusion"
"Proenca H., Neves J.C., Barra S., Marques T., Moreno J.C.","Joint Head Pose/Soft Label Estimation for Human Recognition In-The-Wild",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","12", 7393850,"2444","2456",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995543223&doi=10.1109%2fTPAMI.2016.2522441&partnerID=40&md5=f62e4fbf4e4eeb4bac31153062088a01","Soft biometrics have been emerging to complement other traits and are particularly useful for poor quality data. In this paper, we propose an efficient algorithm to estimate human head poses and to infer soft biometric labels based on the 3D morphology of the human head. Starting by considering a set of pose hypotheses, we use a learning set of head shapes synthesized from anthropometric surveys to derive a set of 3D head centroids that constitutes a metric space. Next, representing queries by sets of 2D head landmarks, we use projective geometry techniques to rank efficiently the joint 3D head centroids/pose hypotheses according to their likelihood of matching each query. The rationale is that the most likely hypotheses are sufficiently close to the query, so a good solution can be found by convex energy minimization techniques. Once a solution has been found, the 3D head centroid and the query are assumed to have similar morphology, yielding the soft label. Our experiments point toward the usefulness of the proposed solution, which can improve the effectiveness of face recognizers and can also be used as a privacy-preserving solution for biometric recognition in public environments. © 2016 IEEE.","homeland security; Privacy-Preserving recognition; Soft biometrics; visual surveillance"
"Bergamasco F., Albarelli A., Cosmo L., Rodola E., Torsello A.","An Accurate and Robust Artificial Marker Based on Cyclic Codes",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","12", 7384749,"2359","2373",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995578519&doi=10.1109%2fTPAMI.2016.2519024&partnerID=40&md5=95273fba8ab2e1b1502efe98749f5096","Artificial markers are successfully adopted to solve several vision tasks, ranging from tracking to calibration. While most designs share the same working principles, many specialized approaches exist to address specific application domains. Some are specially crafted to boost pose recovery accuracy. Others are made robust to occlusion or easy to detect with minimal computational resources. The sheer amount of approaches available in recent literature is indeed a statement to the fact that no silver bullet exists. Furthermore, this is also a hint to the level of scholarly interest that still characterizes this research topic. With this paper we try to add a novel option to the offer, by introducing a general purpose fiducial marker which exhibits many useful properties while being easy to implement and fast to detect. The key ideas underlying our approach are three. The first one is to exploit the projective invariance of conics to jointly find the marker and set a reading frame for it. Moreover, the tag identity is assessed by a redundant cyclic coded sequence implemented using the same circular features used for detection. Finally, the specific design and feature organization of the marker are well suited for several practical tasks, ranging from camera calibration to information payload delivery. © 2016 IEEE.","camera calibration; cyclic codes; fiducial markers; Pose estimation; RUNE tag"
"Zeng Y., Wang C., Gu X., Samaras D., Paragios N.","Higher-Order Graph Principles towards Non-Rigid Surface Registration",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","12", 7404254,"2416","2429",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995693662&doi=10.1109%2fTPAMI.2016.2528240&partnerID=40&md5=049fcceb7a7912a0327ba1f81f7ebcdc","This paper casts surface registration as the problem of finding a set of discrete correspondences through the minimization of an energy function, which is composed of geometric and appearance matching costs, as well as higher-order deformation priors. Two higher-order graph-based formulations are proposed under different deformation assumptions. The first formulation encodes isometric deformations using conformal geometry in a higher-order graph matching problem, which is solved through dual-decomposition and is able to handle partial matching. Despite the isometry assumption, this approach is able to robustly match sparse feature point sets on surfaces undergoing highly anisometric deformations. Nevertheless, its performance degrades significantly when addressing anisometric registration for a set of densely sampled points. This issue is rigorously addressed subsequently through a novel deformation model that is able to handle arbitrary diffeomorphisms between two surfaces. Such a deformation model is introduced into a higher-order Markov Random Field for dense surface registration, and is inferred using a new parallel and memory efficient algorithm. To deal with the prohibitive search space, we also design an efficient way to select a number of matching candidates for each point of the source surface based on the matching results of a sparse set of points. A series of experiments demonstrate the accuracy and the efficiency of the proposed framework, notably in challenging cases of large and/or anisometric deformations, or surfaces that are partially occluded. © 2016 IEEE.","conformal geometry; higher-order graph matching; Higher-Order Markov random fields; Surface registration"
"Lee N.H., Tang R., Priebe C.E., Rosen M.","A Model Selection Approach for Clustering a Multinomial Sequence with Non-Negative Factorization",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","12", 7393849,"2345","2358",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995691247&doi=10.1109%2fTPAMI.2016.2522443&partnerID=40&md5=afffb3fc1b073412168064061d3eb1de","We consider a problem of clustering a sequence of multinomial observations by way of a model selection criterion. We propose a form of a penalty term for the model selection procedure. Our approach subsumes both the conventional AIC and BIC criteria but also extends the conventional criteria in a way that it can be applicable also to a sequence of sparse multinomial observations, where even within a same cluster, the number of multinomial trials may be different for different observations. In addition, as a preliminary estimation step to maximum likelihood estimation, and more generally, to maximum $L-{q}$ estimation, we propose to use reduced rank projection in combination with non-negative factorization. We motivate our approach by showing that our model selection criterion and preliminary estimation step yield consistent estimates under simplifying assumptions. We also illustrate our approach through numerical experiments using real and simulated data. © 2016 IEEE.","Model selection; networks/graphs; non-negative data; Pattern recognition; statistics; stochastic"
"Zhai Y., Ong Y.-S., Tsang I.W.","Making Trillion Correlations Feasible in Feature Grouping and Selection",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","12", 7415982,"2472","2486",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995543411&doi=10.1109%2fTPAMI.2016.2533384&partnerID=40&md5=20fd1895204d51f3416d29623b1b9124","Today, modern databases with 'Big Dimensionality' are experiencing a growing trend. Existing approaches that require the calculations of pairwise feature correlations in their algorithmic designs have scored miserably on such databases, since computing the full correlation matrix (i.e., square of dimensionality in size) is computationally very intensive (i.e., million features would translate to trillion correlations). This poses a notable challenge that has received much lesser attention in the field of machine learning and data mining research. Thus, this paper presents a study to fill in this gap. Our findings on several established databases with big dimensionality across a wide spectrum of domains have indicated that an extremely small portion of the feature pairs contributes significantly to the underlying interactions and there exists feature groups that are highly correlated. Inspired by the intriguing observations, we introduce a novel learning approach that exploits the presence of sparse correlations for the efficient identifications of informative and correlated feature groups from big dimensional data that translates to a reduction in complexity from O(m2) to O(mlogm + Kamn), where Kamin(m,n) generally holds. In particular, our proposed approach considers an explicit incorporation of linear and nonlinear correlation measures as constraints in the learning model. An efficient embedded feature selection strategy, designed to filter out the large number of non-contributing correlations that could otherwise confuse the classifier while identifying the correlated and informative feature groups, forms one of the highlights of our approach. We also demonstrated the proposed method on one-class learning, where notable speedup can be observed when solving one-class problem on big dimensional data. Further, to identify robust informative features with minimal sampling bias, our feature selection strategy embeds the V-fold cross validation in the learning model, so as to seek for features that exhibit stable or consistent performance accuracy on multiple data folds. Extensive empirical studies on both synthetic and several real-world datasets comprising up to 30 million dimensions are subsequently conducted to assess and showcase the efficacy of the proposed approach. © 2016 IEEE.","Big dimensionality; feature grouping; one-class learning; Robust feature selection; sparse correlation"
"Akhtar N., Shafait F., Mian A.","Discriminative Bayesian Dictionary Learning for Classification",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","12", 7404063,"2374","2388",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995544842&doi=10.1109%2fTPAMI.2016.2527652&partnerID=40&md5=23bd65e037fe3c8008fa9935af06651f","We propose a Bayesian approach to learn discriminative dictionaries for sparse representation of data. The proposed approach infers probability distributions over the atoms of a discriminative dictionary using a finite approximation of Beta Process. It also computes sets of Bernoulli distributions that associate class labels to the learned dictionary atoms. This association signifies the selection probabilities of the dictionary atoms in the expansion of class-specific data. Furthermore, the non-parametric character of the proposed approach allows it to infer the correct size of the dictionary. We exploit the aforementioned Bernoulli distributions in separately learning a linear classifier. The classifier uses the same hierarchical Bayesian model as the dictionary, which we present along the analytical inference solution for Gibbs sampling. For classification, a test instance is first sparsely encoded over the learned dictionary and the codes are fed to the classifier. We performed experiments for face and action recognition; and object and scene-category classification using five public datasets and compared the results with state-of-the-art discriminative sparse representation approaches. Experiments show that the proposed Bayesian approach consistently outperforms the existing approaches. © 2016 IEEE.","Bayesian sparse representation; Classification; discriminative dictionary learning; supervised learning"
"Feichtenhofer C., Pinz A., Wildes R.P.","Dynamic Scene Recognition with Complementary Spatiotemporal Features",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","12", 7399428,"2389","2401",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995679710&doi=10.1109%2fTPAMI.2016.2526008&partnerID=40&md5=724fdd3a0f295974ca6b69a2500688c5","This paper presents Dynamically Pooled Complementary Features (DPCF), a unified approach to dynamic scene recognition that analyzes a short video clip in terms of its spatial, temporal and color properties. The complementarity of these properties is preserved through all main steps of processing, including primitive feature extraction, coding and pooling. In the feature extraction step, spatial orientations capture static appearance, spatiotemporal oriented energies capture image dynamics and color statistics capture chromatic information. Subsequently, primitive features are encoded into a mid-level representation that has been learned for the task of dynamic scene recognition. Finally, a novel dynamic spacetime pyramid is introduced. This dynamic pooling approach can handle both global as well as local motion by adapting to the temporal structure, as guided by pooling energies. The resulting system provides online recognition of dynamic scenes that is thoroughly evaluated on the two current benchmark datasets and yields best results to date on both datasets. In-depth analysis reveals the benefits of explicitly modeling feature complementarity in combination with the dynamic spacetime pyramid, indicating that this unified approach should be well-suited to many areas of video analysis. © 2016 IEEE.","Dynamic scenes; feature representations; image dynamics; Spatiotemporal orientation; visual spacetime"
"Wang T., Gong S., Zhu X., Wang S.","Person Re-Identification by Discriminative Selection in Video Ranking",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","12", 7393860,"2501","2514",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995618075&doi=10.1109%2fTPAMI.2016.2522418&partnerID=40&md5=d516b7ffccff76fe219f87bd7abfb512","Current person re-identification (ReID) methods typically rely on single-frame imagery features, whilst ignoring space-time information from image sequences often available in the practical surveillance scenarios. Single-frame (single-shot) based visual appearance matching is inherently limited for person ReID in public spaces due to the challenging visual ambiguity and uncertainty arising from non-overlapping camera views where viewing condition changes can cause significant people appearance variations. In this work, we present a novel model to automatically select the most discriminative video fragments from noisy/incomplete image sequences of people from which reliable space-time and appearance features can be computed, whilst simultaneously learning a video ranking function for person ReID. Using the PRID2011, iLIDS-VID, and HDA+ image sequence datasets, we extensively conducted comparative evaluations to demonstrate the advantages of the proposed model over contemporary gait recognition, holistic image sequence matching and state-of-the-art single-/multi-shot ReID methods. © 2016 IEEE.","discriminative selection; multi-instance ranking; Person re-identification; sequence matching; Video ranking"
"Lopez M.B., Boutellaa E., Hadid A.","Comments on the 'Kinship Face in the Wild' Data Sets",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","11", 7393855,"2342","2344",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992135244&doi=10.1109%2fTPAMI.2016.2522416&partnerID=40&md5=4a35c525db4304977493ee019e8e3b94","The Kinship Face in the Wild data sets, recently published in TPAMI, are currently used as a benchmark for the evaluation of kinship verification algorithms. We recommend that these data sets are no longer used in kinship verification research unless there is a compelling reason that takes into account the nature of the images. We note that most of the image kinship pairs are cropped from the same photographs. Exploiting this cropping information, competitive but biased performance can be obtained using a simple scoring approach, taking only into account the nature of the image pairs rather than any features about kin information. To illustrate our motives, we provide classification results utilizing a simple scoring method based on the image similarity of both images of a kinship pair. Using simply the distance of the chrominance averages of the images in the Lab color space without any training or using any specific kin features, we achieve performance comparable to state-of-the-art methods. We provide the source code to prove the validity of our claims and ensure the repeatability of our experiments. © 2016 IEEE.","biometrics; face recognition; Kinship verification"
"Hong Y., Kwitt R., Singh N., Vasconcelos N., Niethammer M.","Parametric Regression on the Grassmannian",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","11", 7378521,"2284","2297",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992028858&doi=10.1109%2fTPAMI.2016.2516533&partnerID=40&md5=afaa1198c5faa0bad7d5f62d41b3f777","We address the problem of fitting parametric curves on the Grassmann manifold for the purpose of intrinsic parametric regression. We start from the energy minimization formulation of linear least-squares in Euclidean space and generalize this concept to general nonflat Riemannian manifolds, following an optimal-control point of view. We then specialize this idea to the Grassmann manifold and demonstrate that it yields a simple, extensible and easy-to-implement solution to the parametric regression problem. In fact, it allows us to extend the basic geodesic model to (1) a 'time-warped' variant and (2) cubic splines. We demonstrate the utility of the proposed solution on different vision problems, such as shape regression as a function of age, traffic-speed estimation and crowd-counting from surveillance video clips. Most notably, these problems can be conveniently solved within the same framework without any specifically-tailored steps along the processing pipeline. © 2016 IEEE.","cubic splines; geodesic shooting; Grassmann manifold; Parametric regression; time-warping"
"Yang J., Li H., Campbell D., Jia Y.","Go-ICP: A Globally Optimal Solution to 3D ICP Point-Set Registration",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","11", 7368945,"2241","2254",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991746810&doi=10.1109%2fTPAMI.2015.2513405&partnerID=40&md5=908266b3d24d75cea61f2493a38250e1","The Iterative Closest Point (ICP) algorithm is one of the most widely used methods for point-set registration. However, being based on local iterative optimization, ICP is known to be susceptible to local minima. Its performance critically relies on the quality of the initialization and only local optimality is guaranteed. This paper presents the first globally optimal algorithm, named Go-ICP, for Euclidean (rigid) registration of two 3D point-sets under the L2 error metric defined in ICP. The Go-ICP method is based on a branch-and-bound scheme that searches the entire 3D motion space SE(3). By exploiting the special structure of SE(3) geometry, we derive novel upper and lower bounds for the registration error function. Local ICP is integrated into the BnB scheme, which speeds up the new method while guaranteeing global optimality. We also discuss extensions, addressing the issue of outlier robustness. The evaluation demonstrates that the proposed method is able to produce reliable registration results regardless of the initialization. Go-ICP can be applied in scenarios where an optimal solution is desirable or where a good initialization is not always available. © 2015 IEEE.","3D point-set registration; branch-and-bound; global optimization; iterative closest point; SE(3) space search"
"Wang T.-C., Efros A.A., Ramamoorthi R.","Depth Estimation with Occlusion Modeling Using Light-Field Cameras",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","11", 7374709,"2170","2181",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991760569&doi=10.1109%2fTPAMI.2016.2515615&partnerID=40&md5=b87941772bb31daef779890eda4575f0","Light-field cameras have become widely available in both consumer and industrial applications. However, most previous approaches do not model occlusions explicitly, and therefore fail to capture sharp object boundaries. A common assumption is that for a Lambertian scene, a pixel will exhibit photo-consistency, which means all viewpoints converge to a single point when focused to its depth. However, in the presence of occlusions this assumption fails to hold, making most current approaches unreliable precisely where accurate depth information is most important-at depth discontinuities. In this paper, an occlusion-aware depth estimation algorithm is developed; the method also enables identification of occlusion edges, which may be useful in other applications. It can be shown that although photo-consistency is not preserved for pixels at occlusions, it still holds in approximately half the viewpoints. Moreover, the line separating the two view regions (occluded object versus occluder) has the same orientation as that of the occlusion edge in the spatial domain. By ensuring photo-consistency in only the occluded view region, depth estimation can be improved. Occlusion predictions can also be computed and used for regularization. Experimental results show that our method outperforms current state-of-the-art light-field depth estimation algorithms, especially near occlusion boundaries. © 2016 IEEE.","3D reconstruction; Light-fields; occlusion detection"
"Elhamifar E., Sapiro G., Sastry S.S.","Dissimilarity-based sparse subset selection",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","11", 7364258,"2182","2197",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991585566&doi=10.1109%2fTPAMI.2015.2511748&partnerID=40&md5=2cf7ae58202850e091a843204ef29bf0","Finding an informative subset of a large collection of data points or models is at the center of many problems in computer vision, recommender systems, bio/health informatics as well as image and natural language processing. Given pairwise dissimilarities between the elements of a 'source set' and a 'target set,' we consider the problem of finding a subset of the source set, called representatives or exemplars, that can efficiently describe the target set.We formulate the problem as a row-sparsity regularized trace minimization problem. Since the proposed formulation is, in general, NP-hard, we consider a convex relaxation. The solution of our optimization finds representatives and the assignment of each element of the target set to each representative, hence, obtaining a clustering. We analyze the solution of our proposed optimization as a function of the regularization parameter. We show that when the two sets jointly partition into multiple groups, our algorithm finds representatives from all groups and reveals clustering of the sets. In addition, we show that the proposed framework can effectively deal with outliers. Our algorithm works with arbitrary dissimilarities, which can be asymmetric or violate the triangle inequality. To efficiently implement our algorithm, we consider an Alternating Direction Method of Multipliers (ADMM) framework, which results in quadratic complexity in the problem size. We show that the ADMM implementation allows to parallelize the algorithm, hence further reducing the computational time. Finally, by experiments on real-world datasets, we show that our proposed algorithm improves the state of the art on the two problems of scene categorization using representative images and time-series modeling and segmentation using representative models. © 2015 IEEE.","Activity clustering; ADMM optimization; Clustering; Convex programming; Encoding; Model identification; Outliers; Pairwise dissimilarities; Representatives; Sampling; Scene recognition; Simultaneous sparse recovery; Time-series data; Video summarization"
"Hasnat M.A., Alata O., Tremeau A.","Joint Color-Spatial-Directional Clustering and Region Merging (JCSD-RM) for Unsupervised RGB-D Image Segmentation",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","11", 7368926,"2255","2268",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991821704&doi=10.1109%2fTPAMI.2015.2513407&partnerID=40&md5=f737752eb8e85161baf3df56805305a4","Recent advances in depth imaging sensors provide easy access to the synchronized depth with color, called RGB-D image. In this paper, we propose an unsupervised method for indoor RGB-D image segmentation and analysis. We consider a statistical image generation model based on the color and geometry of the scene. Our method consists of a joint color-spatial-directional clustering method followed by a statistical planar region merging method. We evaluate our method on the NYU depth database and compare it with existing unsupervised RGB-D segmentation methods. Results show that, it is comparable with the state of the art methods and it needs less computation time. Moreover, it opens interesting perspectives to fuse color and geometry in an unsupervised manner. © 2015 IEEE.","Bregman divergence; clustering; directional distributions; mixture model; region adjacency graph; region merging; RGB-D image segmentation; Unsupervised"
"Wang X., Turetken E., Fleuret F., Fua P.","Tracking Interacting Objects Using Intertwined Flows",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","11", 7368937,"2312","2326",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991787640&doi=10.1109%2fTPAMI.2015.2513406&partnerID=40&md5=5245463ac97f38ae222844e1fe923603","In this paper, we show that tracking different kinds of interacting objects can be formulated as a network-flow mixed integer program. This is made possible by tracking all objects simultaneously using intertwined flow variables and expressing the fact that one object can appear or disappear at locations where another is in terms of linear flow constraints. Our proposed method is able to track invisible objects whose only evidence is the presence of other objects that contain them. Furthermore, our tracklet-based implementation yields real-time tracking performance. We demonstrate the power of our approach on scenes involving cars and pedestrians, bags being carried and dropped by people, and balls being passed from one player to the next in team sports. In particular, we show that by estimating jointly and globally the trajectories of different types of objects, the presence of the ones which were not initially detected based solely on image evidence can be inferred from the detections of the others. © 2015 IEEE.","interactions; mixed integer programming; Multi-object tracking; network flows"
"Meila M., Chen H.","Bayesian Non-Parametric Clustering of Ranking Data",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","11", 7374715,"2156","2169",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991794214&doi=10.1109%2fTPAMI.2016.2515599&partnerID=40&md5=121e517409baa51f8b2528e3282ac646","This paper studies the estimation of Dirichlet process mixtures over discrete incomplete rankings. The generative model for each mixture component is the generalized Mallows (GM) model, an exponential family model for permutations which extends seamlessly to top-$t$ rankings. While the GM is remarkably tractable in comparison with other permutation models, its conjugate prior is not. Our main contribution is to derive the theory and algorithms for sampling from the desired posterior distributions under this DPM. We introduce a family of partially collapsed Gibbs samplers, containing as one extreme point an exact algorithm based on slice-sampling, and at the other a fast approximate sampler with superior mixing that is still very accurate in all but the lowest ranks. We empirically demonstrate the effectiveness of the approximation in reducing mixing time, the benefits of the Dirichlet process approach over alternative clustering techniques, and the applicability of the approach to exploring large real-world ranking datasets. © 2016 IEEE.","Dirichlet process mixture; generalized Mallows model; non-parametric clustering; Rank data; top-t rankings"
"Kristan M., Matas J., Leonardis A., Vojir T., Pflugfelder R., Fernandez G., Nebehay G., Porikli F., Cehovin L.","A Novel Performance Evaluation Methodology for Single-Target Trackers",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","11", 7379002,"2137","2155",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992034892&doi=10.1109%2fTPAMI.2016.2516982&partnerID=40&md5=bd55a8de6670f9697c3f88171c17ddf1","This paper addresses the problem of single-target tracker performance evaluation. We consider the performance measures, the dataset and the evaluation system to be the most important components of tracker evaluation and propose requirements for each of them. The requirements are the basis of a new evaluation methodology that aims at a simple and easily interpretable tracker comparison. The ranking-based methodology addresses tracker equivalence in terms of statistical significance and practical differences. A fully-annotated dataset with per-frame annotations with several visual attributes is introduced. The diversity of its visual properties is maximized in a novel way by clustering a large number of videos according to their visual attributes. This makes it the most sophistically constructed and annotated dataset to date. A multi-platform evaluation system allowing easy integration of third-party trackers is presented as well. The proposed evaluation methodology was tested on the VOT2014 challenge on the new dataset and 38 trackers, making it the largest benchmark to date. Most of the tested trackers are indeed state-of-the-art since they outperform the standard baselines, resulting in a highly-challenging benchmark. An exhaustive analysis of the dataset from the perspective of tracking difficulty is carried out. To facilitate tracker comparison a new performance visualization technique is proposed. © 2016 IEEE.","model-free tracking; Performance analysis; single-target tracking; tracker evaluation datasets; tracker evaluation methodology; tracker evaluation system"
"Yu X., Huang J., Zhang S., Metaxas D.N.","Face landmark fitting via optimized part mixtures and cascaded deformable model",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","11", 7360185,"2212","2226",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990933998&doi=10.1109%2fTPAMI.2015.2509999&partnerID=40&md5=a94fae75e001555e00a96c1e1a1d526a","This paper addresses the problemof facial landmark localization and tracking from a single camera.We present a two-stage cascaded deformable shape model to effectively and efficiently localize facial landmarks with large head pose variations. In initialization stage, we propose a group sparse optimized mixture model to automatically select the most salient facial landmarks. By introducing 3D face shape model, we apply procrustes analysis to provide pose-aware landmark initialization. In landmark localization stage, the first step uses mean-shift local search with constrained local model to rapidly approach the global optimum. The second step uses component-wise active contours to discriminatively refine the subtle shape variation. Our framework simultaneously handles face detection, pose-robust landmark localization and tracking in real time. Extensive experiments are conducted on both laboratory environmental databases and face-in-the-wild databases. The results reveal that our approach consistently outperforms state-of-the-art methods for face alignment and tracking. © 2015 IEEE.","Deformable shape model; Face landmark localization; Face tracking; Part based model"
"Zhou L., Wang L., Liu L., Ogunbona P., Shen D.","Learning discriminative Bayesian networks from high-dimensional continuous neuroimaging data",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","11", 7364252,"2269","2283",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991693784&doi=10.1109%2fTPAMI.2015.2511754&partnerID=40&md5=c15a852ccb919f2b28ab7a780bab89e8","Due to its causal semantics, Bayesian networks (BN) have been widely employed to discover the underlying data relationship in exploratory studies, such as brain research. Despite its success in modeling the probability distribution of variables, BN is naturally a generative model, which is not necessarily discriminative. This may cause the ignorance of subtle but critical network changes that are of investigation values across populations. In this paper, we propose to improve the discriminative power of BN models for continuous variables from two different perspectives. This brings two general discriminative learning frameworks for Gaussian Bayesian networks (GBN). In the first framework, we employ Fisher kernel to bridge the generative models of GBN and the discriminative classifiers of SVMs, and convert the GBN parameter learning to Fisher kernel learning via minimizing a generalization error bound of SVMs. In the second framework, we employ the max-margin criterion and build it directly upon GBN models to explicitly optimize the classification performance of the GBNs. The advantages and disadvantages of the two frameworks are discussed and experimentally compared. Both of them demonstrate strong power in learning discriminative parameters of GBNs for neuroimaging based brain network analysis, as well as maintaining reasonable representation capacity. The contributions of this paper also include a new Directed Acyclic Graph (DAG) constraint with theoretical guarantee to ensure the graph validity of GBN. © 2015 IEEE.","Bayesian network; Brain network; Discriminative learning; Fisher kernel learning; Max-margin"
"Wang Z., Fan B., Wang G., Wu F.","Exploring Local and Overall Ordinal Information for Robust Feature Description",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","11", 7368924,"2198","2211",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991769704&doi=10.1109%2fTPAMI.2015.2513396&partnerID=40&md5=79dbef2edee52bf0ba5093048bb7ac27","This paper aims to build robust feature descriptors by exploring intensity order information in a patch. To this end, the local intensity order pattern (LIOP) and the overall intensity order pattern (OIOP) are proposed to effectively encode intensity order information of each pixel in different aspects. Specifically, LIOP captures the local ordinal information by using the intensity relationships among all the neighbouring sampling points around a pixel, while OIOP exploits the coarsely quantized overall intensity order of these sampling points. These two kinds of patterns are then separately aggregated into different ordinal bins, leading to two kinds of feature descriptors. Furthermore, as these two kinds of descriptors could encode complementary ordinal information, they are combined together to obtain a discriminative and compact mixed intensity order pattern descriptor. All these descriptors are constructed on the basis of relative relationships of intensities in a rotationally invariant way, making them be inherently invariant to image rotation and any monotonic intensity changes. Experimental results on image matching and object recognition are encouraging, demonstrating the superiorities of our descriptors over the state of the art. © 2015 IEEE.","Feature description; illumination invariance; image matching; intensity order"
"Hauberg S., Feragen A., Enficiaud R., Black M.J.","Scalable robust principal component analysis using Grassmann Averages",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","11", 7364267,"2298","2311",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991736442&doi=10.1109%2fTPAMI.2015.2511743&partnerID=40&md5=7da28e559d0609393aabcacbb2fdbe54","In large datasets, manual data verification is impossible, and we must expect the number of outliers to increase with data size. While principal component analysis (PCA) can reduce data size, and scalable solutions exist, it is well-known that outliers can arbitrarily corrupt the results. Unfortunately, state-of-the-art approaches for robust PCA are not scalable. We note that in a zero-mean dataset, each observation spans a one-dimensional subspace, giving a point on the Grassmann manifold. We show that the average subspace corresponds to the leading principal component for Gaussian data. We provide a simple algorithm for computing this Grassmann Average (GA), and show that the subspace estimate is less sensitive to outliers than PCA for general distributions. Because averages can be efficiently computed, we immediately gain scalability. We exploit robust averaging to formulate the Robust Grassmann Average (RGA) as a form of robust PCA. The resulting Trimmed Grassmann Average (TGA) is appropriate for computer vision because it is robust to pixel outliers. The algorithm has linear computational complexity and minimal memory requirements. We demonstrate TGA for background modeling, video restoration, and shadow removal.We show scalability by performing robust PCA on the entire Star Wars IV movie; a task beyond any current method. Source code is available online. © 2015 IEEE.","Dimensionality reduction; Robust principal component analysis; Subspace estimation"
"Kalogeiton V., Ferrari V., Schmid C.","Analysing Domain Shift Factors between Videos and Images for Object Detection",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","11", 7448416,"2327","2334",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992145846&doi=10.1109%2fTPAMI.2016.2551239&partnerID=40&md5=2436c85b2ad5f492017f46fc96603b31","Object detection is one of the most important challenges in computer vision. Object detectors are usually trained on bounding-boxes from still images. Recently, video has been used as an alternative source of data. Yet, for a given test domain (image or video), the performance of the detector depends on the domain it was trained on. In this paper, we examine the reasons behind this performance gap. We define and evaluate different domain shift factors: spatial location accuracy, appearance diversity, image quality and aspect distribution. We examine the impact of these factors by comparing performance before and after factoring them out. The results show that all four factors affect the performance of the detectors and their combined effect explains nearly the whole performance gap. © 2016 IEEE.","domain adaptation; Object detection; video and image analysis"
"Liu M., Zhang D., Chen S., Xue H.","Joint binary classifier learning for ECOC-based multi-class classification",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","11", 7106533,"2335","2341",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990982240&doi=10.1109%2fTPAMI.2015.2430325&partnerID=40&md5=371392d7bdfd5a147e4c9af99786a2f2","Error-correcting output coding (ECOC) is one of the most widely used strategies for dealing with multi-class problems by decomposing the original multi-class problem into a series of binary sub-problems. In traditional ECOC-based methods, binary classifiers corresponding to those sub-problems are usually trained separately without considering the relationships among these classifiers. However, as these classifiers are established on the same training data, there may be some inherent relationships among them. Exploiting such relationships can potentially improve the generalization performances of individual classifiers, and, thus, boost ECOC learning algorithms. In this paper, we explore to mine and utilize such relationship through a joint classifier learning method, by integrating the training of binary classifiers and the learning of the relationship among them into a unified objective function. We also develop an efficient alternating optimization algorithm to solve the objective function. To evaluate the proposed method, we perform a series of experiments on eleven datasets from the UCI machine learning repository as well as two datasets from real-world image recognition tasks. The experimental results demonstrate the efficacy of the proposed method, compared with state-of-the-art methods for ECOC-based multi-class classification. © 2015 IEEE.","(joint) binary classifier learning; Error-correcting output coding (ECOC); Multi-class classification; Relationship"
"Parra Bustos A., Chin T.-J., Eriksson A., Li H., Suter D.","Fast Rotation Search with Stereographic Projections for 3D Registration",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","11", 7381673,"2227","2240",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992066436&doi=10.1109%2fTPAMI.2016.2517636&partnerID=40&md5=928efdd733eb488c6df410a64d053373","Registering two 3D point clouds involves estimating the rigid transform that brings the two point clouds into alignment. Recently there has been a surge of interest in using branch-and-bound (BnB) optimisation for point cloud registration. While BnB guarantees globally optimal solutions, it is usually too slow to be practical. A fundamental source of difficulty lies in the search for the rotational parameters. In this work, first by assuming that the translation is known, we focus on constructing a fast rotation search algorithm. With respect to an inherently robust geometric matching criterion, we propose a novel bounding function for BnB that is provably tighter than previously proposed bounds. Further, we also propose a fast algorithm to evaluate our bounding function. Our idea is based on using stereographic projections to precompute and index all possible point matches in spatial R-trees for rapid evaluations. The result is a fast and globally optimal rotation search algorithm. To conduct full 3D registration, we co-optimise the translation by embedding our rotation search kernel in a nested BnB algorithm. Since the inner rotation search is very efficient, the overall 6DOF optimisation is speeded up significantly without losing global optimality. On various challenging point clouds, including those taken out of lab settings, our approach demonstrates superior efficiency. © 2016 IEEE.","branch-and-bound; Point cloud registration; R-trees; rotation search; stereographic projections"
"Wang K., He R., Wang L., Wang W., Tan T.","Joint Feature Selection and Subspace Learning for Cross-Modal Retrieval",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","10", 7346492,"2010","2023",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986260172&doi=10.1109%2fTPAMI.2015.2505311&partnerID=40&md5=ac32b419c9ec8e214e34ab45fc723f7a","Cross-modal retrieval has recently drawn much attention due to the widespread existence of multimodal data. It takes one type of data as the query to retrieve relevant data objects of another type, and generally involves two basic problems: the measure of relevance and coupled feature selection. Most previous methods just focus on solving the first problem. In this paper, we aim to deal with both problems in a novel joint learning framework. To address the first problem, we learn projection matrices to map multimodal data into a common subspace, in which the similarity between different modalities of data can be measured. In the learning procedure, the ℓ 21 -norm penalties are imposed on the projection matrices separately to solve the second problem, which selects relevant and discriminative features from different feature spaces simultaneously. A multimodal graph regularization term is further imposed on the projected data,which preserves the inter-modality and intra-modality similarity relationships.An iterative algorithm is presented to solve the proposed joint learning problem, along with its convergence analysis. Experimental results on cross-modal retrieval tasks demonstrate that the proposed method outperforms the state-of-the-art subspace approaches. © 1979-2012 IEEE.","coupled feature selection; cross-modal retrieval; half-quadratic minimization; Subspace learning"
"Liu F., Shen C., Lin G., Reid I.","Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","10", 7346484,"2024","2039",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986257303&doi=10.1109%2fTPAMI.2015.2505283&partnerID=40&md5=35272508bfd48064530f0c64280da10c","In this article, we tackle the problem of depth estimation from single monocular images. Compared with depth estimation using multiple images such as stereo depth perception, depth from monocular images is much more challenging. Prior work typically focuses on exploiting geometric priors or additional sources of information, most using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) set new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimation can be naturally formulated as a continuous conditional random field (CRF) learning problem. Therefore, here we present a deep convolutional neural field model for estimating depths from single monocular images, aiming to jointly explore the capacity of deep CNN and continuous CRF. In particular, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework. We then further propose an equally effective model based on fully convolutional networks and a novel superpixel pooling method, which is about 10 times faster, to speedup the patch-wise convolutions in the deep model. With this more efficient model, we are able to design deeper networks to pursue better performance. Our proposed method can be used for depth estimation of general scenes with no geometric priors nor any extra information injected. In our case, the integral of the partition function can be calculated in a closed form such that we can exactly solve the log-likelihood maximization. Moreover, solving the inference problem for predicting depths of a test image is highly efficient as closed-form solutions exist. Experiments on both indoor and outdoor scene datasets demonstrate that the proposed method outperforms state-of-the-art depth estimation approaches. © 1979-2012 IEEE.","conditional random field (CRF); deep convolutional neural networks (CNN); Depth estimation; fully convolutional networks; superpixel pooling"
"Belagiannis V., Amin S., Andriluka M., Schiele B., Navab N., Ilic S.","3D Pictorial Structures Revisited: Multiple Human Pose Estimation",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","10", 7360209,"1929","1942",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986276581&doi=10.1109%2fTPAMI.2015.2509986&partnerID=40&md5=fa4d90fb8f992a40d9ff17f5a81bf220","We address the problem of 3D pose estimation of multiple humans from multiple views. The transition from single to multiple human pose estimation and from the 2D to 3D space is challenging due to a much larger state space, occlusions and across-view ambiguities when not knowing the identity of the humans in advance. To address these problems, we first create a reduced state space by triangulation of corresponding pairs of body parts obtained by part detectors for each camera view. In order to resolve ambiguities of wrong and mixed parts of multiple humans after triangulation and also those coming from false positive detections, we introduce a 3D pictorial structures (3DPS) model. Our model builds on multi-view unary potentials, while a prior model is integrated into pairwise and ternary potential functions. To balance the potentials' influence, the model parameters are learnt using a Structured SVM (SSVM). The model is generic and applicable to both single and multiple human pose estimation. To evaluate our model on single and multiple human pose estimation, we rely on four different datasets. We first analyse the contribution of the potentials and then compare our results with related work where we demonstrate superior performance. © 1979-2012 IEEE.","3D pictorial structures; Human pose estimation; part-based models"
"Barrett D.P., Barbu A., Siddharth N., Siskind J.M.","Saying What You're Looking For: Linguistics Meets Video Search",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","10", 7346469,"2069","2081",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986267509&doi=10.1109%2fTPAMI.2015.2505297&partnerID=40&md5=72f8f59867463d449f8432aa0926ab66","We present an approach to searching large video corpora for clips which depict a natural-language query in the form of a sentence. Compositional semantics is used to encode subtle meaning differences lost in other approaches, such as the difference between two sentences which have identical words but entirely different meaning: The person rode the horse versus The horse rode the person. Given a sentential query and a natural-language parser, we produce a score indicating how well a video clip depicts that sentence for each clip in a corpus and return a ranked list of clips. Two fundamental problems are addressed simultaneously: detecting and tracking objects, and recognizing whether those tracks depict the query. Because both tracking and object detection are unreliable, our approach uses the sentential query to focus the tracker on the relevant participants and ensures that the resulting tracks are described by the sentential query. While most earlier work was limited to single-word queries which correspond to either verbs or nouns, we search for complex queries which contain multiple phrases, such as prepositional phrases, and modifiers, such as adverbs. We demonstrate this approach by searching for 2,627 naturally elicited sentential queries in 10 Hollywood movies. © 1979-2012 IEEE.","event recognition; language; object detection; Retrieval; sentential video retrieval; tracking; video"
"Sun Y., Wang X., Tang X.","Hybrid Deep Learning for Face Verification",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","10", 7346495,"1997","2009",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986331512&doi=10.1109%2fTPAMI.2015.2505293&partnerID=40&md5=cabe8d127f7ca21307a863884a66dae9","This paper proposes a hybrid convolutional network (ConvNet)-Restricted Boltzmann Machine (RBM) model for face verification. A key contribution of this work is to learn high-level relational visual features with rich identity similarity information. The deep ConvNets in our model start by extracting local relational visual features from two face images in comparison, which are further processed through multiple layers to extract high-level and global relational features. To keep enough discriminative information, we use the last hidden layer neuron activations of the ConvNet as features for face verification instead of those of the output layer. To characterize face similarities from different aspects, we concatenate the features extracted from different face region pairs by different deep ConvNets. The resulting high-dimensional relational features are classified by an RBM for face verification. After pre-training each ConvNet and the RBM separately, the entire hybrid network is jointly optimized to further improve the accuracy. Various aspects of the ConvNet structures, relational features, and face verification classifiers are investigated. Our model achieves the state-of-the-art face verification performance on the challenging LFW dataset under both the unrestricted protocol and the setting when outside data is allowed to be used for training. © 1979-2012 IEEE.","Convolutional networks; deep learning; face recognition"
"Shi X., Guo Z., Nie F., Yang L., You J., Tao D.","Two-Dimensional Whitening Reconstruction for Enhancing Robustness of Principal Component Analysis",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","10", 7331304,"2130","2136",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986321455&doi=10.1109%2fTPAMI.2015.2501810&partnerID=40&md5=60a582dc45bc518529d50b1233891173","Principal component analysis (PCA) is widely applied in various areas, one of the typical applications is in face. Many versions of PCA have been developed for face recognition. However, most of these approaches are sensitive to grossly corrupted entries in a 2D matrix representing a face image. In this paper, we try to reduce the influence of grosses like variations in lighting, facial expressions and occlusions to improve the robustness of PCA. In order to achieve this goal, we present a simple but effective unsupervised preprocessing method, two-dimensional whitening reconstruction (TWR), which includes two stages: 1) A whitening process on a 2D face image matrix rather than a concatenated 1D vector; 2) 2D face image matrix reconstruction. TWR reduces the pixel redundancy of the internal image, meanwhile maintains important intrinsic features. In this way, negative effects introduced by gross-like variations are greatly reduced. Furthermore, the face image with TWR preprocessing could be approximate to a Gaussian signal, on which PCA is more effective. Experiments on benchmark face databases demonstrate that the proposed method could significantly improve the robustness of PCA methods on classification and clustering, especially for the faces with severe illumination changes. © 1979-2012 IEEE.","PCA; preprocessing; robustness; Two-dimensional whitening reconstruction"
"Zhang X., Zou J., He K., Sun J.","Accelerating Very Deep Convolutional Networks for Classification and Detection",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","10", 7332968,"1943","1955",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986325670&doi=10.1109%2fTPAMI.2015.2502579&partnerID=40&md5=8ac164746bedebc5eac0aeba194de908","This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs [1] that have substantially impacted the computer vision community. Unlike previous methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We develop an effective solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD). More importantly, while previous methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction that reduces the rapidly accumulated error when multiple (e.g., ≥ 10) layers are approximated. For the widely used very deep VGG-16 model [1] , our method achieves a whole-model speedup of 4× with merely a 0.3 percent increase of top-5 error in ImageNet classification. Our 4× accelerated VGG-16 model also shows a graceful accuracy degradation for object detection when plugged into the Fast R-CNN detector [2]. © 1979-2012 IEEE.","acceleration; Convolutional neural networks; image classification; object detection"
"Qin Z., Shelton C.R.","Social Grouping for Multi-Target Tracking and Head Pose Estimation in Video",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","10", 7346504,"2082","2095",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986292250&doi=10.1109%2fTPAMI.2015.2505292&partnerID=40&md5=678aed279d4a4cddb79a12c7d483bafa","Many computer vision tasks are more difficult when tackled without contextual information. For example, in multi-camera tracking, pedestrians may look very different in different cameras with varying pose and lighting conditions. Similarly, head direction estimation in high-angle surveillance video in which human head images are low resolution is challenging. Even humans can have trouble without contextual information. In this work, we couple novel contextual information, social grouping, with two important computer vision tasks: multi-target tracking and head pose/direction estimation in surveillance video. These three components are modeled in a probabilistic formulation and we provide effective solvers.We show that social grouping effectively helps to mitigate visual ambiguities in multi-camera tracking and head pose estimation. We further notice that in single-camera multi-target tracking, social grouping provides a natural high-order association cue that avoids existing complex algorithms for high-order track association. In experiments, we demonstrate improvements with our model over models without social grouping context and several state-of-art approaches on a number of publicly available datasets on tracking, head pose estimation, and group discovery. © 1979-2012 IEEE.","context; head pose estimation; multi-camera tracking; Multi-target tracking; social grouping; video analysis"
"Jiang Y., Koppula H.S., Saxena A.","Modeling 3D Environments through Hidden Human Context",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","10", 7346466,"2040","2053",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986305469&doi=10.1109%2fTPAMI.2015.2501811&partnerID=40&md5=4031a89fac340060ec6e0e112dfd61ad","The idea of modeling object-object relations has been widely leveraged in many scene understanding applications. However, as the objects are designed by humans and for human usage, when we reason about a human environment, we reason about it through an interplay between the environment, objects and humans. In this paper, we model environments not only through objects, but also through latent human poses and human-object interactions. In order to handle the large number of latent human poses and a large variety of their interactions with objects, we present Infinite Latent Conditional Random Field (ILCRF) that models a scene as a mixture of CRFs generated from Dirichlet processes. In each CRF, we model objects and object-object relations as existing nodes and edges, and hidden human poses and human-object relations as latent nodes and edges. ILCRF generatively models the distribution of different CRF structures over these latent nodes and edges. We apply the model to the challenging applications of 3D scene labeling and robotic scene arrangement. In extensive experiments, we show that our model significantly outperforms the state-of-the-art results in both applications. We further use our algorithm on a robot for arranging objects in a new scene using the two applications aforementioned. © 1979-2012 IEEE.","3D scene understanding; human context; machine learning; robotics perception"
"Wen L., Lei Z., Lyu S., Li S.Z., Yang M.-H.","Exploiting Hierarchical Dense Structures on Hypergraphs for Multi-Object Tracking",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","10", 7360186,"1983","1996",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986328532&doi=10.1109%2fTPAMI.2015.2509979&partnerID=40&md5=a71dc7935f5ca228016d4bebd5d9920b","Most multi-object tracking algorithms are developed within the tracking-by-detection framework that consider the pairwise appearance similarities between detection responses or tracklets within a limited temporal window, and thus less effective in handling long-term occlusions or distinguishing spatially close targets with similar appearance in crowded scenes. In this work, we propose an algorithm that formulates the multi-object tracking task as one to exploit hierarchical dense structures on an undirected hypergraph constructed based on tracklet affinity. The dense structures indicate a group of vertices that are inter-connected with a set of hyperedges with high affinity values. The appearance and motion similarities among multiple tracklets across the spatio-temporal domain are considered globally by exploiting high-order similarities rather than pairwise ones, thereby facilitating distinguish spatially close targets with similar appearance. In addition, the hierarchical design of the optimization process helps the proposed tracking algorithm handle long-term occlusions robustly. Extensive experiments on various challenging datasets of both multi-pedestrian and multi-face tracking tasks, demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods. © 1979-2012 IEEE.","dense structures; hierarchical; Multi-object tracking; tracklet; undirected affinity hypergraph"
"Rodriguez-Serrano J.A., Larlus D., Dai Z.","Data-Driven Detection of Prominent Objects",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","10", 7360192,"1969","1982",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986286579&doi=10.1109%2fTPAMI.2015.2509988&partnerID=40&md5=ef7743c1717feda66fa4215b9806b7c6","This article deals with the detection of prominent objects in images. As opposed to the standard approaches based on sliding windows, we study a fundamentally different solution by formulating the supervised prediction of a bounding box as an image retrieval task. Indeed, given a global image descriptor, we find the most similar images in an annotated dataset, and transfer the object bounding boxes. We refer to this approach as data-driven detection (DDD). Our key novelty is to design or learn image similarities that explicitly optimize some aspect of the transfer unlike previous work which uses generic representations and unsupervised similarities. In a first variant, we explicitly learn to transfer, by adapting a metric learning approach to work with image and bounding box pairs. Second, we use a representation of images as object probability maps computed from low-level patch classifiers. Experiments show that these two contributions yield in some cases comparable or better results than standard sliding window detectors - despite its conceptual simplicity and run-time efficiency. Our third contribution is an application of prominent object detection, where we improve fine-grained categorization by pre-cropping images with the proposed approach. Finally, we also extend the proposed approach to detect multiple parts of rigid objects. © 1979-2012 IEEE.","fine-grained visual recognition; metric learning; object detection; object part localization; Object recognition"
"Lu N., Miao H.","Clustering Tree-Structured Data on Manifold",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","10", 7346489,"1956","1968",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986244042&doi=10.1109%2fTPAMI.2015.2505282&partnerID=40&md5=73669f57ffd0cf89b9508ad4f193d54c","Tree-structured data usually contain both topological and geometrical information, and are necessarily considered on manifold instead of euclidean space for appropriate data parameterization and analysis. In this study, we propose a novel tree-structured data parameterization, called Topology-Attribute matrix (T-A matrix), so the data clustering task can be conducted on matrix manifold. We incorporate the structure constraints embedded in data into the non-negative matrix factorization method to determine meta-trees from the T-A matrix, and the signature vector of each single tree can then be extracted by meta-tree decomposition. The meta-tree space turns out to be a cone space, in which we explore the distance metric and implement the clustering algorithm based on the concepts like Fréchet mean. Finally, the T-A matrix based clustering (TAMBAC) framework is evaluated and compared using both simulated data and real retinal images to illustrate its efficiency and accuracy. © 1979-2012 IEEE.","Clustering; geodesic; nonnegative matrix factorization; tree-structured data"
"Milan A., Schindler K., Roth S.","Multi-Target Tracking by Discrete-Continuous Energy Minimization",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","10", 7346505,"2054","2068",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986292356&doi=10.1109%2fTPAMI.2015.2505309&partnerID=40&md5=73c4968215d01a1111cd93dda74e65a5","The task of tracking multiple targets is often addressed with the so-called tracking-by-detection paradigm, where the first step is to obtain a set of target hypotheses for each frame independently. Tracking can then be regarded as solving two separate, but tightly coupled problems. The first is to carry out data association, i.e., to determine the origin of each of the available observations. The second problem is to reconstruct the actual trajectories that describe the spatio-temporal motion pattern of each individual target. The former is inherently a discrete problem, while the latter should intuitively be modeled in continuous space. Having to deal with an unknown number of targets, complex dependencies, and physical constraints, both are challenging tasks on their own and thus most previous work focuses on one of these subproblems. Here, we present a multi-target tracking approach that explicitly models both tasks as minimization of a unified discrete-continuous energy function. Trajectory properties are captured through global label costs, a recent concept from multi-model fitting, which we introduce to tracking. Specifically, label costs describe physical properties of individual tracks, e.g., linear and angular dynamics, or entry and exit points. We further introduce pairwise label costs to describe mutual interactions between targets in order to avoid collisions. By choosing appropriate forms for the individual energy components, powerful discrete optimization techniques can be leveraged to address data association, while the shapes of individual trajectories are updated by gradient-based continuous energy minimization. The proposed method achieves state-of-the-art results on diverse benchmark sequences. © 1979-2012 IEEE.","discrete-continuous optimization; Multi-object tracking; tracking-by-detection; visual surveillance"
"Shahroudy A., Ng T.-T., Yang Q., Wang G.","Multimodal Multipart Learning for Action Recognition in Depth Videos",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","10", 7346486,"2123","2129",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986267489&doi=10.1109%2fTPAMI.2015.2505295&partnerID=40&md5=3946538ab164f09a7762913822be0822","The articulated and complex nature of human actions makes the task of action recognition difficult. One approach to handle this complexity is dividing it to the kinetics of body parts and analyzing the actions based on these partial descriptors. We propose a joint sparse regression based learning method which utilizes the structured sparsity to model each action as a combination of multimodal features from a sparse set of body parts. To represent dynamics and appearance of parts, we employ a heterogeneous set of depth and skeleton based features. The proper structure of multimodal multipart features are formulated into the learning framework via the proposed hierarchical mixed norm, to regularize the structured features of each part and to apply sparsity between them, in favor of a group feature selection. Our experimental results expose the effectiveness of the proposed learning method in which it outperforms other methods in all three tested datasets while saturating one of them by achieving perfect accuracy. © 1979-2012 IEEE.","Action recognition; group feature selection; joint sparse regression; kinect; mixed norms; structured sparsity"
"Hare S., Golodetz S., Saffari A., Vineet V., Cheng M.-M., Hicks S.L., Torr P.H.S.","Struck: Structured Output Tracking with Kernels",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","10", 7360205,"2096","2109",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986269170&doi=10.1109%2fTPAMI.2015.2509974&partnerID=40&md5=fbdcdba2bf303b49dc74da6a710e4c5a","Adaptive tracking-by-detection methods are widely used in computer vision for tracking arbitrary objects. Current approaches treat the tracking problem as a classification task and use online learning techniques to update the object model. However, for these updates to happen one needs to convert the estimated object position into a set of labelled training examples, and it is not clear how best to perform this intermediate step. Furthermore, the objective for the classifier (label prediction) is not explicitly coupled to the objective for the tracker (estimation of object position). In this paper, we present a framework for adaptive visual object tracking based on structured output prediction. By explicitly allowing the output space to express the needs of the tracker, we avoid the need for an intermediate classification step. Our method uses a kernelised structured output support vector machine (SVM), which is learned online to provide adaptive tracking. To allow our tracker to run at high frame rates, we (a) introduce a budgeting mechanism that prevents the unbounded growth in the number of support vectors that would otherwise occur during tracking, and (b) show how to implement tracking on the GPU. Experimentally, we show that our algorithm is able to outperform state-of-the-art trackers on various benchmark videos. Additionally, we show that we can easily incorporate additional features and kernels into our framework, which results in increased tracking performance. © 1979-2012 IEEE.","budget maintenance; GPU-based tracking; structured output SVMs; Tracking-by-detection"
"Seshadri K., Savvides M.","Towards a Unified Framework for Pose, Expression, and Occlusion Tolerant Automatic Facial Alignment",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","10", 7346480,"2110","2122",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986260136&doi=10.1109%2fTPAMI.2015.2505301&partnerID=40&md5=8508df41cd9fecf8208764608a00e887","We propose a facial alignment algorithm that is able to jointly deal with the presence of facial pose variation, partial occlusion of the face, and varying illumination and expressions. Our approach proceeds from sparse to dense landmarking steps using a set of specific models trained to best account for the shape and texture variation manifested by facial landmarks and facial shapes across pose and various expressions. We also propose the use of a novel ℓ 1 -regularized least squares approach that we incorporate into our shape model, which is an improvement over the shape model used by several prior Active Shape Model (ASM) based facial landmark localization algorithms. Our approach is compared against several state-of-the-art methods on many challenging test datasets and exhibits a higher fitting accuracy on all of them. © 1979-2012 IEEE.","active shape models (ASMs); automatic facial landmark localization; Facial alignment; ℓ1-regularized least squares"
"Valera I., Ruiz F.J.R., Perez-Cruz F.","Infinite factorial unbounded-state hidden Markov model",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","9", 7322279,"1816","1828",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981346564&doi=10.1109%2fTPAMI.2015.2498931&partnerID=40&md5=95b60ecf962c629dea1d4940916b293f","There are many scenarios in artificial intelligence, signal processing or medicine, in which a temporal sequence consists of several unknown overlapping independent causes, and we are interested in accurately recovering those canonical causes. Factorial hidden Markov models (FHMMs) present the versatility to provide a good fit to these scenarios. However, in some scenarios, the number of causes or the number of states of the FHMM cannot be known or limited a priori. In this paper, we propose an infinite factorial unbounded-state hidden Markov model (IFUHMM), in which the number of parallel hidden Markovmodels (HMMs) and states in each HMM are potentially unbounded. We rely on a Bayesian nonparametric (BNP) prior over integer-valued matrices, in which the columns represent the Markov chains, the rows the time indexes, and the integers the state for each chain and time instant. First, we extend the existent infinite factorial binary-state HMM to allow for any number of states. Then, we modify this model to allow for an unbounded number of states and derive an MCMC-based inference algorithm that properly deals with the trade-off between the unbounded number of states and chains. We illustrate the performance of our proposed models in the power disaggregation problem. © 1979-2012 IEEE.","Bayesian nonparametrics; Gibbs sampling; hidden Markov models; reversible jump Markov chain Monte Carlo; slice sampling; Time series; variational inference"
"Du M., Chellappa R.","Face Association for Videos Using Conditional Random Fields and Max-Margin Markov Networks",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","9", 7317788,"1762","1773",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981285640&doi=10.1109%2fTPAMI.2015.2497689&partnerID=40&md5=265235592b7d8e2f1bc0a4b7316483ba","We address the video-based face association problem, in which one attempts to extract the face tracks of multiple subjects while maintaining label consistency. Traditional tracking algorithms have difficulty in handling this task, especially when challenging nuisance factors like motion blur, low resolution or significant camera motions are present. We demonstrate that contextual features, in addition to face appearance itself, play an important role in this case. We propose principled methods to combine multiple features using Conditional Random Fields and Max-Margin Markov networks to infer labels for the detected faces. Different from many existing approaches, our algorithms work in online mode and hence have a wider range of applications. We address issues such as parameter learning, inference and handling false positves/negatives that arise in the proposed approach. Finally, we evaluate our approach on several public databases. © 2015 IEEE.","conditional random field; contextual features; Face association; max-margin Markov networks; tracking by detection"
"Neumann L., Matas J.","Real-Time Lexicon-Free Scene Text Localization and Recognition",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","9", 7313008,"1872","1885",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981285560&doi=10.1109%2fTPAMI.2015.2496234&partnerID=40&md5=ae61740e939debc0e51cf4a441d9e553","An end-to-end real-time text localization and recognition method is presented. Its real-time performance is achieved by posing the character detection and segmentation problem as an efficient sequential selection from the set of Extremal Regions. The ER detector is robust against blur, low contrast and illumination, color and texture variation. In the first stage, the probability of each ER being a character is estimated using features calculated by a novel algorithm in constant time and only ERs with locally maximal probability are selected for the second stage, where the classification accuracy is improved using computationally more expensive features. A highly efficient clustering algorithm then groups ERs into text lines and an OCR classifier trained on synthetic fonts is exploited to label character regions. The most probable character sequence is selected in the last stage when the context of each character is known. The method was evaluated on three public datasets. On the ICDAR 2013 dataset the method achieves state-of-the-art results in text localization; on the more challenging SVT dataset, the proposed method significantly outperforms the state-of-the-art methods and demonstrates that the proposed pipeline can incorporate additional prior knowledge about the detected text. The proposed method was exploited as the baseline in the ICDAR 2015 Robust Reading competition, where it compares favourably to the state-of-the art. © 2016 IEEE.","end-to-end text recognition; photo OCR; scene text; Text-in-the wild"
"Wei Y., Xia W., Lin M., Huang J., Ni B., Dong J., Zhao Y., Yan S.","HCP: A flexible CNN framework for multi-label image classification",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","9", 7305792,"1901","1907",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981331874&doi=10.1109%2fTPAMI.2015.2491929&partnerID=40&md5=d133ec3f502d6682bd8b3279c60eecc1","Convolutional Neural Network (CNN) has demonstrated promising performance in single-label image classification tasks. However, how CNN best copes with multi-label images still remains an open problem, mainly due to the complex underlying object layouts and insufficient multi-label training images. In this work, we propose a flexible deep CNN infrastructure, called Hypotheses-CNN-Pooling (HCP), where an arbitrary number of object segment hypotheses are taken as the inputs, then a shared CNN is connected with each hypothesis, and finally the CNN output results from different hypotheses are aggregated with max pooling to produce the ultimate multi-label predictions. Some unique characteristics of this flexible deep CNN infrastructure include: 1) no ground-truth bounding box information is required for training; 2) the whole HCP infrastructure is robust to possibly noisy and/or redundant hypotheses; 3) the shared CNN is flexible and can be well pre-trained with a large-scale single-label image dataset, e.g., ImageNet; and 4) it may naturally output multi-label prediction results. Experimental results on Pascal VOC 2007 and VOC 2012 multi-label image datasets well demonstrate the superiority of the proposed HCP infrastructure over other state-of-the-arts. In particular, the mAP reaches 90.5% by HCP only and 93.2% after the fusion with our complementary result in [12] based on hand-crafted features on the VOC 2012 dataset. © 1979-2012 IEEE.","CNN; Deep Learning; Multi-label Classification"
"Wu T., Li B., Zhu S.-C.","Learning And-Or Model to Represent Context and Occlusion for Car Detection and Viewpoint Estimation",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","9", 7317819,"1829","1843",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981274424&doi=10.1109%2fTPAMI.2015.2497699&partnerID=40&md5=5524799f5a61c05b4ae911db6515f4ab","This paper presents a method for learning an And-Or model to represent context and occlusion for car detection and viewpoint estimation. The learned And-Or model represents car-to-car context and occlusion configurations at three levels: (i) spatially-aligned cars, (ii) single car under different occlusion configurations, and (iii) a small number of parts. The And-Or model embeds a grammar for representing large structural and appearance variations in a reconfigurable hierarchy. The learning process consists of two stages in a weakly supervised way (i.e., only bounding boxes of single cars are annotated). First, the structure of the And-Or model is learned with three components: (a) mining multi-car contextual patterns based on layouts of annotated single car bounding boxes, (b) mining occlusion configurations between single cars, and (c) learning different combinations of part visibility based on CAD simulations. The And-Or model is organized in a directed and acyclic graph which can be inferred by Dynamic Programming. Second, the model parameters (for appearance, deformation and bias) are jointly trained using Weak-Label Structural SVM. In experiments, we test our model on four car detection datasets - the KITTI dataset [1] , the PASCAL VOC2007 car dataset [2] , and two self-collected car datasets, namely the Street-Parking car dataset and the Parking-Lot car dataset, and three datasets for car viewpoint estimation - the PASCAL VOC2006 car dataset [2] , the 3D car dataset [3] , and the PASCAL3D+ car dataset [4]. Compared with state-of-the-art variants of deformable part-based models and other methods, our model achieves significant improvement consistently on the four detection datasets, and comparable performance on car viewpoint estimation. © 2015 IEEE.","and-or graph; Car detection; car viewpoint estimation; context; hierarchical model; occlusion modeling"
"Strelow D., Wang Q., Si L., Eriksson A.","General, nested, and constrained Wiberg minimization",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","9", 7293691,"1803","1815",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981301237&doi=10.1109%2fTPAMI.2015.2487987&partnerID=40&md5=96279ddc7907debcb7ed12f3fef80383","Wiberg matrix factorization breaks a matrix Y into low-rank factors U and V by solving for V in closed form given U, linearizing V(U) about U , and iteratively minimizing ∥Y - UV(U)∥2 with respect to U only. This approach factors the matrix while effectively removing V from the minimization. Recently Eriksson and van den Hengel extended this approach to L1, minimizing ∥Y - UV(U)∥1. We generalize their approach beyond factorization to minimize ∥Y - f(U, V)∥1 for more general functions f(U, V) that are nonlinear in each of two sets of variables. We demonstrate the idea with a practical Wiberg algorithm for L1 bundle adjustment. One Wiberg minimization can be nested inside another, effectively removing two of three sets of variables from a minimization. We demonstrate this idea with a nested Wiberg algorithm for L1 projective bundle adjustment, solving for camera matrices, points, and projective depths. Wiberg minimization also generalizes to handle nonlinear constraints, and we demonstrate this idea with Constrained Wiberg Minimization for Multiple Instance Learning (CWM-MIL), which removes one set of variables from the constrained optimization. Our experiments emphasize isolating the effect of Wiberg by comparing against the algorithm it modifies, successive linear programming. © 1979-2012 IEEE.","L1 minimization; Low-rank matrix factorization; multiple instance learning; structure-from-motion; successive linear programming"
"You S., Tan R.T., Kawakami R., Mukaigawa Y., Ikeuchi K.","Adherent raindrop modeling, detectionand removal in video",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","9", 7299675,"1721","1733",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981343215&doi=10.1109%2fTPAMI.2015.2491937&partnerID=40&md5=8560429f49588acef9c84eb9d25fecbf","Raindrops adhered to a windscreen or window glass can significantly degrade the visibility of a scene. Modeling, detecting and removing raindrops will, therefore, benefit many computer vision applications, particularly outdoor surveillance systems and intelligent vehicle systems. In this paper, a method that automatically detects and removes adherent raindrops is introduced. The core idea is to exploit the local spatio-temporal derivatives of raindrops. To accomplish the idea, we first model adherent raindrops using law of physics, and detect raindrops based on these models in combination with motion and intensity temporal derivatives of the input video. Having detected the raindrops, we remove them and restore the images based on an analysis that some areas of raindrops completely occludes the scene, and some other areas occlude only partially. For partially occluding areas, we restore them by retrieving as much as possible information of the scene, namely, by solving a blending function on the detected partially occluding areas using the temporal intensity derivative. For completely occluding areas, we recover them by using a video completion technique. Experimental results using various real videos show the effectiveness of our method. © 1979-2012 IEEE.","Outdoor vision; raindrop detection; raindrop removal; rainy scenes"
"Zhou F., De La Torre F.","Factorized Graph Matching",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","9", 7331665,"1774","1789",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981250109&doi=10.1109%2fTPAMI.2015.2501802&partnerID=40&md5=80072ea5a165d0be2a35335440fc9312","Graph matching (GM) is a fundamental problem in computer science, and it plays a central role to solve correspondence problems in computer vision. GM problems that incorporate pairwise constraints can be formulated as a quadratic assignment problem (QAP). Although widely used, solving the correspondence problem through GM has two main limitations: (1) the QAP is NP-hard and difficult to approximate; (2) GM algorithms do not incorporate geometric constraints between nodes that are natural in computer vision problems. To address aforementioned problems, this paper proposes factorized graph matching (FGM). FGM factorizes the large pairwise affinity matrix into smaller matrices that encode the local structure of each graph and the pairwise affinity between edges. Four are the benefits that follow from this factorization: (1) There is no need to compute the costly (in space and time) pairwise affinity matrix; (2) The factorization allows the use of a path-following optimization algorithm, that leads to improved optimization strategies and matching performance; (3) Given the factorization, it becomes straight-forward to incorporate geometric transformations (rigid and non-rigid) to the GM problem. (4) Using a matrix formulation for the GM problem and the factorization, it is easy to reveal commonalities and differences between different GM methods. The factorization also provides a clean connection with other matching algorithms such as iterative closest point; Experimental results on synthetic and real databases illustrate how FGM outperforms state-of-the-art algorithms for GM. The code is available at http://humansensing.cs.cmu.edu/fgm. © 2015 IEEE.","feature matching; Graph matching; iterative closet point method; quadratic assignment problem"
"Kaltwang S., Todorovic S., Pantic M.","Doubly Sparse Relevance Vector Machine for Continuous Facial Behavior Estimation",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","9", 7331650,"1748","1761",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981268345&doi=10.1109%2fTPAMI.2015.2501824&partnerID=40&md5=0ba633c459e8805a4ef1838f59b349b5","Certain inner feelings and physiological states like pain are subjective states that cannot be directly measured, but can be estimated from spontaneous facial expressions. Since they are typically characterized by subtle movements of facial parts, analysis of the facial details is required. To this end, we formulate a new regression method for continuous estimation of the intensity of facial behavior interpretation, called Doubly Sparse Relevance Vector Machine (DSRVM). DSRVM enforces double sparsity by jointly selecting the most relevant training examples (a.k.a. relevance vectors) and the most important kernels associated with facial parts relevant for interpretation of observed facial expressions. This advances prior work on multi-kernel learning, where sparsity of relevant kernels is typically ignored. Empirical evaluation on challenging Shoulder Pain videos, and the benchmark DISFA and SEMAINE datasets demonstrate that DSRVM outperforms competing approaches with a multi-fold reduction of running times in training and testing. © 2015 IEEE.","facial expressions; multiple kernel learning; Regression; relevance vector machine"
"Hauberg S.","Principal Curves on Riemannian Manifolds",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","9", 7312494,"1915","1921",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981285576&doi=10.1109%2fTPAMI.2015.2496166&partnerID=40&md5=487080ecbfdea412f1fac9a137fc3961","Euclidean statistics are often generalized to Riemannian manifolds by replacing straight-line interpolations with geodesic ones. While these Riemannian models are familiar-looking, they are restricted by the inflexibility of geodesics, and they rely on constructions which are optimal only in Euclidean domains. We consider extensions of Principal Component Analysis (PCA) to Riemannian manifolds. Classic Riemannian approaches seek a geodesic curve passing through the mean that optimizes a criteria of interest. The requirements that the solution both is geodesic and must pass through the mean tend to imply that the methods only work well when the manifold is mostly flat within the support of the generating distribution. We argue that instead of generalizing linear Euclidean models, it is more fruitful to generalize non-linear Euclidean models. Specifically, we extend the classic Principal Curves from Hastie & Stuetzle to data residing on a complete Riemannian manifold. We show that for elliptical distributions in the tangent of spaces of constant curvature, the standard principal geodesic is a principal curve. The proposed model is simple to compute and avoids many of the pitfalls of traditional geodesic approaches. We empirically demonstrate the effectiveness of the Riemannian principal curves on several manifolds and datasets. © 2016 IEEE.","differential geometry; Principal component analysis; principal curves; Riemannian manifolds"
"Yu M., Shao L., Zhen X., He X.","Local Feature Discriminant Projection",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","9", 7317789,"1908","1914",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981226140&doi=10.1109%2fTPAMI.2015.2497686&partnerID=40&md5=a549085b82598e5f126aa9f504cbd723","In this paper, we propose a novel subspace learning algorithm called Local Feature Discriminant Projection (LFDP) for supervised dimensionality reduction of local features. LFDP is able to efficiently seek a subspace to improve the discriminability of local features for classification. We make three novel contributions. First, the proposed LFDP is a general supervised subspace learning algorithm which provides an efficient way for dimensionality reduction of large-scale local feature descriptors. Second, we introduce the Differential Scatter Discriminant Criterion (DSDC) to the subspace learning of local feature descriptors which avoids the matrix singularity problem. Third, we propose a generalized orthogonalization method to impose on projections, leading to a more compact and less redundant subspace. Extensive experimental validation on three benchmark datasets including UIUC-Sports, Scene-15 and MIT Indoor demonstrates that the proposed LFDP outperforms other dimensionality reduction methods and achieves state-of-the-art performance for image classification. © 2016 IEEE.","Dimensionality reduction; fisher vector; image classification; image-to-class distance; local feature"
"Taghia J., Leijon A.","Variational Inference for Watson Mixture Model",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","9", 7322293,"1886","1900",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981276074&doi=10.1109%2fTPAMI.2015.2498935&partnerID=40&md5=a9280fa9288584b8c9e4396a81f5619a","This paper addresses modelling data using the Watson distribution. The Watson distribution is one of the simplest distributions for analyzing axially symmetric data. This distribution has gained some attention in recent years due to its modeling capability. However, its Bayesian inference is fairly understudied due to difficulty in handling the normalization factor. Recent development of Markov chain Monte Carlo (MCMC) sampling methods can be applied for this purpose. However, these methods can be prohibitively slow for practical applications. A deterministic alternative is provided by variational methods that convert inference problems into optimization problems. In this paper, we present a variational inference for Watson mixture models. First, the variational framework is used to side-step the intractability arising from the coupling of latent states and parameters. Second, the variational free energy is further lower bounded in order to avoid intractable moment computation. The proposed approach provides a lower bound on the log marginal likelihood and retains distributional information over all parameters. Moreover, we show that it can regulate its own complexity by pruning unnecessary mixture components while avoiding over-fitting. We discuss potential applications of the modeling with Watson distributions in the problem of blind source separation, and clustering gene expression data sets. © 2015 IEEE.","axially symmetric; Bayesian inference; blind source separation; clustering on the unit hypersphere; gene expression; mixture model; variational inference; Watson distribution"
"Li S., Ngan K.N., Paramesran R., Sheng L.","Real-Time Head Pose Tracking with Online Face Template Reconstruction",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","9", 7328312,"1922","1928",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981276091&doi=10.1109%2fTPAMI.2015.2500221&partnerID=40&md5=7a63290a99c72c85426fcfc24297cade","We propose a real-time method to accurately track the human head pose in the 3-dimensional (3D) world. Using a RGB-Depth camera, a face template is reconstructed by fitting a 3D morphable face model, and the head pose is determined by registering this user-specific face template to the input depth video. © 2015 IEEE.","deformable face model; Head pose tracking; iterative closest point; RGB-Depth camera"
"Chakraborty A., Das A., Roy-Chowdhury A.K.","Network consistent data association",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","9", 7299668,"1859","1871",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981321191&doi=10.1109%2fTPAMI.2015.2491922&partnerID=40&md5=a75a839e81c52d77e9d684c98d47124e","Existing data association techniques mostly focus on matching pairs of data-point sets and then repeating this process along space-time to achieve long term correspondences. However, in many problems such as person re-identification, a set of data-points may be observed at multiple spatio-temporal locations and/or by multiple agents in a network and simply combining the local pairwise association results between sets of data-points often leads to inconsistencies over the global space-time horizons. In this paper, we propose a Novel Network Consistent Data Association (NCDA) framework formulated as an optimization problem that not only maintains consistency in association results across the network, but also improves the pairwise data association accuracies. The proposed NCDA can be solved as a binary integer program leading to a globally optimal solution and is capable of handling the challenging data-association scenario where the number of data-points varies across different sets of instances in the network. We also present an online implementation of NCDA method that can dynamically associate new observations to already observed data-points in an iterative fashion, while maintaining network consistency. We have tested both the batch and the online NCDA in two application areas - person re-identification and spatio-temporal cell tracking and observed consistent and highly accurate data association results in all the cases. © 1979-2012 IEEE.","Data association; integer program; network consistency; person re-identification; spatio-temporal cell tracking"
"Kong Y., Fu Y.","Max-margin action prediction machine",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","9", 7299663,"1844","1858",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981322940&doi=10.1109%2fTPAMI.2015.2491928&partnerID=40&md5=1376bc175d1cb924acc9a98378b21f4a","The speed with which intelligent systems can react to an action depends on how soon it can be recognized. The ability to recognize ongoing actions is critical in many applications, for example, spotting criminal activity. It is challenging, since decisions have to be made based on partial videos of temporally incomplete action executions. In this paper, we propose a novel discriminative multi-scale kernelized model for predicting the action class from a partially observed video. The proposed model captures temporal dynamics of human actions by explicitly considering all the history of observed features as well as features in smaller temporal segments. A compositional kernel is proposed to hierarchically capture the relationships between partial observations as well as the temporal segments, respectively. We develop a new learning formulation, which elegantly captures the temporal evolution over time, and enforces the label consistency between segments and corresponding partial videos. We prove that the proposed learning formulation minimizes the upper bound of the empirical risk. Experimental results on four public datasets show that the proposed approach outperforms state-of-the-art action prediction methods. © 1979-2012 IEEE.","Action prediction; action recognition; composite kernel; sequential data; structured SVM"
"Dosovitskiy A., Fischer P., Springenberg J.T., Riedmiller M., Brox T.","Discriminative unsupervised feature learning with exemplar convolutional neural networks",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","9", 7312476,"1734","1747",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981328865&doi=10.1109%2fTPAMI.2015.2496141&partnerID=40&md5=44af504379547f43a7669c96d63d2ef7","Deep convolutional networks have proven to be very successful in learning task specific features that allow for unprecedented performance on various computer vision tasks. Training of such networks follows mostly the supervised learning paradigm, where sufficiently many input-output pairs are required for training. Acquisition of large training sets is one of the key challenges, when approaching a new task. In this paper, we aim for generic feature learning and present an approach for training a convolutional network using only unlabeled data. To this end, we train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. In contrast to supervised network training, the resulting feature representation is not class specific. It rather provides robustness to the transformations that have been applied during training. This generic feature representation allows for classification results that outperform the state of the art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101, Caltech-256). While features learned with our approach cannot compete with class specific features from supervised training on a classification task, we show that they are advantageous on geometric matching problems, where they also outperform the SIFT descriptor. © 1979-2012 IEEE.","Convolutional networks; descriptor matching; feature learning; image classification; unsupervised learning"
"Crispim-Junior C.F., Buso V., Avgerinakis K., Meditskos G., Briassouli A., Benois-Pineau J., Kompatsiaris I.Y., Brémond F.","Semantic Event Fusion of Different Visual Modality Concepts for Activity Recognition",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","8", 7423809,"1598","1611",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978646652&doi=10.1109%2fTPAMI.2016.2537323&partnerID=40&md5=394bfbd5c165b1fdd037976ec2dc1b3e","Combining multimodal concept streams from heterogeneous sensors is a problem superficially explored for activity recognition.Most studies explore simple sensors in nearly perfect conditions, where temporal synchronization is guaranteed.Sophisticated fusion schemes adopt problem-specific graphical representations of events that are generally deeply linked with their training data and focused on a single sensor.This paper proposes a hybrid framework between knowledge-driven and probabilistic-driven methods for event representation and recognition.It separates semantic modeling from raw sensor data by using an intermediate semantic representation, namely concepts.It introduces an algorithm for sensor alignment that uses concept similarity as a surrogate for the inaccurate temporal information of real life scenarios.Finally, it proposes the combined use of an ontology language, to overcome the rigidity of previous approaches at model definition, and a probabilistic interpretation for ontological models, which equips the framework with a mechanism to handle noisy and ambiguous concept observations, an ability that most knowledge-driven methods lack.We evaluate our contributions in multimodal recordings of elderly people carrying out IADLs.Results demonstrated that the proposed framework outperforms baseline methods both in event recognition performance and in delimiting the temporal boundaries of event instances. © 1979-2012 IEEE.","activity recognition; concept synchronization; Knowledge representation formalism and methods; multimedia perceptual system; uncertainty and probabilistic reasoning; vision and scene understanding"
"Von Marcard T., Pons-Moll G., Rosenhahn B.","Human Pose Estimation from Video and IMUs",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","8", 7393844,"1533","1547",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978427623&doi=10.1109%2fTPAMI.2016.2522398&partnerID=40&md5=eb4fb2e0d8925dcd940350783d9aa794","In this work, we present an approach to fuse video with sparse orientation data obtained from inertial sensors to improve and stabilize full-body human motion capture. Even though video data is a strong cue for motion analysis, tracking artifacts occur frequently due to ambiguities in the images, rapid motions, occlusions or noise. As a complementary data source, inertial sensors allow for accurate estimation of limb orientations even under fast motions. However, accurate position information cannot be obtained in continuous operation. Therefore, we propose a hybrid tracker that combines video with a small number of inertial units to compensate for the drawbacks of each sensor type: on the one hand, we obtain drift-free and accurate position information from video data and, on the other hand, we obtain accurate limb orientations and good performance under fast motions from inertial sensors. In several experiments we demonstrate the increased performance and stability of our human motion tracker. © 1979-2012 IEEE.","animation; Human pose estimation; IMU; inertial sensors; motion capture; multisensor fusion"
"Roudposhti K.K., Nunes U., Dias J.","Probabilistic social behavior analysis by exploring body motion-based patterns",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","8", 7312488,"1679","1691",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978763318&doi=10.1109%2fTPAMI.2015.2496209&partnerID=40&md5=db01d33fa9bc27fd5f8ab2a7bcd42047","Understanding human behavior through nonverbal-based features, is interesting in several applications such as surveillance, ambient assisted living and human-robot interaction. In this article in order to analyze human behaviors in social context, we propose a new approach which explores interrelations between body part motions in scenarios with people doing a conversation. The novelty of this method is that we analyze body motion-based features in frequency domain to estimate different human social patterns: Interpersonal Behaviors (IBs) and a Social Role (SR). To analyze the dynamics and interrelations of people's body motions, a human movement descriptor is used to extract discriminative features, and a multi-layer Dynamic Bayesian Network (DBN) technique is proposed to model the existent dependencies. Laban Movement Analysis (LMA) is a well-known human movement descriptor, which provides efficient mid-level information of human body motions. The mid-level information is useful to extract the complex interdependencies. The DBN technique is tested in different scenarios to model the mentioned complex dependencies. The study is applied for obtaining four IBs (Interest, Indicator, Empathy and Emphasis) to estimate one SR (Leading).The obtained results give a good indication of the capabilities of the proposed approach for people interaction analysis with potential applications in human-robot interaction. © 1979-2012 IEEE.","Bayesian approach; frequency domain; human movement analysis; social role; Social signal processing"
"Chang J.Y.","Nonparametric Feature Matching Based Conditional Random Fields for Gesture Recognition from Multi-Modal Video",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","8", 7384735,"1612","1625",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978435071&doi=10.1109%2fTPAMI.2016.2519021&partnerID=40&md5=7f74f2c92b6e5cc985249623ca205247","We present a new gesture recognition method that is based on the conditional random field (CRF) model using multiple feature matching. Our approach solves the labeling problem, determining gesture categories and their temporal ranges at the same time. A generative probabilistic model is formalized and probability densities are nonparametrically estimated by matching input features with a training dataset. In addition to the conventional skeletal joint-based features, the appearance information near the active hand in an RGB image is exploited to capture the detailed motion of fingers. The estimated likelihood function is then used as the unary term for our CRF model. The smoothness term is also incorporated to enforce the temporal coherence of our solution. Frame-wise recognition results can then be obtained by applying an efficient dynamic programming technique. To estimate the parameters of the proposed CRF model, we incorporate the structured support vector machine (SSVM) framework that can perform efficient structured learning by using large-scale datasets. Experimental results demonstrate that our method provides effective gesture recognition results for challenging real gesture datasets. By scoring 0.8563 in the mean Jaccard index, our method has obtained the state-of-the-art results for the gesture recognition track of the 2014 ChaLearn Looking at People (LAP) Challenge. © 1979-2012 IEEE.","conditional random field; Gesture recognition; nonparametric estimation; structured learning"
"Alameda-Pineda X., Staiano J., Subramanian R., Batrinca L., Ricci E., Lepri B., Lanz O., Sebe N.","SALSA: A novel dataset for multimodal group behavior analysis",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","8", 7313015,"1707","1720",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978760339&doi=10.1109%2fTPAMI.2015.2496269&partnerID=40&md5=44152168dc02054589ddd1fee3a96bf3","Studying free-standing conversational groups (FCGs) in unstructured social settings (e.g., cocktail party ) is gratifying due to the wealth of information available at the group (mining social networks) and individual (recognizing native behavioral and personality traits) levels. However, analyzing social scenes involving FCGs is also highly challenging due to the difficulty in extracting behavioral cues such as target locations, their speaking activity and head/body pose due to crowdedness and presence of extreme occlusions. To this end, we propose SALSA, a novel dataset facilitating multimodal and Synergetic sociAL Scene Analysis, and make two main contributions to research on automated social interaction analysis: (1) SALSA records social interactions among 18 participants in a natural, indoor environment for over 60 minutes, under the poster presentation and cocktail party contexts presenting difficulties in the form of low-resolution images, lighting variations, numerous occlusions, reverberations and interfering sound sources; (2) To alleviate these problems we facilitate multimodal analysis by recording the social interplay using four static surveillance cameras and sociometric badges worn by each participant, comprising the microphone, accelerometer, bluetooth and infrared sensors. In addition to raw data, we also provide annotations concerning individuals' personality as well as their position, head, body orientation and F-formation information over the entire event duration. Through extensive experiments with state-of-the-art approaches, we show (a) the limitations of current methods and (b) how the recorded multiple cues synergetically aid automatic analysis of social interactions. SALSA is available at http://tev.fbk.eu/salsa. © 1979-2012 IEEE.","F-formations; free-standing conversational groups; head and body pose; Multimodal group behavior analysis; multimodal social data sets; personality traits; speaker recognition; tracking"
"Wan J., Guo G., Li S.Z.","Explore Efficient Local Features from RGB-D Data for One-Shot Learning Gesture Recognition",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","8", 7368923,"1626","1639",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978426908&doi=10.1109%2fTPAMI.2015.2513479&partnerID=40&md5=c506169a85202a90f21c436635c48bd8","Availability of handy RGB-D sensors has brought about a surge of gesture recognition research and applications. Among various approaches, one shot learning approach is advantageous because it requires minimum amount of data. Here, we provide a thorough review about one-shot learning gesture recognition from RGB-D data and propose a novel spatiotemporal feature extracted from RGB-D data, namely mixed features around sparse keypoints (MFSK). In the review, we analyze the challenges that we are facing, and point out some future research directions which may enlighten researchers in this field. The proposed MFSK feature is robust and invariant to scale, rotation and partial occlusions. To alleviate the insufficiency of one shot training samples, we augment the training samples by artificially synthesizing versions of various temporal scales, which is beneficial for coping with gestures performed at varying speed. We evaluate the proposed method on the Chalearn gesture dataset (CGD). The results show that our approach outperforms all currently published approaches on the challenging data of CGD, such as translated, scaled and occluded subsets. When applied to the RGB-D datasets that are not one-shot (e.g., the Cornell Activity Dataset-60 and MSR Daily Activity 3D dataset), the proposed feature also produces very promising results under leave-one-out cross validation or one-shot learning. © 1979-2012 IEEE.","bag of visual words model; gesture reco gnition; One-shot learning; RGB-D data"
"Corneanu C.A., Simón M.O., Cohn J.F., Guerrero S.E.","Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression Recognition: History, Trends, and Affect-Related Applications",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","8", 7374704,"1548","1568",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978370387&doi=10.1109%2fTPAMI.2016.2515606&partnerID=40&md5=e03d991b8f43cd519c346877991eb4af","Facial expressions are an important way through which humans interact socially. Building a system capable of automatically recognizing facial expressions from images and video has been an intense field of study in recent years. Interpreting such expressions remains challenging and much research is needed about the way they relate to human affect. This paper presents a general overview of automatic RGB, 3D, thermal and multimodal facial expression analysis. We define a new taxonomy for the field, encompassing all steps from face detection to facial expression recognition, and describe and classify the state of the art methods accordingly. We also present the important datasets and the bench-marking of most influential methods. We conclude with a general discussion about trends, important questions and future lines of research. © 1979-2012 IEEE.","3D; affect; emotion recognition; Facial expression; multimodal; RGB; thermal"
"Yu M., Liu L., Shao L.","Structure-preserving binary representations for RGB-D action recognition",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","8", 7299652,"1651","1664",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978764118&doi=10.1109%2fTPAMI.2015.2491925&partnerID=40&md5=a30555abdd082ad2b9e1545510f3fbd3","In this paper, we propose a novel binary local representation for RGB-D video data fusion with a structure-preserving projection. Our contribution consists of two aspects. Toacquire a general feature for the video data, we convert the problem to describing the gradient fields of RGB and depth information of video sequences. With the local fluxes of the gradient fields, which include the orientation and the magnitude of the neighborhood of each point, a new kind of continuous local descriptor called Local Flux Feature(LFF) is obtained. Then the LFFs from RGB and depth channels are fused into a Hamming space via the Structure Preserving Projection (SPP). Specifically, an orthogonal projection matrix is applied to preserve the pairwise structure with a shape constraint to avoid the collapse of data structure in the projected space. Furthermore, a bipartite graph structure of data is taken into consideration, which is regarded as a higher level connection between samples and classes than the pairwise structure of local features. Theextensive experiments show not only the high efficiency of binary codes and the effectiveness of combining LFFs from RGB-D channels via SPP on various action recognition benchmarks of RGB-D data, but also the potential power of LFF for general action recognition. © 1979-2012 IEEE.","binary; dimensionality reduction; flux; local feature; RGB-D fusion; structure-preserving"
"Wu D., Pigou L., Kindermans P.-J., Le N.D.-H., Shao L., Dambre J., Odobez J.-M.","Deep Dynamic Neural Networks for Multimodal Gesture Segmentation and Recognition",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","8", 7423804,"1583","1597",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978655918&doi=10.1109%2fTPAMI.2016.2537340&partnerID=40&md5=3171f2f523070d00ad9ba086c3a193e9","This paper describes a novel method called Deep Dynamic Neural Networks (DDNN) for multimodal gesture recognition. A semi-supervised hierarchical dynamic framework based on a Hidden Markov Model (HMM) is proposed for simultaneous gesture segmentation and recognition where skeleton joint information, depth and RGB images, are the multimodal input observations. Unlike most traditional approaches that rely on the construction of complex handcrafted features, our approach learns high-level spatio-temporal representations using deep neural networks suited to the input modality: a Gaussian-Bernouilli Deep Belief Network (DBN) to handle skeletal dynamics, and a 3D Convolutional Neural Network (3DCNN) to manage and fuse batches of depth and RGB images. This is achieved through the modeling and learning of the emission probabilities of the HMM required to infer the gesture sequence. This purely data driven approach achieves a Jaccard index score of 0.81 in the ChaLearn LAP gesture spotting challenge. The performance is on par with a variety of state-of-the-art hand-tuned feature-based approaches and other learning-based methods, therefore opening the door to the use of deep learning techniques in order to further explore multimodal time series data. © 1979-2012 IEEE.","convolutional neural networks; deep belief networks; Deep learning; gesture recognition; hidden Markov models"
"Zhao R., Martinez A.M.","Labeled graph kernel for behavior analysis",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","8", 7274731,"1640","1650",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978761857&doi=10.1109%2fTPAMI.2015.2481404&partnerID=40&md5=b51aeaa58081b2d86b46d55831e3fbcb","Automatic behavior analysis from video is a major topic in many areas of research, including computer vision, multimedia, robotics, biology, cognitive science, social psychology, psychiatry, and linguistics. Two major problems are of interest when analyzing behavior. First, we wish to automatically categorize observed behaviors into a discrete set of classes (i.e., classification). For example, to determine word production from video sequences in sign language. Second, we wish to understand the relevance of each behavioral feature in achieving this classification (i.e., decoding). For instance, to know which behavior variables are used to discriminate between the words apple and onion in American Sign Language (ASL). The present paper proposes to model behavior using a labeled graph, where the nodes define behavioral features and the edges are labels specifying their order (e.g., before, overlaps, start). In this approach, classification reduces to a simple labeled graph matching. Unfortunately, the complexity of labeled graph matching grows exponentially with the number of categories we wish to represent. Here, we derive a graph kernel to quickly and accurately compute this graph similarity. This approach is very general and can be plugged into any kernel-based classifier. Specifically, we derive a Labeled Graph Support Vector Machine (LGSVM) and a Labeled Graph Logistic Regressor (LGLR) that can be readily employed to discriminate between many actions (e.g., sign language concepts). The derived approach can be readily used for decoding too, yielding invaluable information for the understanding of a problem (e.g., to know how to teach a sign language). The derived algorithms allow us to achieve higher accuracy results than those of state-of-the-art algorithms in a fraction of the time. We show experimental results on a variety of problems and datasets, including multimodal data. © 1979-2012 IEEE.","classification; computational model; decoding; Graph matching; kernel; multimodal"
"Panagakis Y., Nicolaou M.A., Zafeiriou S., Pantic M.","Robust correlated and individual component analysis",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","8", 7317798,"1665","1678",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978763651&doi=10.1109%2fTPAMI.2015.2497700&partnerID=40&md5=eb80c9749cd218e3a593fe7df580e769","Recovering correlated and individual components of two, possibly temporally misaligned, sets of data is a fundamental task in disciplines such as image, vision, and behavior computing, with application to problems such as multi-modal fusion (via correlated components), predictive analysis, and clustering (via the individual ones). Here, we study the extraction of correlated and individual components under real-world conditions, namely i) the presence of gross non-Gaussian noise and ii) temporally misaligned data. In this light, we propose a method for the Robust Correlated and Individual Component Analysis (RCICA) of two sets of data in the presence of gross, sparse errors. We furthermore extend RCICA in order to handle temporal incongruities arising in the data. To this end, two suitable optimization problems are solved. The generality of the proposed methods is demonstrated by applying them onto 4 applications, namely i) heterogeneous face recognition, ii) multi-modal feature fusion for human behavior analysis (i.e., audio-visual prediction of interest and conflict), iii) face clustering, and iv) thetemporal alignment of facial expressions. Experimental results on 2 synthetic and 7 real world datasets indicate the robustness and effectiveness of the proposed methodson these application domains, outperforming other state-of-the-art methods in the field. © 1979-2012 IEEE.","canonical correlation analysis; individual components; low-rank; Multi-modal analysis; sparsity; time warping"
"Sigalas M., Pateraki M., Trahanias P.","Full-Body Pose Tracking - The Top View Reprojection Approach",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","8", 7332960,"1569","1582",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978438183&doi=10.1109%2fTPAMI.2015.2502582&partnerID=40&md5=c1811306333d50679b8b9b332e2ab557","Recent introduction of low-cost depth cameras triggered a number of interesting works, pushing forward the state-of-the-art in human body pose extraction and tracking. However, despite the remarkable progress, many of the contemporary methods cope inadequately with complex scenarios, involving multiple interacting users, under the presence of severe inter- and intra-occlusions. In this work, we present a model-based approach for markerless articulated full body pose extraction and tracking in RGB-D sequences. A cylinder-based model is employed to represent the human body. For each body part a set of hypotheses is generated and tracked over time by a Particle Filter. To evaluate each hypothesis, we employ a novel metric that considers the reprojected Top View of the corresponding body part. The latter, in conjunction with depth information, effectively copes with difficult and ambiguous cases, such as severe occlusions. For evaluation purposes, we conducted several series of experiments using data from a public human action database, as well as own-collected data involving varying number of interacting users. The performance of the proposed method has been further compared against that of the Microsoft's Kinect SDK and NiTETM using ground truth information. The results obtained attest for the effectiveness of our approach. © 1979-2012 IEEE.","Human pose estimation; model-based; particle filtering; tracking"
"Wandt B., Ackermann H., Rosenhahn B.","3D Reconstruction of human motion from monocular image sequences",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","8", 7451280,"1505","1516",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978744027&doi=10.1109%2fTPAMI.2016.2553028&partnerID=40&md5=ea9f22d773b8a3d9ddff0123de790fa0","This article tackles the problem of estimating non-rigid human 3D shape and motion from image sequences taken by uncalibrated cameras. Similar to other state-of-the-art solutions we factorize 2D observations in camera parameters, base poses and mixing coefficients. Existing methods require sufficient camera motion during the sequence to achieve a correct 3D reconstruction. To obtain convincing 3D reconstructions from arbitrary camera motion, our method is based on a-priorly trained base poses. We show that strong periodic assumptions on the coefficients can be used to define an efficient and accurate algorithm for estimating periodic motion such as walking patterns. For the extension to non-periodic motion we propose a novel regularization term based on temporal bone length constancy. In contrast to other works, the proposed method does not use a predefined skeleton or anthropometric constraints and can handle arbitrary camera motion. We achieve convincing 3D reconstructions, even under the influence of noise and occlusions. Multiple experiments based on a 3D error metric demonstrate the stability of the proposed method. Compared to other state-of-the-art methods our algorithm shows a significant improvement. © 1979-2012 IEEE.","3D reconstruction; factorization; Human motion; structure and motion"
"Zhou F., De La Torre F.","Spatio-Temporal Matching for Human Pose Estimation in Video",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","8", 7399418,"1492","1504",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978426230&doi=10.1109%2fTPAMI.2016.2526002&partnerID=40&md5=de299cce7ab477bf380e028b4fa8da3b","Detection and tracking humans in videos have been long-standing problems in computer vision. Most successful approaches (e.g., deformable parts models) heavily rely on discriminative models to build appearance detectors for body joints and generative models to constrain possible body configurations (e.g., trees). While these 2D models have been successfully applied to images (and with less success to videos), a major challenge is to generalize these models to cope with camera views. In order to achieve view-invariance, these 2D models typically require a large amount of training data across views that is difficult to gather and time-consuming to label. Unlike existing 2D models, this paper formulates the problem of human detection in videos as spatio-temporal matching (STM) between a 3D motion capture model and trajectories in videos. Our algorithm estimates the camera view and selects a subset of tracked trajectories that matches the motion of the 3D model. The STM is efficiently solved with linear programming, and it is robust to tracking mismatches, occlusions and outliers. To the best of our knowledge this is the first paper that solves the correspondence between video and 3D motion capture data for human pose detection. Experiments on the CMU motion capture, Human3.6M, Berkeley MHAD and CMU MAD databases illustrate the benefits of our method over state-of-the-art approaches. © 1979-2012 IEEE.","dense trajectories; Human pose estimation; spatio-temporal bilinear model; trajectory matching"
"Ye M., Shen Y., Du C., Pan Z., Yang R.","Real-time simultaneous pose and shape estimation for articulated objects using a single depth camera",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","8", 7457693,"1517","1532",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978630545&doi=10.1109%2fTPAMI.2016.2557783&partnerID=40&md5=5d34341d35b0a36c8503a3d9b634eaf6","In this paper we present a novel real-time algorithm for simultaneous pose and shape estimation for articulated objects, such as human beings and animals. The key of our pose estimation component is to embed the articulated deformation model with exponential-maps-based parametrization into a Gaussian Mixture Model. Benefiting from this probabilistic measurement model, our algorithm requires no explicit point correspondences as opposed to most existing methods. Consequently, our approach is less sensitive to local minimum and handles fast and complex motions well. Moreover, our novel shape adaptation algorithm based on the same probabilistic model automatically captures the shape of the subjects during the dynamic pose estimation process. The personalized shape model in turn improves the tracking accuracy. Furthermore, we propose novel approaches to use either a mesh model or a sphere-set model as the template for both pose and shape estimation under this unified framework. Extensive evaluations on publicly available data sets demonstrate that our method outperforms most state-of-the-art pose estimation algorithms with large margin, especially in the case of challenging motions. Furthermore, our shape estimation method achieves comparable accuracy with state of the arts, yet requires neither statistical shape model nor extra calibration procedure. Our algorithm is not only accurate but also fast, we have implemented the entire processing pipeline on GPU. It can achieve up to 60 frames per second on a middle-range graphics card. © 1979-2012 IEEE.","depth cues; Generative pose tracking; motion; range data; real-time tracking; shape registration; surface fitting"
"Neverova N., Wolf C., Taylor G., Nebout F.","ModDrop: Adaptive multi-modal gesture recognition",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","8", 7169562,"1692","1706",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978743208&doi=10.1109%2fTPAMI.2015.2461544&partnerID=40&md5=6f1c3e7426d76f602cc0097f1ea034d0","We present a method for gesture detection and localisation based on multi-scale and multi-modal deep learning. Each visual modality captures spatial information at a particular spatial scale (such as motion of the upper body or a hand), and the whole system operates at three temporal scales. Key to our technique is a training strategy which exploits: i) careful initialization of individual modalities; and ii) gradual fusion involving random dropping of separate channels (dubbed ModDrop) for learning cross-modality correlations while preserving uniqueness of each modality-specific representation. We present experiments on the ChaLearn 2014 Looking at People Challenge gesture recognition track, in which we placed first out of 17 teams. Fusing multiple modalities at several spatial and temporal scales leads to a significant increase in recognition rates, allowing the model to compensate for errors of the individual classifiers as well as noise in the separate channels. Futhermore, the proposed ModDrop training technique ensures robustness of the classifier to missing signals in one or several channels to produce meaningful predictions from any number of available modalities. In addition, we demonstrate the applicability of the proposed fusion scheme to modalities of arbitrary nature by experiments on the same dataset augmented with audio. © 1979-2012 IEEE.","convolutional neural networks; deep learning; Gesture recognition; multi-modal learning"
"Bao C., Ji H., Quan Y., Shen Z.","Dictionary Learning for Sparse Coding: Algorithms and Convergence Analysis",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","7", 7293682,"1356","1369",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976502654&doi=10.1109%2fTPAMI.2015.2487966&partnerID=40&md5=c148a2b6507bb1e574bca3e30248b2d8","In recent years, sparse coding has been widely used in many applications ranging from image processing to pattern recognition. Most existing sparse coding based applications require solving a class of challenging non-smooth and non-convex optimization problems. Despite the fact that many numerical methods have been developed for solving these problems, it remains an open problem to find a numerical method which is not only empirically fast, but also has mathematically guaranteed strong convergence. In this paper, we propose an alternating iteration scheme for solving such problems. A rigorous convergence analysis shows that the proposed method satisfies the global convergence property: the whole sequence of iterates is convergent and converges to a critical point. Besides the theoretical soundness, the practical benefit of the proposed method is validated in applications including image restoration and recognition. Experiments show that the proposed method achieves similar results with less computation when compared to widely used methods such as K-SVD. © 2015 IEEE.","Convergence analysis; dictionary learning; non-convex optimization; sparse coding"
"Sironi A., Turetken E., Lepetit V., Fua P.","Multiscale Centerline Detection",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","7", 7172549,"1327","1341",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976503562&doi=10.1109%2fTPAMI.2015.2462363&partnerID=40&md5=be35218dffe60e81f902b51bf3005781","Finding the centerline and estimating the radius of linear structures is a critical first step in many applications, ranging from road delineation in 2D aerial images to modeling blood vessels, lung bronchi, and dendritic arbors in 3D biomedical image stacks. Existing techniques rely either on filters designed to respond to ideal cylindrical structures or on classification techniques. The former tend to become unreliable when the linear structures are very irregular while the latter often has difficulties distinguishing centerline locations from neighboring ones, thus losing accuracy. We solve this problem by reformulating centerline detection in terms of a regression problem. We first train regressors to return the distances to the closest centerline in scale-space, and we apply them to the input images or volumes. The centerlines and the corresponding scale then correspond to the regressors local maxima, which can be easily identified. We show that our method outperforms state-of-the-art techniques for various 2D and 3D datasets. Moreover, our approach is very generic and also performs well on contour detection. We show an improvement above recent contour detection algorithms on the BSDS500 dataset. © 2015 IEEE.","automated reconstruction; boundary detection; Centerline detection; linear structures; multiscale detection; neuron tracing; radial estimation; regression; road tracing"
"Hong S., Choi J., Feyereisl J., Han B., Davis L.S.","Joint Image Clustering and Labeling by Matrix Factorization",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","7", 7293679,"1411","1424",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976499089&doi=10.1109%2fTPAMI.2015.2487982&partnerID=40&md5=04a41efae7793838311174e762a9865c","We propose a novel algorithm to cluster and annotate a set of input images jointly, where the images are clustered into several discriminative groups and each group is identified with representative labels automatically. For these purposes, each input image is first represented by a distribution of candidate labels based on its similarity to images in a labeled reference image database. A set of these label-based representations are then refined collectively through a non-negative matrix factorization with sparsity and orthogonality constraints; the refined representations are employed to cluster and annotate the input images jointly. The proposed approach demonstrates performance improvements in image clustering over existing techniques, and illustrates competitive image labeling accuracy in both quantitative and qualitative evaluation. In addition, we extend our joint clustering and labeling framework to solving the weakly-supervised image classification problem and obtain promising results. © 2015 IEEE.","Image clustering; image labeling; label feature; Non-negative matrix factorization with sparsity and orthogonality constraints (SO-NMF)"
"Akata Z., Perronnin F., Harchaoui Z., Schmid C.","Label-Embedding for Image Classification",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","7", 7293699,"1425","1438",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976520273&doi=10.1109%2fTPAMI.2015.2487986&partnerID=40&md5=5650e8b1d9d2165a0f1727ce4ec654e5","Attributes act as intermediate representations that enable parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function that measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. Label embedding enjoys a built-in ability to leverage alternative sources of information instead of or in addition to attributes, such as, e.g., class hierarchies or textual descriptions. Moreover, label embedding encompasses the whole range of learning settings from zero-shot learning to regular learning with a large number of labeled examples. © 2015 IEEE.","Attributes; Fine Grained Image Classification; Image classification; Label Embedding; Subspace Learning"
"Carreira J., Vicente S., Agapito L., Batista J.","Lifting Object Detection Datasets into 3D",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","7", 7110617,"1342","1355",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976340358&doi=10.1109%2fTPAMI.2015.2435707&partnerID=40&md5=d4f1a2878907827865d115af2614b6d2","While data has certainly taken the center stage in computer vision in recent years, it can still be difficult to obtain in certain scenarios. In particular, acquiring ground truth 3D shapes of objects pictured in 2D images remains a challenging feat and this has hampered progress in recognition-based object reconstruction from a single image. Here we propose to bypass previous solutions such as 3D scanning or manual design, that scale poorly, and instead populate object category detection datasets semi-automatically with dense, per-object 3D reconstructions, bootstrapped from:(i) class labels, (ii) ground truth figure-ground segmentations and (iii) a small set of keypoint annotations. Our proposed algorithm first estimates camera viewpoint using rigid structure-from-motion and then reconstructs object shapes by optimizing over visual hull proposals guided by loose within-class shape similarity assumptions. The visual hull sampling process attempts to intersect an object's projection cone with the cones of minimal subsets of other similar objects among those pictured from certain vantage points. We show that our method is able to produce convincing per-object 3D reconstructions and to accurately estimate cameras viewpoints on one of the most challenging existing object-category detection datasets, PASCAL VOC. We hope that our results will re-stimulate interest on joint object recognition and 3D reconstruction from a single image. © 2015 IEEE.","Object reconstruction; structure-from-motion; viewpoint estimation; Visual hulls"
"Najafi A., Joudaki A., Fatemizadeh E.","Nonlinear Dimensionality Reduction via Path-Based Isometric Mapping",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","7", 7293680,"1452","1464",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976463227&doi=10.1109%2fTPAMI.2015.2487981&partnerID=40&md5=32e78921e75ffbf6eeb153e1bd6a861e","Nonlinear dimensionality reduction methods have demonstrated top-notch performance in many pattern recognition and image classification tasks. Despite their popularity, they suffer from highly expensive time and memory requirements, which render them inapplicable to large-scale datasets. To leverage such cases we propose a new method called ""Path-Based Isomap"". Similar to Isomap, we exploit geodesic paths to find the low-dimensional embedding. However, instead of preserving pairwise geodesic distances, the low-dimensional embedding is computed via a path-mapping algorithm. Due to the much fewer number of paths compared to number of data points, a significant improvement in time and memory complexity with a comparable performance is achieved. The method demonstrates state-of-the-art performance on well-known synthetic and real-world datasets, as well as in the presence of noise. © 2015 IEEE.","geodesic path; manifold learning; Nonlinear dimensionality reduction; Optimization criteria"
"Kamal A.T., Bappy J.H., Farrell J.A., Roy-Chowdhury A.K.","Distributed Multi-Target Tracking and Data Association in Vision Networks",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","7", 7286852,"1397","1410",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976489771&doi=10.1109%2fTPAMI.2015.2484339&partnerID=40&md5=05d904b3a38450370984642da68c08f0","Distributed algorithms have recently gained immense popularity. With regards to computer vision applications, distributed multi-target tracking in a camera network is a fundamental problem. The goal is for all cameras to have accurate state estimates for all targets. Distributed estimation algorithms work by exchanging information between sensors that are communication neighbors. Vision-based distributed multi-target state estimation has at least two characteristics that distinguishes it from other applications. First, cameras are directional sensors and often neighboring sensors may not be sensing the same targets, i.e., they are naive with respect to that target. Second, in the presence of clutter and multiple targets, each camera must solve a data association problem. This paper presents an information-weighted, consensus-based, distributed multi-target tracking algorithm referred to as the Multi-target Information Consensus (MTIC) algorithm that is designed to address both the naivety and the data association problems. It converges to the centralized minimum mean square error estimate. The proposed MTIC algorithm and its extensions to non-linear camera models, termed as the Extended MTIC (EMTIC), are robust to false measurements and limited resources like power, bandwidth and the real-time operational requirements. Simulation and experimental analysis are provided to support the theoretical results. © 2015 IEEE.","Camera networks; consensus; data association; distributed tracking"
"Pont-Tuset J., Marques F.","Supervised Evaluation of Image Segmentation and Object Proposal Techniques",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","7", 7274729,"1465","1478",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976520638&doi=10.1109%2fTPAMI.2015.2481406&partnerID=40&md5=5567ed0d3cd7793763ea38a90eb7686d","This paper tackles the supervised evaluation of image segmentation and object proposal algorithms. It surveys, structures, and deduplicates the measures used to compare both segmentation results and object proposals with a ground truth database; and proposes a new measure: the precision-recall for objects and parts. To compare the quality of these measures, eight state-of-the-art object proposal techniques are analyzed and two quantitative meta-measures involving nine state of the art segmentation methods are presented. The meta-measures consist in assuming some plausible hypotheses about the results and assessing how well each measure reflects these hypotheses. As a conclusion of the performed experiments, this paper proposes the tandem of precision-recall curves for boundaries and for objects-and-parts as the tool of choice for the supervised evaluation of image segmentation. We make the datasets and code of all the measures publicly available. © 2015 IEEE.","Image segmentation; meta-measures; object proposals; supervised evaluation"
"Aldoma A., Tombari F., Di Stefano L., Vincze M.","A Global Hypothesis Verification Framework for 3D Object Recognition in Clutter",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","7", 7299676,"1383","1396",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976402556&doi=10.1109%2fTPAMI.2015.2491940&partnerID=40&md5=4c188c55268fdd132e0c10fdd04586d6","Pipelines to recognize 3D objects despite clutter and occlusions usually end up with a final verification stage whereby recognition hypotheses are validated or dismissed based on how well they explain sensor measurements. Unlike previous work, we propose a Global Hypothesis Verification (GHV) approach which regards all hypotheses jointly so as to account for mutual interactions. GHV provides a principled framework to tackle the complexity of our visual world by leveraging on a plurality of recognition paradigms and cues. Accordingly, we present a 3D object recognition pipeline deploying both global and local 3D features as well as shape and color. Thereby, and facilitated by the robustness of the verification process, diverse object hypotheses can be gathered and weak hypotheses need not be suppressed too early to trade sensitivity for specificity. Experiments demonstrate the effectiveness of our proposal, which significantly improves over the state-of-art and attains ideal performance (no false negatives, no false positives) on three out of the six most relevant and challenging benchmark datasets. © 2015 IEEE.","3D object recognition; correspondence grouping; hypothesis verification; scene understanding"
"O'Toole M., Mather J., Kutulakos K.N.","3D Shape and Indirect Appearance by Structured Light Transport",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","7", 7442841,"1298","1312",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976444898&doi=10.1109%2fTPAMI.2016.2545662&partnerID=40&md5=0a1630c9c03adf75d48aba0b5f5c7522","We consider the problem of deliberately manipulating the direct and indirect light flowing through a time-varying, general scene in order to simplify its visual analysis. Our approach rests on a crucial link between stereo geometry and light transport: while direct light always obeys the epipolar geometry of a projector-camera pair, indirect light overwhelmingly does not. We show that it is possible to turn this observation into an imaging method that analyzes light transport in real time in the optical domain, prior to acquisition. This yields three key abilities that we demonstrate in an experimental camera prototype: (1) producing a live indirect-only video stream for any scene, regardless of geometric or photometric complexity; (2) capturing images that make existing structured-light shape recovery algorithms robust to indirect transport; and (3) turning them into one-shot methods for dynamic 3D shape capture. © 2016 IEEE.","coded exposure; coded illumination; direct/global separation; dynamic 3D shape capture; epipolar constraints; inter-reflections; Light transport; multi-path interference; primal-dual coding; structured light 3D scanning; subsurface scattering"
"Chandraker M.","The Information Available to a Moving Observer on Shape with Unknown, Isotropic BRDFs",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","7", 7274730,"1283","1297",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976506364&doi=10.1109%2fTPAMI.2015.2481415&partnerID=40&md5=9baf9b4e102d1ff46515bd8d6e54e3df","Psychophysical studies show motion cues inform about shape even with unknown reflectance. Recent works in computer vision have considered shape recovery for an object of unknown BRDF using light source or object motions. This paper proposes a theory that addresses the remaining problem of determining shape from the (small or differential) motion of the camera, for unknown isotropic BRDFs. Our theory derives a differential stereo relation that relates camera motion to surface depth, which generalizes traditional Lambertian assumptions. Under orthographic projection, we show differential stereo may not determine shape for general BRDFs, but suffices to yield an invariant for several restricted (still unknown) BRDFs exhibited by common materials. For the perspective case, we show that differential stereo yields the surface depth for unknown isotropic BRDF and unknown directional lighting, while additional constraints are obtained with restrictions on the BRDF or lighting. The limits imposed by our theory are intrinsic to the shape recovery problem and independent of choice of reconstruction method. We also illustrate trends shared by theories on shape from differential motion of light source, object or camera, to relate the hardness of surface reconstruction to the complexity of imaging setup. © 2015 IEEE.","differential theory; general BRDF; multiview stereo; Surface reconstruction"
"Schuler C.J., Hirsch M., Harmeling S., Scholkopf B.","Learning to Deblur",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","7", 7274732,"1439","1451",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976521325&doi=10.1109%2fTPAMI.2015.2481418&partnerID=40&md5=20f9434cfd8fe22520609f05b89388e7","We describe a learning-based approach to blind image deconvolution. It uses a deep layered architecture, parts of which are borrowed from recent work on neural network learning, and parts of which incorporate computations that are specific to image deconvolution. The system is trained end-to-end on a set of artificially generated training examples, enabling competitive performance in blind deconvolution, both with respect to quality and runtime. © 2016 IEEE.","machine learning; neural networks; Sharpening and deblurring"
"Saeidi R., Astudillo R.F., Kolossa D.","Uncertain LDA: Including Observation Uncertainties in Discriminative Transforms",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","7", 7274733,"1479","1488",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976515960&doi=10.1109%2fTPAMI.2015.2481420&partnerID=40&md5=5cb923831d19594f3176a23af90673c3","Linear discriminant analysis (LDA) is a powerful technique in pattern recognition to reduce the dimensionality of data vectors. It maximizes discriminability by retaining only those directions that minimize the ratio of within-class and between-class variance. In this paper, using the same principles as for conventional LDA, we propose to employ uncertainties of the noisy or distorted input data in order to estimate maximally discriminant directions. We demonstrate the efficiency of the proposed uncertain LDA on two applications using state-of-the-art techniques. First, we experiment with an automatic speech recognition task, in which the uncertainty of observations is imposed by real-world additive noise. Next, we examine a full-scale speaker recognition system, considering the utterance duration as the source of uncertainty in authenticating a speaker. The experimental results show that when employing an appropriate uncertainty estimation algorithm, uncertain LDA outperforms its conventional LDA counterpart. © 2015 IEEE.","LDA; linear discriminant analysis; speaker recognition; speech recognition; Uncertainty"
"Fu Y., Lam A., Sato I., Okabe T., Sato Y.","Reflectance and Fluorescence Spectral Recovery via Actively Lit RGB Images",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","7", 7115178,"1313","1326",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976407662&doi=10.1109%2fTPAMI.2015.2439270&partnerID=40&md5=8259b9c6e8104de717690fa920e3da89","In recent years, fluorescence analysis of scenes has received attention in computer vision. Fluorescence can provide additional information about scenes, and has been used in applications such as camera spectral sensitivity estimation, 3D reconstruction, and color relighting. In particular, hyperspectral images of reflective-fluorescent scenes provide a rich amount of data. However, due to the complex nature of fluorescence, hyperspectral imaging methods rely on specialized equipment such as hyperspectral cameras and specialized illuminants. In this paper, we propose a more practical approach to hyperspectral imaging of reflective-fluorescent scenes using only a conventional RGB camera and varied colored illuminants. The key idea of our approach is to exploit a unique property of fluorescence: the chromaticity of fluorescent emissions are invariant under different illuminants. This allows us to robustly estimate spectral reflectance and fluorescent emission chromaticity. We then show that given the spectral reflectance and fluorescent chromaticity, the fluorescence absorption and emission spectra can also be estimated. We demonstrate in results that all scene spectra can be accurately estimated from RGB images. Finally, we show that our method can be used to accurately relight scenes under novel lighting. © 2015 IEEE.","Fluorescent Chromaticity Invariance; Reflectance and Fluorescence Spectra Recovery; Varying Illumination"
"Swoboda P., Shekhovtsov A., Kappes J.H., Schnorr C., Savchynskyy B.","Partial Optimality by Pruning for MAP-Inference with General Graphical Models",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","7", 7297868,"1370","1382",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976522368&doi=10.1109%2fTPAMI.2015.2484327&partnerID=40&md5=8350387a3959f586400ed4189f93e575","We consider the energy minimization problem for undirected graphical models, also known as MAP-inference problem for Markov random fields which is NP-hard in general. We propose a novel polynomial time algorithm to obtain a part of its optimal non-relaxed integral solution. Our algorithm is initialized with variables taking integral values in the solution of a convex relaxation of the MAP-inference problem and iteratively prunes those, which do not satisfy our criterion for partial optimality. We show that our pruning strategy is in a certain sense theoretically optimal. Also empirically our method outperforms previous approaches in terms of the number of persistently labelled variables. The method is very general, as it is applicable to models with arbitrary factors of an arbitrary order and can employ any solver for the considered relaxed problem. Our method's runtime is determined by the runtime of the convex relaxation solver for the MAP-inference problem. © 2015 IEEE.","energy minimization; Local polytope; MAP-inference; Markov random fields; partial optimality; persistency"
"Yan J., Cho M., Zha H., Yang X., Chu S.M.","Multi-Graph Matching via Affinity Optimization with Graduated Consistency Regularization",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 7254216,"1228","1242",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969899005&doi=10.1109%2fTPAMI.2015.2477832&partnerID=40&md5=47662aaf7195ef227e830ecd429e7114","This paper addresses the problem of matching common node correspondences among multiple graphs referring to an identical or related structure. This multi-graph matching problem involves two correlated components: i) the local pairwise matching affinity across pairs of graphs; ii) the global matching consistency that measures the uniqueness of the pairwise matchings by different composition orders. Previous studies typically either enforce the matching consistency constraints in the beginning of an iterative optimization, which may propagate matching error both over iterations and across graph pairs; or separate affinity optimization and consistency enforcement into two steps. This paper is motivated by the observation that matching consistency can serve as a regularizer in the affinity objective function especially when the function is biased due to noises or inappropriate modeling. We propose composition-based multi-graph matching methods to incorporate the two aspects by optimizing the affinity score, meanwhile gradually infusing the consistency. We also propose two mechanisms to elicit the common inliers against outliers. Compelling results on synthetic and real images show the competency of our algorithms. © 2015 IEEE.","feature correspondence; Graph matching"
"Xu Y., Géraud T., Najman L.","Connected Filtering on Tree-Based Shape-Spaces",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 7273932,"1126","1140",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969844603&doi=10.1109%2fTPAMI.2015.2441070&partnerID=40&md5=c6b4b823071cbe4d7f95645c733e5547","Connected filters are well-known for their good contour preservation property. A popular implementation strategy relies on tree-based image representations: for example, one can compute an attribute characterizing the connected component represented by each node of the tree and keep only the nodes for which the attribute is sufficiently high. This operation can be seen as a thresholding of the tree, seen as a graph whose nodes are weighted by the attribute. Rather than being satisfied with a mere thresholding, we propose to expand on this idea, and to apply connected filters on this latest graph. Consequently, the filtering is performed not in the space of the image, but in the space of shapes built from the image. Such a processing of shape-space filtering is a generalization of the existing tree-based connected operators. Indeed, the framework includes the classical existing connected operators by attributes. It also allows us to propose a class of novel connected operators from the leveling family, based on non-increasing attributes. Finally, we also propose a new class of connected operators that we call morphological shapings. Some illustrations and quantitative evaluations demonstrate the usefulness and robustness of the proposed shape-space filters. © 2015 IEEE.","blood vessel segmentation; connected filtering; graph; Mathematical morphology; Max-tree; Min-tree; shape-space filtering; shapebased lower/upper leveling; shaping; tree of shapes"
"Xu X., Li W., Xu D., Tsang I.W.","Co-Labeling for Multi-View Weakly Labeled Learning",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 7243351,"1113","1125",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969765369&doi=10.1109%2fTPAMI.2015.2476813&partnerID=40&md5=3fe344ff11e73d31eab553dfdf8cda2b","It is often expensive and time consuming to collect labeled training samples in many real-world applications. To reduce human effort on annotating training samples, many machine learning techniques (e.g., semi-supervised learning (SSL), multi-instance learning (MIL), etc.) have been studied to exploit weakly labeled training samples. Meanwhile, when the training data is represented with multiple types of features, many multi-view learning methods have shown that classifiers trained on different views can help each other to better utilize the unlabeled training samples for the SSL task. In this paper, we study a new learning problem called multi-view weakly labeled learning, in which we aim to develop a unified approach to learn robust classifiers by effectively utilizing different types of weakly labeled multi-view data from a broad range of tasks including SSL, MIL and relative outlier detection (ROD). We propose an effective approach called co-labeling to solve the multi-view weakly labeled learning problem. Specifically, we model the learning problem on each view as a weakly labeled learning problem, which aims to learn an optimal classifier from a set of pseudo-label vectors generated by using the classifiers trained from other views. Unlike traditional co-training approaches using a single pseudo-label vector for training each classifier, our co-labeling approach explores different strategies to utilize the predictions from different views, biases and iterations for generating the pseudo-label vectors, making our approach more robust for real-world applications. Moreover, to further improve the weakly labeled learning on each view, we also exploit the inherent group structure in the pseudo-label vectors generated from different strategies, which leads to a new multi-layer multiple kernel learning problem. Promising results for text-based image retrieval on the NUS-WIDE dataset as well as news classification and text categorization on several real-world multi-view datasets clearly demonstrate that our proposed co-labeling approach achieves state-of-the-art performance for various multi-view weakly labeled learning problems including multi-view SSL, multi-view MIL and multi-view ROD. © 2015 IEEE.","multi-instance learning; multi-view learning; relative outlier detection; semi-supervised learning; weakly labeled learning"
"Korman S., Avidan S.","Coherency Sensitive Hashing",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 7254191,"1099","1112",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969753090&doi=10.1109%2fTPAMI.2015.2477814&partnerID=40&md5=bc9b0edce9e4abc3f49d32f42ccc2e95","Coherency Sensitive Hashing (CSH) extends Locality Sensitivity Hashing (LSH) and PatchMatch to quickly find matching patches between two images. LSH relies on hashing, which maps similar patches to the same bin, in order to find matching patches. PatchMatch, on the other hand, relies on the observation that images are coherent, to propagate good matches to their neighbors in the image plane, using random patch assignment to seed the initial matching. CSH relies on hashing to seed the initial patch matching and on image coherence to propagate good matches. In addition, hashing lets it propagate information between patches with similar appearance (i.e., map to the same bin). This way, information is propagated much faster because it can use similarity in appearance space or neighborhood in the image plane. As a result, CSH is at least three to four times faster than PatchMatch and more accurate, especially in textured regions, where reconstruction artifacts are most noticeable to the human eye. We verified CSH on a new, large scale, data set of 133 image pairs and experimented on several extensions, including: k nearest neighbor search, the addition of rotation and matching three dimensional patches in videos. © 2015 IEEE.","Image Matching; Nearest Neighbor Fields; Patch Matching; Video Matching"
"Yan Y., Ricci E., Subramanian R., Liu G., Lanz O., Sebe N.","A Multi-Task Learning Framework for Head Pose Estimation under Target Motion",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 7254213,"1070","1083",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969760936&doi=10.1109%2fTPAMI.2015.2477843&partnerID=40&md5=96435db0991b889b67a924edcf605dd8","Recently, head pose estimation (HPE) from low-resolution surveillance data has gained in importance. However, monocular and multi-view HPE approaches still work poorly under target motion, as facial appearance distorts owing to camera perspective and scale changes when a person moves around. To this end, we propose FEGA-MTL, a novel framework based on Multi-Task Learning (MTL) for classifying the head pose of a person who moves freely in an environment monitored by multiple, large field-of-view surveillance cameras. Upon partitioning the monitored scene into a dense uniform spatial grid, FEGA-MTL simultaneously clusters grid partitions into regions with similar facial appearance, while learning region-specific head pose classifiers. In the learning phase, guided by two graphs which a-priori model the similarity among (1) grid partitions based on camera geometry and (2) head pose classes, FEGA-MTL derives the optimal scene partitioning and associated pose classifiers. Upon determining the target's position using a person tracker at test time, the corresponding region-specific classifier is invoked for HPE. The FEGA-MTL framework naturally extends to a weakly supervised setting where the target's walking direction is employed as a proxy in lieu of head orientation. Experiments confirm that FEGA-MTL significantly outperforms competing single-task and multi-task learning methods in multi-view settings. © 2015 IEEE.","graph guided; head pose classification; multi-camera systems; Multi-task learning; video surveillance"
"Cinbis R.G., Verbeek J., Schmid C.","Approximate Fisher Kernels of Non-iid Image Models for Image Categorization",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 7286858,"1084","1098",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969795482&doi=10.1109%2fTPAMI.2015.2484342&partnerID=40&md5=dcda470a149b932c7a21a45c52c8e4b8","The bag-of-words (BoW) model treats images as sets of local descriptors and represents them by visual word histograms. The Fisher vector (FV) representation extends BoW, by considering the first and second order statistics of local descriptors. In both representations local descriptors are assumed to be identically and independently distributed (iid), which is a poor assumption from a modeling perspective. It has been experimentally observed that the performance of BoW and FV representations can be improved by employing discounting transformations such as power normalization. In this paper, we introduce non-iid models by treating the model parameters as latent variables which are integrated out, rendering all local regions dependent. Using the Fisher kernel principle we encode an image by the gradient of the data log-likelihood w.r.t. the model hyper-parameters. Our models naturally generate discounting effects in the representations; suggesting that such transformations have proven successful because they closely correspond to the representations obtained for non-iid models. To enable tractable computation, we rely on variational free-energy bounds to learn the hyper-parameters and to compute approximate Fisher kernels. Our experimental evaluation results validate that our models lead to performance improvements comparable to using power normalization, as employed in state-of-the-art feature aggregation methods. © 2015 IEEE.","Fisher kernels; image classification; object recognition; Statistical image representations"
"Garro V., Giachetti A.","Scale Space Graph Representation and Kernel Matching for Non Rigid and Textured 3D Shape Retrieval",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 7254190,"1258","1271",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969802012&doi=10.1109%2fTPAMI.2015.2477823&partnerID=40&md5=6d4a87c6c71549b948fad6e52600ac0c","In this paper we introduce a novel framework for 3D object retrieval that relies on tree-based shape representations (TreeSha) derived from the analysis of the scale-space of the Auto Diffusion Function (ADF) and on specialized graph kernels designed for their comparison. By coupling maxima of the Auto Diffusion Function with the related basins of attraction, we can link the information at different scales encoding spatial relationships in a graph description that is isometry invariant and can easily incorporate texture and additional geometrical information as node and edge features. Using custom graph kernels it is then possible to estimate shape dissimilarities adapted to different specific tasks and on different categories of models, making the procedure a powerful and flexible tool for shape recognition and retrieval. Experimental results demonstrate that the method can provide retrieval scores similar or better than state-of-the-art on textured and non textured shape retrieval benchmarks and give interesting insights on effectiveness of different shape descriptors and graph kernels. © 2015 IEEE.","Auto Diffusion Function; graph kernel; graph-based representation; matching; non-rigid shape retrieval; shape descriptors; textured 3D model retrieval"
"Demi M.","Contour Tracking with a Spatio-Temporal Intensity Moment",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 7265087,"1141","1154",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969884474&doi=10.1109%2fTPAMI.2015.2478438&partnerID=40&md5=ad9168e17fb37f84915c606ecd383093","Standard edge detection operators such as the Laplacian of Gaussian and the gradient of Gaussian can be used to track contours in image sequences. When using edge operators, a contour, which is determined on a frame of the sequence, is simply used as a starting contour to locate the nearest contour on the subsequent frame. However, the strategy used to look for the nearest edge points may not work when tracking contours of non isolated gray level discontinuities. In these cases, strategies derived from the optical flow equation, which look for similar gray level distributions, appear to be more appropriate since these can work with a lower frame rate than that needed for strategies based on pure edge detection operators. However, an optical flow strategy tends to propagate the localization errors through the sequence and an additional edge detection procedure is essential to compensate for such a drawback. In this paper a spatio-temporal intensity moment is proposed which integrates the two basic functions of edge detection and tracking. © 2015 IEEE.","Contour tracking; edge detection; intensity moments; optical flow"
"Inoue N., Shinoda K.","Fast Coding of Feature Vectors Using Neighbor-to-Neighbor Search",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 7274762,"1170","1184",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969801354&doi=10.1109%2fTPAMI.2015.2481390&partnerID=40&md5=9ead9239816551355e1432e5584f7750","Searching for matches to high-dimensional vectors using hard/soft vector quantization is the most computationally expensive part of various computer vision algorithms including the bag of visual word (BoW). This paper proposes a fast computation method, Neighbor-to-Neighbor (NTN) search [1], which skips some calculations based on the similarity of input vectors. For example, in image classification using dense SIFT descriptors, the NTN search seeks similar descriptors from a point on a grid to an adjacent point. Applications of the NTN search to vector quantization, a Gaussian mixture model, sparse coding, and a kernel codebook for extracting image or video representation are presented in this paper. We evaluated the proposed method on image and video benchmarks: the PASCAL VOC 2007 Classification Challenge and the TRECVID 2010 Semantic Indexing Task. NTN-VQ reduced the coding cost by 77.4 percent, and NTN-GMM reduced it by 89.3 percent, without any significant degradation in classification performance. © 2015 IEEE.","Gaussian mixture model; image classification; Neighbor-to-neighbor search; vector quantization; video semantic indexing"
"Demirkus M., Precup D., Clark J.J., Arbel T.","Hierarchical Spatio-Temporal Probabilistic Graphical Model with Multiple Feature Fusion for Binary Facial Attribute Classification in Real-World Face Videos",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 7274747,"1185","1203",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969760227&doi=10.1109%2fTPAMI.2015.2481396&partnerID=40&md5=d9e1d4b7935dd8257d65351f4176c00b","Recent literature shows that facial attributes, i.e., contextual facial information, can be beneficial for improving the performance of real-world applications, such as face verification, face recognition, and image search. Examples of face attributes include gender, skin color, facial hair, etc. How to robustly obtain these facial attributes (traits) is still an open problem, especially in the presence of the challenges of real-world environments: non-uniform illumination conditions, arbitrary occlusions, motion blur and background clutter. What makes this problem even more difficult is the enormous variability presented by the same subject, due to arbitrary face scales, head poses, and facial expressions. In this paper, we focus on the problem of facial trait classification in real-world face videos. We have developed a fully automatic hierarchical and probabilistic framework that models the collective set of frame class distributions and feature spatial information over a video sequence. The experiments are conducted on a large real-world face video database that we have collected, labelled and made publicly available. The proposed method is flexible enough to be applied to any facial classification problem. Experiments on a large, real-world video database McGillFaces [1] of 18,000 video frames reveal that the proposed framework outperforms alternative approaches, by up to 16.96 and 10.13%, for the facial attributes of gender and facial hair, respectively. © 2015 IEEE.","attribute classification; Bag-of-words; face; facial hair detection; facial trait; gender classification; hierarchical graphical model; Local invariant features; occlusion; probabilistic modelling; real-world environment; real-world face video; spatio-temporal model"
"Tao M.W., Su J.-C., Wang T.-C., Malik J., Ramamoorthi R.","Depth Estimation and Specular Removal for Glossy Surfaces Using Point and Line Consistency with Light-Field Cameras",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 7254196,"1155","1169",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969752525&doi=10.1109%2fTPAMI.2015.2477811&partnerID=40&md5=ee01c7b2c6f4b943deb6570e0264b9f6","Light-field cameras have now become available in both consumer and industrial applications, and recent papers have demonstrated practical algorithms for depth recovery from a passive single-shot capture. However, current light-field depth estimation methods are designed for Lambertian objects and fail or degrade for glossy or specular surfaces. The standard Lambertian photoconsistency measure considers the variance of different views, effectively enforcing point-consistency, i.e., that all views map to the same point in RGB space. This variance or point-consistency condition is a poor metric for glossy surfaces. In this paper, we present a novel theory of the relationship between light-field data and reflectance from the dichromatic model. We present a physically-based and practical method to estimate the light source color and separate specularity. We present a new photo consistency metric, line-consistency, which represents how viewpoint changes affect specular points. We then show how the new metric can be used in combination with the standard Lambertian variance or point-consistency measure to give us results that are robust against scenes with glossy surfaces. With our analysis, we can also robustly estimate multiple light source colors and remove the specular component from glossy objects. We show that our method outperforms current state-of-the-art specular removal and depth estimation algorithms in multiple real world scenarios using the consumer Lytro and Lytro Illum light field cameras. © 2015 IEEE.","3D reconstruction; dichromatic reflection model; Light fields; reflection components separation; specular-free image"
"Perrone D., Favaro P.","A Clearer Picture of Total Variation Blind Deconvolution",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 7254197,"1041","1055",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969769913&doi=10.1109%2fTPAMI.2015.2477819&partnerID=40&md5=428cc5412cb7455585ad21f035f9fbfc","Blind deconvolution is the problem of recovering a sharp image and a blur kernel from a noisy blurry image. Recently, there has been a significant effort on understanding the basic mechanisms to solve blind deconvolution. While this effort resulted in the deployment of effective algorithms, the theoretical findings generated contrasting views on why these approaches worked. On the one hand, one could observe experimentally that alternating energy minimization algorithms converge to the desired solution. On the other hand, it has been shown that such alternating minimization algorithms should fail to converge and one should instead use a so-called Variational Bayes approach. To clarify this conundrum, recent work showed that a good image and blur prior is instead what makes a blind deconvolution algorithm work. Unfortunately, this analysis did not apply to algorithms based on total variation regularization. In this manuscript, we provide both analysis and experiments to get a clearer picture of blind deconvolution. Our analysis reveals the very reason why an algorithm based on total variation works. We also introduce an implementation of this algorithm and show that, in spite of its extreme simplicity, it is very robust and achieves a performance comparable to the top performing algorithms. © 2015 IEEE.","blind deconvolution; Deblurring; total variation"
"Armanfard N., Reilly J.P., Komeili M.","Local Feature Selection for Data Classification",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 7265078,"1217","1227",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969761314&doi=10.1109%2fTPAMI.2015.2478471&partnerID=40&md5=de095b8d35648007895fea1ae5e3df67","Typical feature selection methods choose an optimal global feature subset that is applied over all regions of the sample space. In contrast, in this paper we propose a novel localized feature selection (LFS) approach whereby each region of the sample space is associated with its own distinct optimized feature set, which may vary both in membership and size across the sample space. This allows the feature set to optimally adapt to local variations in the sample space. An associated method for measuring the similarities of a query datum to each of the respective classes is also proposed. The proposed method makes no assumptions about the underlying structure of the samples; hence the method is insensitive to the distribution of the data over the sample space. The method is efficiently formulated as a linear programming optimization problem. Furthermore, we demonstrate the method is robust against the over-fitting problem. Experimental results on eleven synthetic and real-world data sets demonstrate the viability of the formulation and the effectiveness of the proposed algorithm. In addition we show several examples where localized feature selection produces better results than a global feature selection method. © 2015 IEEE.","Classification; Linear Programming; Local Feature Selection"
"Zheng Q., Kumar A., Pan G.","A 3D Feature Descriptor Recovered from a Single 2D Palmprint Image",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 7464743,"1272","1279",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969792544&doi=10.1109%2fTPAMI.2015.2509968&partnerID=40&md5=ea6c7b89c020c23ed34e33b15cd820dd","Design and development of efficient and accurate feature descriptors is critical for the success of many computer vision applications. This paper proposes a new feature descriptor, referred to as DoN, for the 2D palmprint matching. The descriptor is extracted for each point on the palmprint. It is based on the ordinal measure which partially describes the difference of the neighboring points' normal vectors. DoN has at least two advantages: 1) it describes the 3D information, which is expected to be highly stable under commonly occurring illumination variations during contactless imaging; 2) the size of DoN for each point is only one bit, which is computationally simple to extract, easy to match, and efficient to storage. We show that such 3D information can be extracted from a single 2D palmprint image. The analysis for the effectiveness of ordinal measure for palmprint matching is also provided. Four publicly available 2D palmprint databases are used to evaluate the effectiveness of DoN, both for identification and the verification. Our method on all these databases achieves the state-of-the-art performance. © 2015 IEEE.","3D feature from a single 2D image; biometrics; contactless palmprint matching; ordinal features; Palmprint recognition"
"Zheng Y., Zhang Y.-J., Larochelle H.","A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 7258387,"1056","1069",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969791928&doi=10.1109%2fTPAMI.2015.2476802&partnerID=40&md5=601f11c3605f2a12b5208ada13fa5709","Topic modeling based on latent Dirichlet allocation (LDA) has been a framework of choice to deal with multimodal data, such as in image annotation tasks. Another popular approach to model the multimodal data is through deep neural networks, such as the deep Boltzmann machine (DBM). Recently, a new type of topic model called the Document Neural Autoregressive Distribution Estimator (DocNADE) was proposed and demonstrated state-of-the-art performance for text document modeling. In this work, we show how to successfully apply and extend this model to multimodal data, such as simultaneous image classification and annotation. First, we propose SupDocNADE, a supervised extension of DocNADE, that increases the discriminative power of the learned hidden topic features and show how to employ it to learn a joint representation from image visual words, annotation words and class label information. We test our model on the LabelMe and UIUC-Sports data sets and show that it compares favorably to other topic models. Second, we propose a deep extension of our model and provide an efficient way of training the deep model. Experimental results show that our deep model outperforms its shallow version and reaches state-of-the-art performance on the Multimedia Information Retrieval (MIR) Flickr data set. © 2015 IEEE.","Deep neural network; Multimodal data modeling; Neural autoregressive model; Topic model"
"Loosli G., Canu S., Ong C.S.","Learning SVM in Kreǐn Spaces",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 7254195,"1204","1216",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969758513&doi=10.1109%2fTPAMI.2015.2477830&partnerID=40&md5=a70c687997e9f7de004e25a134d68421","This paper presents a theoretical foundation for an SVM solver in Kreǐn spaces. Up to now, all methods are based either on the matrix correction, or on non-convex minimization, or on feature-space embedding. Here we justify and evaluate a solution that uses the original (indefinite) similarity measure, in the original Kreǐn space. This solution is the result of a stabilization procedure. We establish the correspondence between the stabilization problem (which has to be solved) and a classical SVM based on minimization (which is easy to solve). We provide simple equations to go from one to the other (in both directions). This link between stabilization and minimization problems is the key to obtain a solution in the original Kreǐn space. Using KSVM, one can solve SVM with usually troublesome kernels (large negative eigenvalues or large numbers of negative eigenvalues). We show experiments showing that our algorithm KSVM outperforms all previously proposed approaches to deal with indefinite matrices in SVM-like kernel methods. © 2015 IEEE.","classification; dissimilarity; indefinite kernel; Krein spaces; stabilization problem; SVM"
"Paisitkriangkrai S., Shen C., Van Den Hengel A.","Pedestrian detection with spatially pooled features and structured ensemble learning",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","6", 2474388,"1243","1257",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969651337&doi=10.1109%2fTPAMI.2015.2474388&partnerID=40&md5=abc54a1855d6638e6d392992978021b9","Many typical applications of object detection operate within a prescribed false-positive range. In this situation the performance of a detector should be assessed on the basis of the area under the ROC curve over that range, rather than over the full curve, as the performance outside the prescribed range is irrelevant. This measure is labelled as the partial area under the ROC curve (pAUC). We propose a novel ensemble learning method which achieves a maximal detection rate at a user-defined range of false positive rates by directly optimizing the partial AUC using structured learning. In addition, in order to achieve high object detection performance, we propose a new approach to extracting low-level visual features based on spatial pooling. Incorporating spatial pooling improves the translational invariance and thus the robustness of the detection process. Experimental results on both synthetic and realworld data sets demonstrate the effectiveness of our approach, and we show that it is possible to train state-of-the-art pedestrian detectors using the proposed structured ensemble learning method with spatially pooled features. The result is the current best reported performance on the Caltech-USA pedestrian detection dataset. © 2015 IEEE.","Boosting; Ensemble learning; Pedestrian detection; Spatial pooling; Structured learning"
"Zhang Z., Luo P., Loy C.C., Tang X.","Learning Deep Representation for Face Alignment with Auxiliary Attributes",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7208848,"918","930",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963829815&doi=10.1109%2fTPAMI.2015.2469286&partnerID=40&md5=8d8ca419210a45c026b68f1e325eab0b","In this study, we show that landmark detection or face alignment task is not a single and independent problem. Instead, its robustness can be greatly improved with auxiliary information. Specifically, we jointly optimize landmark detection together with the recognition of heterogeneous but subtly correlated facial attributes, such as gender, expression, and appearance attributes. This is non-trivial since different attribute inference tasks have different learning difficulties and convergence rates. To address this problem, we formulate a novel tasks-constrained deep model, which not only learns the inter-task correlation but also employs dynamic task coefficients to facilitate the optimization convergence when learning multiple complex tasks. Extensive evaluations show that the proposed task-constrained learning (i) outperforms existing face alignment methods, especially in dealing with faces with severe occlusion and pose variation, and (ii) reduces model complexity drastically compared to the state-of-the-art methods based on cascaded deep model. © 2015 IEEE.","convolutional network; deep learning; Face Alignment; face landmark detection"
"Goulermas J.Y., Kostopoulos A., Mu T.","A New Measure for Analyzing and Fusing Sequences of Objects",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7214308,"833","848",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963823235&doi=10.1109%2fTPAMI.2015.2470671&partnerID=40&md5=3ee1e5455fe47e6effdbf1d52531c95b","This work is related to the combinatorial data analysis problem of seriation used for data visualization and exploratory analysis. Seriation re-sequences the data, so that more similar samples or objects appear closer together, whereas dissimilar ones are further apart. Despite the large number of current algorithms to realize such re-sequencing, there has not been a systematic way for analyzing the resulting sequences, comparing them, or fusing them to obtain a single unifying one. We propose a new positional proximity measure that evaluates the similarity of two arbitrary sequences based on their agreement on pairwise positional information of the sequenced objects. Furthermore, we present various statistical properties of this measure as well as its normalized version modeled as an instance of the generalized correlation coefficient. Based on this measure, we define a new procedure for consensus seriation that fuses multiple arbitrary sequences based on a quadratic assignment problem formulation and an efficient way of approximating its solution. We also derive theoretical links with other permutation distance functions and present their associated combinatorial optimization forms for consensus tasks. The utility of the proposed contributions is demonstrated through the comparison and fusion of multiple seriation algorithms we have implemented, using many real-world datasets from different application domains. © 2015 IEEE.","combinatorial data analysis; consensus/ensemble seriation; positional proximity coefficient; quadratic assignment problem; sequencing; Seriation"
"Lin Z., Huang Y.","Fast Multidimensional Ellipsoid-Specific Fitting by Alternating Direction Method of Multipliers",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7208883,"1021","1026",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963823231&doi=10.1109%2fTPAMI.2015.2469283&partnerID=40&md5=8acc9855efeb10af734c1b062c80e0eb","Many problems in computer vision can be formulated as multidimensional ellipsoid-specific fitting, which is to minimize the residual error such that the underlying quadratic surface is a multidimensional ellipsoid. In this paper, we present a fast and robust algorithm for solving ellipsoid-specific fitting directly. Our method is based on the alternating direction method of multipliers, which does not introduce extra positive semi-definiteness constraints. The computation complexity is thus significantly lower than those of semi-definite programming (SDP) based methods. More specifically, to fit n data points into a p dimensional ellipsoid, our complexity is O(p6 + np4)+O(p3) , where the former O results from preprocessing data once, while that of the state-of-the-art SDP method is O(p6 + np4 + n3/2p2) for each iteration. The storage complexity of our algorithm is about 1/2np2, which is at most 1/4 of those of SDP methods. Extensive experiments testify to the great speed and accuracy advantages of our method over the state-of-the-art approaches. The implementation of our method is also much simpler than SDP based methods. © 2015 IEEE.","alternating direction method of multipliers; ellipsoid-specific fitting; Multidimensional ellipsoid"
"Mudunuri S.P., Biswas S.","Low Resolution Face Recognition Across Variations in Pose and Illumination",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7208853,"1034","1040",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963805222&doi=10.1109%2fTPAMI.2015.2469282&partnerID=40&md5=f647a63f42e4ec7ae3e6233ac17b2ad1","We propose a completely automatic approach for recognizing low resolution face images captured in uncontrolled environment. The approach uses multidimensional scaling to learn a common transformation matrix for the entire face which simultaneously transforms the facial features of the low resolution and the high resolution training images such that the distance between them approximates the distance had both the images been captured under the same controlled imaging conditions. Stereo matching cost is used to obtain the similarity of two images in the transformed space. Though this gives very good recognition performance, the time taken for computing the stereo matching cost is significant. To overcome this limitation, we propose a reference-based approach in which each face image is represented by its stereo matching cost from a few reference images. Experimental evaluation on the real world challenging databases and comparison with the state-of-the-art super-resolution, classifier based and cross modal synthesis techniques show the effectiveness of the proposed algorithm. © 2015 IEEE.","Face recognition; low resolution; multidimensional scaling; stereo matching; super resolution"
"Solera F., Calderara S., Cucchiara R.","Socially Constrained Structural Learning for Groups Detection in Crowd",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7214317,"995","1008",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963825528&doi=10.1109%2fTPAMI.2015.2470658&partnerID=40&md5=881f9956c4554e3d14aca14d72f483e5","Modern crowd theories agree that collective behavior is the result of the underlying interactions among small groups of individuals. In this work, we propose a novel algorithm for detecting social groups in crowds by means of a Correlation Clustering procedure on people trajectories. The affinity between crowd members is learned through an online formulation of the Structural SVM framework and a set of specifically designed features characterizing both their physical and social identity, inspired by Proxemic theory, Granger causality, DTW and Heat-maps. To adhere to sociological observations, we introduce a loss function (G -MITRE) able to deal with the complexity of evaluating group detection performances. We show our algorithm achieves state-of-the-art results when relying on both ground truth trajectories and tracklets previously extracted by available detector/tracker systems. © 2015 IEEE.","Correlation Clustering; Crowd analysis; Granger causality; group detection; Proxemic theory; Structural SVM"
"Heller J., Havlena M., Pajdla T.","Globally Optimal Hand-Eye Calibration Using Branch-and-Bound",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7206586,"1027","1033",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963788166&doi=10.1109%2fTPAMI.2015.2469299&partnerID=40&md5=83b734ad68d9edeaf89c25595c528a07","This paper introduces a novel solution to the hand-eye calibration problem. It uses camera measurements directly and, at the same time, requires neither prior knowledge of the external camera calibrations nor a known calibration target. Our algorithm uses branch-and-bound approach to minimize an objective function based on the epipolar constraint. Further, it employs Linear Programming to decide the bounding step of the algorithm.Our technique is able to recover both the unknown rotation and translation simultaneously and the solution is guaranteed to be globally optimal with respect to the L∞-norm. © 2015 IEEE.","branch-and-bound algorithm; global optimization; Hand-eye calibration"
"Yoon J.H., Yang M.-H., Yoon K.-J.","Interacting Multiview Tracker",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7226831,"903","917",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963828940&doi=10.1109%2fTPAMI.2015.2473862&partnerID=40&md5=b8edb7cc6346b8c111ff96291850f5fc","A robust algorithm is proposed for tracking a target object in dynamic conditions including motion blurs, illumination changes, pose variations, and occlusions. To cope with these challenging factors, multiple trackers based on different feature representations are integrated within a probabilistic framework. Each view of the proposed multiview (multi-channel) feature learning algorithm is concerned with one particular feature representation of a target object from which a tracker is developed with different levels of reliability. With the multiple trackers, the proposed algorithm exploits tracker interaction and selection for robust tracking performance. In the tracker interaction, a transition probability matrix is used to estimate dependencies between trackers. Multiple trackers communicate with each other by sharing information of sample distributions. The tracker selection process determines the most reliable tracker with the highest probability. To account for object appearance changes, the transition probability matrix and tracker probability are updated in a recursive Bayesian framework by reflecting the tracker reliability measured by a robust tracker likelihood function that learns to account for both transient and stable appearance changes. Experimental results on benchmark datasets demonstrate that the proposed interacting multiview algorithm performs robustly and favorably against state-of-the-art methods in terms of several quantitative metrics. © 2015 IEEE.","multiple features; multiview representations; Object tracking; tracker interaction; transition probability matrix"
"Cherian A., Morellas V., Papanikolopoulos N.","Bayesian Nonparametric Clustering for Positive Definite Matrices",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7159063,"862","874",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963819642&doi=10.1109%2fTPAMI.2015.2456903&partnerID=40&md5=e20fa2cf2126abc43e82b922b4b088a8","Symmetric Positive Definite (SPD) matrices emerge as data descriptors in several applications of computer vision such as object tracking, texture recognition, and diffusion tensor imaging. Clustering these data matrices forms an integral part of these applications, for which soft-clustering algorithms (K-Means, expectation maximization, etc.) are generally used. As is well-known, these algorithms need the number of clusters to be specified, which is difficult when the dataset scales. To address this issue, we resort to the classical nonparametric Bayesian framework by modeling the data as a mixture model using the Dirichlet process (DP) prior. Since these matrices do not conform to the Euclidean geometry, rather belongs to a curved Riemannian manifold,existing DP models cannot be directly applied. Thus, in this paper, we propose a novel DP mixture model framework for SPD matrices. Using the log-determinant divergence as the underlying dissimilarity measure to compare these matrices, and further using the connection between this measure and the Wishart distribution, we derive a novel DPM model based on the Wishart-Inverse-Wishart conjugate pair. We apply this model to several applications in computer vision. Our experiments demonstrate that our model is scalable to the dataset size and at the same time achieves superior accuracy compared to several state-of-the-art parametric and nonparametric clustering algorithms. © 2015 IEEE.","Dirichlet process; nonparametric methods; positive definite matrices; Region covariances"
"Zhang J., Sclaroff S.","Exploiting Surroundedness for Saliency Detection: A Boolean Map Approach",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7226835,"889","902",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963837737&doi=10.1109%2fTPAMI.2015.2473844&partnerID=40&md5=2dcf19b6a6e5d8d322c4c209ccd5e172","We demonstrate the usefulness of surroundedness for eye fixation prediction by proposing a Boolean Map based Saliency model (BMS). In our formulation, an image is characterized by a set of binary images, which are generated by randomly thresholding the image's feature maps in a whitened feature space. Based on a Gestalt principle of figure-ground segregation, BMS computes a saliency map by discovering surrounded regions via topological analysis of Boolean maps. Furthermore, we draw a connection between BMS and the Minimum Barrier Distance to provide insight into why and how BMS can properly captures the surroundedness cue via Boolean maps. The strength of BMS is verified by its simplicity, efficiency and superior performance compared with 10 state-of-the-art methods on seven eye tracking benchmark datasets. © 2015 IEEE.","Boolean map; eye fixation prediction; minimum barrier distance; Saliency detection"
"Tau M., Hassner T.","Dense Correspondences across Scenes and Scales",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7229334,"875","888",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963812941&doi=10.1109%2fTPAMI.2015.2474356&partnerID=40&md5=6533c55cd0591ec6e8492fff5597d271","We seek a practical method for establishing dense correspondences between two images with similar content, but possibly different 3D scenes. One of the challenges in designing such a system is the local scale differences of objects appearing in the two images. Previous methods often considered only few image pixels; matching only pixels for which stable scales may be reliably estimated. Recently, others have considered dense correspondences, but with substantial costs associated with generating, storing and matching scale invariant descriptors. Our work is motivated by the observation that pixels in the image have contexts - the pixels around them - which may be exploited in order to reliably estimate local scales. We make the following contributions. (i) We show that scales estimated in sparse interest points may be propagated to neighboring pixels where this information cannot be reliably determined. Doing so allows scale invariant descriptors to be extracted anywhere in the image. (ii) We explore three means for propagating this information: using the scales at detected interest points, using the underlying image information to guide scale propagation in each image separately, and using both images together. Finally, (iii), we provide extensive qualitative and quantitative results, demonstrating that scale propagation allows for accurate dense correspondences to be obtained even between very different images, with little computational costs beyond those required by existing methods. © 1979-2012 IEEE.","feature representation; Image representation"
"Li X., Shen C., Dick A., Zhang Z.M., Zhuang Y.","Online Metric-Weighted Linear Representations for Robust Visual Tracking",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7206595,"931","950",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963905016&doi=10.1109%2fTPAMI.2015.2469276&partnerID=40&md5=6c7529da5d80979fd843321b10b21016","In this paper, we propose a visual tracker based on a metric-weighted linear representation of appearance. In order to capture the interdependence of different feature dimensions, we develop two online distance metric learning methods using proximity comparison information and structured output learning. The learned metric is then incorporated into a linear representation of appearance. We show that online distance metric learning significantly improves the robustness of the tracker, especially on those sequences exhibiting drastic appearance changes. In order to bound growth in the number of training samples, we design a time-weighted reservoir sampling method. Moreover, we enable our tracker to automatically perform object identification during the process of object tracking, by introducing a collection of static template samples belonging to several object classes of interest. Object identification results for an entire video sequence are achieved by systematically combining the tracking information and visual recognition at each frame. Experimental results on challenging video sequences demonstrate the effectiveness of the method for both inter-frame tracking and object identification. © 2015 IEEE.","linear representation; reservoir sampling; structured metric learning; Visual tracking"
"Fu Y., Lam A., Sato I., Okabe T., Sato Y.","Separating Reflective and Fluorescent Components Using High Frequency Illumination in the Spectral Domain",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7226850,"965","978",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963805175&doi=10.1109%2fTPAMI.2015.2473839&partnerID=40&md5=e7b27b80b0d922d9cb57367eb41116ed","Hyperspectral imaging is beneficial to many applications but most traditional methods do not consider fluorescent effects which are present in everyday items ranging from paper to even our food. Furthermore, everyday fluorescent items exhibit a mix of reflection and fluorescence so proper separation of these components is necessary for analyzing them. In recent years, effective imaging methods have been proposed but most require capturing the scene under multiple illuminants. In this paper, we demonstrate efficient separation and recovery of reflectance and fluorescence emission spectra through the use of two high frequency illuminations in the spectral domain. With the obtained fluorescence emission spectra from our high frequency illuminants, we then describe how to estimate the fluorescence absorption spectrum of a material given its emission spectrum. In addition, we provide an in depth analysis of our method and also show that filters can be used in conjunction with standard light sources to generate the required high frequency illuminants. We also test our method under ambient light and demonstrate an application of our method to synthetic relighting of real scenes. © 1979-2012 IEEE.","Fluorescence absorption and emission spectra; high frequency illumination; reflectance spectra"
"Saremi S., Sejnowski T.J.","Correlated Percolation, Fractal Structures, and Scale-Invariant Distribution of Clusters in Natural Images",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7274754,"1016","1020",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963799892&doi=10.1109%2fTPAMI.2015.2481402&partnerID=40&md5=f90c5539a63f5598933ca7a70b0edd19","Natural images are scale invariant with structures at all length scales.We formulated a geometric view of scale invariance in natural images using percolation theory, which describes the behavior of connected clusters on graphs.We map images to the percolation model by defining clusters on a binary representation for images. We show that critical percolating structures emerge in natural images and study their scaling properties by identifying fractal dimensions and exponents for the scale-invariant distributions of clusters. This formulation leads to a method for identifying clusters in images from underlying structures as a starting point for image segmentation. © 1979-2012 IEEE.","fractal structures; image segmentation; Natural image statistics; percolation theory; scale invariance"
"Seth S., Eugster M.J.A.","Archetypal Analysis for Nominal Observations",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7214318,"849","861",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963831013&doi=10.1109%2fTPAMI.2015.2470655&partnerID=40&md5=768b3abe6bebad5d627438688a4e8bf8","Archetypal analysis is a popular exploratory tool that explains a set of observations as compositions of few 'pure' patterns. The standard formulation of archetypal analysis addresses this problem for real valued observations by finding the approximate convex hull. Recently, a probabilistic formulation has been suggested which extends this framework to other observation types such as binary and count. In this article we further extend this framework to address the general case of nominal observations which includes, for example, multiple-option questionnaires. We view archetypal analysis in a generative framework: this allows explicit control over choosing a suitable number of archetypes by assigning appropriate prior information, and finding efficient update rules using variational Bayes'. We demonstrate the efficacy of this approach extensively on simulated data, and three real world examples: Austrian guest survey dataset, German credit dataset, and SUN attribute image dataset. © 2015 IEEE.","Archetypal analysis; clustering; nominal observations; prototype; simplex visualization; variational Bayes"
"Agudo A., Moreno-Noguer F., Calvo B., Montiel J.M.M.","Sequential Non-Rigid Structure from Motion Using Physical Priors",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7208859,"979","994",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963799893&doi=10.1109%2fTPAMI.2015.2469293&partnerID=40&md5=a713ed813950794900475e551560792c","We propose a new approach to simultaneously recover camera pose and 3D shape of non-rigid and potentially extensible surfaces from a monocular image sequence. For this purpose, we make use of the Extended Kalman Filter based Simultaneous Localization And Mapping (EKF-SLAM) formulation, a Bayesian optimization framework traditionally used in mobile robotics for estimating camera pose and reconstructing rigid scenarios. In order to extend the problem to a deformable domain we represent the object's surface mechanics by means of Navier's equations, which are solved using a Finite Element Method (FEM). With these main ingredients, we can further model the material's stretching, allowing us to go a step further than most of current techniques, typically constrained to surfaces undergoing isometric deformations. We extensively validate our approach in both real and synthetic experiments, and demonstrate its advantages with respect to competing methods. More specifically, we show that besides simultaneously retrieving camera pose and non-rigid shape, our approach is adequate for both isometric and extensible surfaces, does not require neither batch processing all the frames nor tracking points over the whole sequence and runs at several frames per second. © 2015 IEEE.","Extended Kalman Filter; Finite Element Method; Non-Rigid Structure from Motion; tracking"
"He X., Zhang C., Zhang L., Li X.","A-Optimal Projection for Image Representation",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7115188,"1009","1015",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964545783&doi=10.1109%2fTPAMI.2015.2439252&partnerID=40&md5=fc65ac8722d935a548e8dcc79182c799","We consider the problem of image representation from the perspective of statistical design. Recent studies have shown that images are possibly sampled from a low dimensional manifold despite of the fact that the ambient space is usually very high dimensional. Learning low dimensional image representations is crucial for many image processing tasks such as recognition and retrieval. Most of the existing approaches for learning low dimensional representations, such as principal component analysis (PCA) and locality preserving projections (LPP), aim at discovering the geometrical or discriminant structures in the data. In this paper, we take a different perspective from statistical experimental design, and propose a novel dimensionality reduction algorithm called A-Optimal Projection (AOP). AOP is based on a linear regression model. Specifically, AOP finds the optimal basis functions so that the expected prediction error of the regression model can be minimized if the new representations are used for training the model. Experimental results suggest that the proposed approach provides a better representation and achieves higher accuracy in image retrieval. © 1979-2012 IEEE.","Dimensionality reduction; image representation; optimal design"
"Seyedhosseini M., Tasdizen T.","Semantic Image Segmentation with Contextual Hierarchical Models",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","5", 7226830,"951","964",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963853275&doi=10.1109%2fTPAMI.2015.2473846&partnerID=40&md5=930a08805d1f930018750be914743874","Semantic segmentation is the problem of assigning an object label to each pixel. It unifies the image segmentation and object recognition problems. The importance of using contextual information in semantic segmentation frameworks has been widely realized in the field. We propose a contextual framework, called contextual hierarchical model (CHM), which learns contextual information in a hierarchical framework for semantic segmentation. At each level of the hierarchy, a classifier is trained based on downsampled input images and outputs of previous levels. Our model then incorporates the resulting multi-resolution contextual information into a classifier to segment the input image at original resolution. This training strategy allows for optimization of a joint posterior probability at multiple resolutions through the hierarchy. Contextual hierarchical model is purely based on the input image patches and does not make use of any fragments or shape examples. Hence, it is applicable to a variety of problems such as object segmentation and edge detection. We demonstrate that CHM performs at par with state-of-the-art on Stanford background and Weizmann horse datasets. It also outperforms state-of-the-art edge detection methods on NYU depth dataset and achieves state-of-the-art on Berkeley segmentation dataset (BSDS 500). © 2015 IEEE.","connectome; edge detection; hierarchical models; image segmentation; membrane detection; Semantic segmentation"
"Song S., Chandraker M., Guest C.C.","High Accuracy Monocular SFM and Scale Correction for Autonomous Driving",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","4", 7206590,"730","743",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963746392&doi=10.1109%2fTPAMI.2015.2469274&partnerID=40&md5=7d9fd2d6b6929459062ef51e5a883f7e","We present a real-time monocular visual odometry system that achieves high accuracy in real-world autonomous driving applications. First, we demonstrate robust monocular SFM that exploits multithreading to handle driving scenes with large motions and rapidly changing imagery. To correct for scale drift, we use known height of the camera from the ground plane. Our second contribution is a novel data-driven mechanism for cue combination that allows highly accurate ground plane estimation by adapting observation covariances of multiple cues, such as sparse feature matching and dense inter-frame stereo, based on their relative confidences inferred from visual data on a per-frame basis. Finally, we demonstrate extensive benchmark performance and comparisons on the challenging KITTI dataset, achieving accuracy comparable to stereo and exceeding prior monocular systems. Our SFM system is optimized to output pose within 50 ms in the worst case, while average case operation is over 30 fps. Our framework also significantly boosts the accuracy of applications like object localization that rely on the ground plane. © 2015 IEEE.","Ground plane estimation; Monocular structure-from-motion; Object localization; Scale drift"
"Deng J., Krause J., Stark M., Fei-Fei L.","Leveraging the wisdom of the crowd for fine-grained recognition",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","4", 7115172,"666","676",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963610846&doi=10.1109%2fTPAMI.2015.2439285&partnerID=40&md5=0059a955474e960735b700b26a14c6bd","Fine-grained recognition concerns categorization at sub-ordinate levels, where the distinction between object classes is highly local. Compared to basic level recognition, fine-grained categorization can be more challenging as there are in general less data and fewer discriminative features. This necessitates the use of a stronger prior for feature selection. In this work, we include humans in the loop to help computers select discriminative features. We introduce a novel online game called 'Bubbles' that reveals discriminative features humans use. The player's goal is to identify the category of a heavily blurred image. During the game, the player can choose to reveal full details of circular regions ('bubbles'), with a certain penalty. With proper setup the game generates discriminative bubbles with assured quality. We next propose the 'BubbleBank' representation that uses the human selected bubbles to improve machine recognition performance. Finally, we demonstrate how to extend BubbleBank to a view-invariant 3D representation. Experiments demonstrate that our approach yields large improvements over the previous state of the art on challenging benchmarks. © 1979-2012 IEEE.","Crowdsourcing; Gamification; Human Computation; Object Recognition"
"Martins P., Henriques J.F., Caseiro R., Batista J.","Bayesian constrained local models revisited",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","4", 7172556,"704","716",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963734699&doi=10.1109%2fTPAMI.2015.2462343&partnerID=40&md5=c1eebbce263935168da61084f3324904","This paper presents a novel Bayesian formulation for aligning faces in unseen images. Our approach revisits the Constrained Local Models (CLM) formulation where an ensemble of local feature detectors are constrained to lie within the subspace spanned by a Point Distribution Model (PDM). Fitting such a model to an image typically involves two main steps: a local search using a detector, obtaining response maps for each landmark (likelihood term) and a global optimization that finds the PDM parameters that jointly maximize all the detections at once. The so-called global optimization can be posed as a Bayesian inference problem, where the posterior distribution of the shape (and pose) parameters can be inferred in a maximum a posteriori (MAP) sense. This work introduces an extended Bayesian global optimization strategy that includes two novel additions: (1) to perform second order updates of the PDM parameters (accounting for their covariance) and (2) to model the underlying dynamics of the shape variations, encoded in the prior term, by using recursive Bayesian estimation. Extensive evaluations were performed against state-of-the-art methods on several standard datasets (IMM, BioID, XM2VTS, LFW and FGNET Talking Face). Results show that the proposed approach significantly increases the fitting performance. © 1979-2012 IEEE.","Active Shape Models (ASM); Constrained Local Models (CLM); Face registration; Non-rigid face alignment"
"Oh T.-H., Tai Y.-W., Bazin J.-C., Kim H., Kweon I.S.","Partial sum minimization of singular values in robust PCA: Algorithm and applications",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","4", 7182339,"744","758",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963759984&doi=10.1109%2fTPAMI.2015.2465956&partnerID=40&md5=2ecf964979c5f959412c92f8e2a44322","Robust Principal Component Analysis (RPCA) via rank minimization is a powerful tool for recovering underlying low-rank structure of clean data corrupted with sparse noise/outliers. In many low-level vision problems, not only it is known that the underlying structure of clean data is low-rank, but the exact rank of clean data is also known. Yet, when applying conventional rank minimization for those problems, the objective function is formulated in a way that does not fully utilize a priori target rank information about the problems. This observation motivates us to investigate whether there is a better alternative solution when using rank minimization. In this paper, instead of minimizing the nuclear norm, we propose to minimize the partial sum of singular values, which implicitly encourages the target rank constraint. Our experimental analyses show that, when the number of samples is deficient, our approach leads to a higher success rate than conventional rank minimization, while the solutions obtained by the two approaches are almost identical when the number of samples is more than sufficient. We apply our approach to various low-level vision problems, e.g., high dynamic range imaging, motion edge detection, photometric stereo, image alignment and recovery, and show that our results outperform those obtained by the conventional nuclear norm rank minimization method. © 1979-2012 IEEE.","Alternating direction method of multipliers; Rank minimization; Robust principal component analysis; Sparse and low-rank decomposition; Truncated nuclear norm"
"Feng L., Bhanu B.","Semantic Concept Co-Occurrence Patterns for Image Annotation and Retrieval",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","4", 7206599,"785","799",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963781889&doi=10.1109%2fTPAMI.2015.2469281&partnerID=40&md5=7c482b834490a34b464b94322c9491e3","Describing visual image contents by semantic concepts is an effective and straightforward way to facilitate various high level applications. Inferring semantic concepts from low-level pictorial feature analysis is challenging due to the semantic gap problem, while manually labeling concepts is unwise because of a large number of images in both online and offline collections. In this paper, we present a novel approach to automatically generate intermediate image descriptors by exploiting concept co-occurrence patterns in the pre-labeled training set that renders it possible to depict complex scene images semantically. Our work is motivated by the fact that multiple concepts that frequently co-occur across images form patterns which could provide contextual cues for individual concept inference. We discover the co-occurrence patterns as hierarchical communities by graph modularity maximization in a network with nodes and edges representing concepts and co-occurrence relationships separately. A random walk process working on the inferred concept probabilities with the discovered co-occurrence patterns is applied to acquire the refined concept signature representation. Through experiments in automatic image annotation and semantic image retrieval on several challenging datasets, we demonstrate the effectiveness of the proposed concept co-occurrence patterns as well as the concept signature representation in comparison with state-of-the-art approaches. © 2015 IEEE.","Community detection; Contextual information; Hierarchical co-occurrence patterns; Image concept signature"
"Zitnick C.L., Vedantam R., Parikh D.","Adopting abstract images for semantic scene understanding",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","4", 6942196,"627","638",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963603427&doi=10.1109%2fTPAMI.2014.2366143&partnerID=40&md5=73cbc8f156862f3132009446546dc776","Relating visual information to its linguistic semantic meaning remains an open and challenging area of research. The semantic meaning of images depends on the presence of objects, their attributes and their relations to other objects. But precisely characterizing this dependence requires extracting complex visual information from an image, which is in general a difficult and yet unsolved problem. In this paper, we propose studying semantic information in abstract images created from collections of clip art. Abstract images provide several advantages over real images. They allow for the direct study of how to infer high-level semantic information, since they remove the reliance on noisy low-level object, attribute and relation detectors, or the tedious hand-labeling of real images. Importantly, abstract images also allow the ability to generate sets of semantically similar scenes. Finding analogous sets of real images that are semantically similar would be nearly impossible. We create 1,002 sets of 10 semantically similar abstract images with corresponding written descriptions. We thoroughly analyze this dataset to discover semantically important features, the relations of words to visual features and methods for measuring semantic similarity. Finally, we study the relation between the saliency and memorability of objects and their semantic importance. © 1979-2012 IEEE.","Abstract Images; Linguistic Meaning; Memorability; Saliency; Semantic Scene Understanding"
"Brubaker M.A., Geiger A., Urtasun R.","Map-based probabilistic visual self-localization",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","4", 7152950,"652","665",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963540048&doi=10.1109%2fTPAMI.2015.2453975&partnerID=40&md5=e895bc4e50856ec5f029bd50d71e71a1","Accurate and efficient self-localization is a critical problem for autonomous systems. This paper describes an affordable solution to vehicle self-localization which uses odometry computed from two video cameras and road maps as the sole inputs. The core of the method is a probabilistic model for which an efficient approximate inference algorithm is derived. The inference algorithm is able to utilize distributed computation in order to meet the real-time requirements of autonomous systems in some instances. Because of the probabilistic nature of the model the method is capable of coping with various sources of uncertainty including noise in the visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, the proposed method is able to localize a vehicle to 4 m on average after 52 seconds of driving on maps which contain more than 2,150 km of drivable roads. © 1979-2012 IEEE.","Localization; OpenStreetMaps; Visual odometry"
"Hauagge D., Wehrwein S., Bala K., Snavely N.","Photometric ambient occlusion for intrinsic image decomposition",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","4", 7152924,"639","651",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963532891&doi=10.1109%2fTPAMI.2015.2453959&partnerID=40&md5=eff46e4c05bef188e8e47092244918ac","We present a method for computing ambient occlusion (AO) for a stack of images of a Lambertian scene from a fixed viewpoint. Ambient occlusion, a concept common in computer graphics, characterizes the local visibility at a point: it approximates how much light can reach that point from different directions without getting blocked by other geometry. While AO has received surprisingly little attention in vision, we show that it can be approximated using simple, per-pixel statistics over image stacks, based on a simplified image formation model. We use our derived AO measure to compute reflectance and illumination for objects without relying on additional smoothness priors, and demonstrate state-of-the art performance on the MIT Intrinsic Images benchmark. We also demonstrate our method on several synthetic and real scenes, including 3D printed objects with known ground truth geometry. © 1979-2012 IEEE.","Ambient occlusion; Image stacks; Intrinsic images; Pixel statistics"
"Schmidt U., Jancsary J., Nowozin S., Roth S., Rother C.","Cascades of regression tree fields for image restoration",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","4", 7117404,"677","689",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963532659&doi=10.1109%2fTPAMI.2015.2441053&partnerID=40&md5=939d61e363824275a2bf00d76f49de65","Conditional random fields (CRFs) are popular discriminative models for computer vision and have been successfully applied in the domain of image restoration, especially to image denoising. For image deblurring, however, discriminative approaches have been mostly lacking. We posit two reasons for this: First, the blur kernel is often only known at test time, requiring any discriminative approach to cope with considerable variability. Second, given this variability it is quite difficult to construct suitable features for discriminative prediction. To address these challenges we first show a connection between common half-quadratic inference for generative image priors and Gaussian CRFs. Based on this analysis, we then propose a cascade model for image restoration that consists of a Gaussian CRF at each stage. Each stage of our cascade is semi-parametric, i.e., it depends on the instance-specific parameters of the restoration problem, such as the blur kernel. We train our model by loss minimization with synthetically generated training data. Our experiments show that when applied to non-blind image deblurring, the proposed approach is efficient and yields state-of-the-art restoration quality on images corrupted with synthetic and real blur. Moreover, we demonstrate its suitability for image denoising, where we achieve competitive results for grayscale and color images. © 1979-2012 IEEE.","Conditional random fields; Image deblurring; Image restoration; Loss-based training; Prediction cascade"
"Hosang J., Benenson R., Dollar P., Schiele B.","What Makes for Effective Detection Proposals?",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","4", 7182356,"814","830",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963773434&doi=10.1109%2fTPAMI.2015.2465908&partnerID=40&md5=dd4f2ee8e96bf0a766f1fd87dcdc7a83","Current top performing object detectors employ detection proposals to guide the search for objects, thereby avoiding exhaustive sliding window search across images. Despite the popularity and widespread use of detection proposals, it is unclear which trade-offs are made when using them during object detection. We provide an in-depth analysis of twelve proposal methods along with four baselines regarding proposal repeatability, ground truth annotation recall on PASCAL, ImageNet, and MS COCO, and their impact on DPM, R-CNN, and Fast R-CNN detection performance. Our analysis shows that for object detection improving proposal localisation accuracy is as important as improving recall. We introduce a novel metric, the average recall (AR), which rewards both high recall and good localisation and correlates surprisingly well with detection performance. Our findings show common strengths and weaknesses of existing methods, and provide insights and metrics for selecting and tuning proposal methods. © 2015 IEEE.","Computer Vision; Detection proposals; object detection"
"Amer M.R., Todorovic S.","Sum product networks for activity recognition",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","4", 7182341,"800","813",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963716208&doi=10.1109%2fTPAMI.2015.2465955&partnerID=40&md5=3599bfeb214d85807bb3d05c0262cc97","This paper addresses detection and localization of human activities in videos. We focus on activities that may have variable spatiotemporal arrangements of parts, and numbers of actors. Such activities are represented by a sum-product network (SPN). A product node in SPN represents a particular arrangement of parts, and a sum node represents alternative arrangements. The sums and products are hierarchically organized, and grounded onto space-time windows covering the video. The windows provide evidence about the activity classes based on the Counting Grid (CG) model of visual words. This evidence is propagated bottom-up and top-down to parse the SPN graph for the explanation of the video. The node connectivity and model parameters of SPN and CG are jointly learned under two settings, weakly supervised, and supervised. For evaluation, we use our new Volleyball dataset, along with the benchmark datasets VIRAT, UT-Interactions, KTH, and TRECVID MED 2011. Our video classification and activity localization are superior to those of the state of the art on these datasets. © 1979-2012 IEEE.","Activity Recognition; Hierarchical Models; Sum-Product Networks"
"Osadchy M., Keren D., Raviv D.","Recognition using hybrid classifiers",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","4", 7182338,"759","771",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963762566&doi=10.1109%2fTPAMI.2015.2465910&partnerID=40&md5=4da4dda16eba72d6720ea850510ed70e","A canonical problem in computer vision is category recognition (e.g., find all instances of human faces, cars etc., in an image). Typically, the input for training a binary classifier is a relatively small sample of positive examples, and a huge sample of negative examples, which can be very diverse, consisting of images from a large number of categories. The difficulty of the problem sharply increases with the dimension and size of the negative example set. We propose to alleviate this problem by applying a 'hybrid' classifier, which replaces the negative samples by a prior, and then finds a hyperplane which separates the positive samples from this prior. The method is extended to kernel space and to an ensemble-based approach. The resulting binary classifiers achieve an identical or better classification rate than SVM, while requiring far smaller memory and lower computational complexity to train and apply. © 1979-2012 IEEE.","Large scale learning; Object detection; Object recognition"
"Barron J.T., Malik J.","Intrinsic scene properties from a single RGB-D image",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","4", 7115131,"690","703",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963540098&doi=10.1109%2fTPAMI.2015.2439286&partnerID=40&md5=759bea175d3c05ac4f0e632c12090b7b","In this paper, we present a technique for recovering a model of shape, illumination, reflectance, and shading from a single image taken from an RGB-D sensor. To do this, we extend the SIRFS ('shape, illumination and reflectance from shading') model, which recovers intrinsic scene properties from a single image [1]. Though SIRFS works well on neatly segmented images of objects, it performs poorly on images of natural scenes which often contain occlusion and spatially-varying illumination. We therefore present Scene-SIRFS, a generalization of SIRFS in which we model a scene using a mixture of shapes and a mixture of illuminations, where those mixture components are embedded in a 'soft' segmentation-like representation of the input image. We use the noisy depth maps provided by RGB-D sensors (such as the Microsoft Kinect) to guide and improve shape estimation. Our model takes as input a single RGB-D image and produces as output an improved depth map, a set of surface normals, a reflectance image, a shading image, and a spatially varying model of illumination. The output of our model can be used for graphics applications such as relighting and retargeting, or for more broad applications (recognition, segmentation) involving RGB-D images. © 1979-2012 IEEE.","Computer vision; Depth sensing; Illumination estimation; Intrinsic images; Machine learning; Normalized cuts; Segmentation; Shape estimation; Shape from shading"
"Kulkarni K., Turaga P.","Reconstruction-Free Action Inference from Compressive Imagers",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","4", 7208864,"772","784",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963777092&doi=10.1109%2fTPAMI.2015.2469288&partnerID=40&md5=b15e6c9e3749a1854f5a82cd8464b834","Persistent surveillance from camera networks, such as at parking lots, UAVs, etc., often results in large amounts of video data, resulting in significant challenges for inference in terms of storage, communication and computation. Compressive cameras have emerged as a potential solution to deal with the data deluge issues in such applications. However, inference tasks such as action recognition require high quality features which implies reconstructing the original video data. Much work in compressive sensing (CS) theory is geared towards solving the reconstruction problem, where state-of-the-art methods are computationally intensive and provide low-quality results at high compression rates. Thus, reconstruction-free methods for inference are much desired. In this paper, we propose reconstruction-free methods for action recognition from compressive cameras at high compression ratios of 100 and above. Recognizing actions directly from CS measurements requires features which are mostly nonlinear and thus not easily applicable. This leads us to search for such properties that are preserved in compressive measurements. To this end, we propose the use of spatiooral smashed filters, which are compressive domain versions of pixel-domain matched filters. We conduct experiments on publicly available databases and show that one can obtain recognition rates that are comparable to the oracle method in uncompressed setup, even for high compression ratios. © 2015 IEEE.","Action recognition; Compressive Sensing; Reconstruction-free"
"Shi J., Yan Q., Xu L., Jia J.","Hierarchical Image Saliency Detection on Extended CSSD",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","4", 7182346,"717","729",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963725053&doi=10.1109%2fTPAMI.2015.2465960&partnerID=40&md5=913ce62b0ff73f37a83c806f1533dfe3","Complex structures commonly exist in natural images. When an image contains small-scale high-contrast patterns either in the background or foreground, saliency detection could be adversely affected, resulting erroneous and non-uniform saliency assignment. The issue forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. Different from varying patch sizes or downsizing images, we measure region-based scales. The final saliency values are inferred optimally combining all the saliency cues in different scales using hierarchical inference. Through our inference model, single-scale information is selected to obtain a saliency map. Our method improves detection quality on many images that cannot be handled well traditionally. We also construct an extended Complex Scene Saliency Dataset (ECSSD) to include complex but general natural images. © 2015 IEEE.","Region scale; saliency detection"
"Sun Y., Gao J., Hong X., Mishra B., Yin B.","Heterogeneous Tensor Decomposition for Clustering via Manifold Optimization",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","3", 7182334,"476","489",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962074616&doi=10.1109%2fTPAMI.2015.2465901&partnerID=40&md5=6138ac9cd37265d201fcc2f9f724043e","Tensor clustering is an important tool that exploits intrinsically rich structures in real-world multiarray or Tensor datasets. Often in dealing with those datasets, standard practice is to use subspace clustering that is based on vectorizing multiarray data. However, vectorization of tensorial data does not exploit complete structure information. In this paper, we propose a subspace clustering algorithm without adopting any vectorization process. Our approach is based on a novel heterogeneous Tucker decomposition model taking into account cluster membership information. We propose a new clustering algorithm that alternates between different modes of the proposed heterogeneous tensor model. All but the last mode have closed-form updates. Updating the last mode reduces to optimizing over the multinomial manifold for which we investigate second order Riemannian geometry and propose a trust-region algorithm. Numerical experiments show that our proposed algorithm compete effectively with state-of-the-art clustering algorithms that are based on tensor factorization. © 1979-2012 IEEE.","Fisher metric; multinomial manifold; Riemannian optimization; Tensor clustering; trust-region"
"Ding C., Choi J., Tao D., Davis L.S.","Multi-Directional Multi-Level Dual-Cross Patterns for Robust Face Recognition",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","3", 7172530,"518","531",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962053224&doi=10.1109%2fTPAMI.2015.2462338&partnerID=40&md5=66c775ea9f810a088c4b49c30158387a","To perform unconstrained face recognition robust to variations in illumination, pose and expression, this paper presents a new scheme to extract 'Multi-Directional Multi-Level Dual-Cross Patterns' (MDML-DCPs) from face images. Specifically, the MDML-DCPs scheme exploits the first derivative of Gaussian operator to reduce the impact of differences in illumination and then computes the DCP feature at both the holistic and component levels. DCP is a novel face image descriptor inspired by the unique textural structure of human faces. It is computationally efficient and only doubles the cost of computing local binary patterns, yet is extremely robust to pose and expression variations. MDML-DCPs comprehensively yet efficiently encodes the invariant characteristics of a face image from multiple levels into patterns that are highly discriminative of inter-personal differences but robust to intra-personal variations. Experimental results on the FERET, CAS-PERL-R1, FRGC 2.0, and LFW databases indicate that DCP outperforms the state-of-the-art local descriptors (e.g., LBP, LTP, LPQ, POEM, tLBP, and LGXP) for both face identification and face verification tasks. More impressively, the best performance is achieved on the challenging LFW and FRGC 2.0 databases by deploying MDML-DCPs in a simple recognition scheme. © 1979-2012 IEEE.","face image descriptors; face image representation; Face recognition"
"Iwata T., Lloyd J.R., Ghahramani Z.","Unsupervised Many-to-Many Object Matching for Relational Data",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","3", 7208879,"607","617",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962107031&doi=10.1109%2fTPAMI.2015.2469284&partnerID=40&md5=3725ba929538b659f810a97762b1b46d","We propose a method for unsupervised many-to-many object matching from multiple networks, which is the task of finding correspondences between groups of nodes in different networks. For example, the proposed method can discover shared word groups from multi-lingual document-word networks without cross-language alignment information. We assume that multiple networks share groups, and each group has its own interaction pattern with other groups. Using infinite relational models with this assumption, objects in different networks are clustered into common groups depending on their interaction patterns, discovering a matching. The effectiveness of the proposed method is experimentally demonstrated by using synthetic and real relational data sets, which include applications to cross-domain recommendation without shared user/item identifiers and multi-lingual word clustering. © 1979-2012 IEEE.","Bayesian Nonparametrics; MCMC; Relational Data; Stochastic Block Model; Unsupervised Object Matching"
"Loog M.","Contrastive Pessimistic Likelihood Estimation for Semi-Supervised Classification",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","3", 7150421,"462","475",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962069488&doi=10.1109%2fTPAMI.2015.2452921&partnerID=40&md5=18b6138ba233cde2fe9ed991fb631563","Improvement guarantees for semi-supervised classifiers can currently only be given under restrictive conditions on the data. We propose a general way to perform semi-supervised parameter estimation for likelihood-based classifiers for which, on the full training set, the estimates are never worse than the supervised solution in terms of the log-likelihood. We argue, moreover, that we may expect these solutions to really improve upon the supervised classifier in particular cases. In a worked-out example for LDA, we take it one step further and essentially prove that its semi-supervised version is strictly better than its supervised counterpart. The two new concepts that form the core of our estimation principle are contrast and pessimism. The former refers to the fact that our objective function takes the supervised estimates into account, enabling the semi-supervised solution to explicitly control the potential improvements over this estimate. The latter refers to the fact that our estimates are conservative and therefore resilient to whatever form the true labeling of the unlabeled data takes on. Experiments demonstrate the improvements in terms of both the log-likelihood and the classification error rate on independent test sets. © 1979-2012 IEEE.","contrast; linear discriminant analysis; Maximum likelihood; pessimism; semi-supervised learning"
"Yin M., Gao J., Lin Z.","Laplacian Regularized Low-Rank Representation and Its Applications",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","3", 7172559,"504","517",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962090301&doi=10.1109%2fTPAMI.2015.2462360&partnerID=40&md5=bc006c5e8ba7820835352411deee6c3f","Low-rank representation (LRR) has recently attracted a great deal of attention due to its pleasing efficacy in exploring low-dimensional subspace structures embedded in data. For a given set of observed data corrupted with sparse errors, LRR aims at learning a lowest-rank representation of all data jointly. LRR has broad applications in pattern recognition, computer vision and signal processing. In the real world, data often reside on low-dimensional manifolds embedded in a high-dimensional ambient space. However, the LRR method does not take into account the non-linear geometric structures within data, thus the locality and similarity information among data may be missing in the learning process. To improve LRR in this regard, we propose a general Laplacian regularized low-rank representation framework for data representation where a hypergraph Laplacian regularizer can be readily introduced into, i.e., a Non-negative Sparse Hyper-Laplacian regularized LRR model (NSHLRR). By taking advantage of the graph regularizer, our proposed method not only can represent the global low-dimensional structures, but also capture the intrinsic non-linear geometric information in data. The extensive experimental results on image clustering, semi-supervised image classification and dimensionality reduction tasks demonstrate the effectiveness of the proposed method. © 1979-2012 IEEE.","Graph; Hyper- Laplacian; Laplacian Matrix; Low-Rank Representation; Manifold Structure; Regularization"
"Ristin M., Guillaumin M., Gall J., Van Gool L.","Incremental Learning of Random Forests for Large-Scale Image Classification",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","3", 7164339,"490","503",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962054043&doi=10.1109%2fTPAMI.2015.2459678&partnerID=40&md5=a1d800670ec624d6762bcf48c62e4787","Large image datasets such as ImageNet or open-ended photo websites like Flickr are revealing new challenges to image classification that were not apparent in smaller, fixed sets. In particular, the efficient handling of dynamically growing datasets, where not only the amount of training data but also the number of classes increases over time, is a relatively unexplored problem. In this challenging setting, we study how two variants of Random Forests (RF) perform under four strategies to incorporate new classes while avoiding to retrain the RFs from scratch. The various strategies account for different trade-offs between classification accuracy and computational efficiency. In our extensive experiments, we show that both RF variants, one based on Nearest Class Mean classifiers and the other on SVMs, outperform conventional RFs and are well suited for incrementally learning new classes. In particular, we show that RFs initially trained with just 10 classes can be extended to 1,000 classes with an acceptable loss of accuracy compared to training from the full data and with great computational savings compared to retraining for each new batch of classes. © 1979-2012 IEEE.","Incremental learning; large-scale image classification; random forests"
"Liu T., Tao D.","Classification with Noisy Labels by Importance Reweighting",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","3", 7159100,"447","461",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962090090&doi=10.1109%2fTPAMI.2015.2456899&partnerID=40&md5=0e3a56c883e53121041b5e9180455f54","In this paper, we study a classification problem in which sample labels are randomly corrupted. In this scenario, there is an unobservable sample with noise-free labels. However, before being observed, the true labels are independently flipped with a probability ρ in [0,0.5) , and the random label noise can be class-conditional. Here, we address two fundamental problems raised by this scenario. The first is how to best use the abundant surrogate loss functions designed for the traditional classification problem when there is label noise. We prove that any surrogate loss function can be used for classification with noisy labels by using importance reweighting, with consistency assurance that the label noise does not ultimately hinder the search for the optimal classifier of the noise-free sample. The other is the open problem of how to obtain the noise rate ρ. We show that the rate is upper bounded by the conditional probability P(Y|X) of the noisy sample. Consequently, the rate can be estimated, because the upper bound can be easily reached in classification problems. Experimental results on synthetic and real datasets confirm the efficiency of our methods. © 1979-2012 IEEE.","Classification; consistency; importance reweighting; label noise; noise rate estimation"
"Zhang Q., Song X., Shao X., Zhao H., Shibasaki R.","Object Discovery: Soft Attributed Graph Mining",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","3", 7159067,"532","545",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962094508&doi=10.1109%2fTPAMI.2015.2456892&partnerID=40&md5=ee3e61e8628121f68971e617ae3ac479","We categorize this research in terms of its contribution to both graph theory and computer vision. From the theoretical perspective, this study can be considered as the first attempt to formulate the idea of mining maximal frequent subgraphs in the challenging domain of messy visual data, and as a conceptual extension to the unsupervised learning of graph matching. We define a soft attributed pattern (SAP) to represent the common subgraph pattern among a set of attributed relational graphs (ARGs), considering both their structure and attributes. Regarding the differences between ARGs with fuzzy attributes and conventional labeled graphs, we propose a new mining strategy that directly extracts the SAP with the maximal graph size without applying node enumeration. Given an initial graph template and a number of ARGs, we develop an unsupervised method to modify the graph template into the maximal-size SAP. From a practical perspective, this research develops a general platform for learning the category model (i.e., the SAP) from cluttered visual data (i.e., the ARGs) without labeling 'what is where,' thereby opening the possibility for a series of applications in the era of big visual data. Experiments demonstrate the superior performance of the proposed method on RGB/RGB-D images and videos. © 1979-2012 IEEE.","Attributed Relational Graphs; Big Visual Data; Graph Matching; Graph Mining; Ubiquitous Learning"
"Zheng W.-S., Gong S., Xiang T.","Towards Open-World Person Re-Identification by One-Shot Group-Based Verification",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","3", 7152932,"591","606",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962094670&doi=10.1109%2fTPAMI.2015.2453984&partnerID=40&md5=a80f05abdc3fc4921dd5006814afcd24","Solving the problem of matching people across non-overlapping multi-camera views, known as person re-identification (re-id), has received increasing interests in computer vision. In a real-world application scenario, a watch-list (gallery set) of a handful of known target people are provided with very few (in many cases only a single) image(s) (shots) per target. Existing re-id methods are largely unsuitable to address this open-world re-id challenge because they are designed for (1) a closed-world scenario where the gallery and probe sets are assumed to contain exactly the same people, (2) person-wise identification whereby the model attempts to verify exhaustively against each individual in the gallery set, and (3) learning a matching model using multi-shots. In this paper, a novel transfer local relative distance comparison (t-LRDC) model is formulated to address the open-world person re-identification problem by one-shot group-based verification. The model is designed to mine and transfer useful information from a labelled open-world non-target dataset. Extensive experiments demonstrate that the proposed approach outperforms both non-transfer learning and existing transfer learning based re-id methods. © 1979-2012 IEEE.","Group-based verification; open-world reidentification; transfer relative distance comparison"
"Biswas S.K., Milanfar P.","One Shot Detection with Laplacian Object and Fast Matrix Cosine Similarity",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","3", 7152953,"546","562",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962092489&doi=10.1109%2fTPAMI.2015.2453950&partnerID=40&md5=01af4de165ea4965070172000b2effdb","One shot, generic object detection involves searching for a single query object in a larger target image. Relevant approaches have benefited from features that typically model the local similarity patterns. In this paper, we combine local similarity (encoded by local descriptors) with a global context (i.e., a graph structure) of pairwise affinities among the local descriptors, embedding the query descriptors into a low dimensional but discriminatory subspace. Unlike principal components that preserve global structure of feature space, we actually seek a linear approximation to the Laplacian eigenmap that permits us a locality preserving embedding of high dimensional region descriptors. Our second contribution is an accelerated but exact computation of matrix cosine similarity as the decision rule for detection, obviating the computationally expensive sliding window search. We leverage the power of Fourier transform combined with integral image to achieve superior runtime efficiency that allows us to test multiple hypotheses (for pose estimation) within a reasonably short time. Our approach to one shot detection is training-free, and experiments on the standard data sets confirm the efficacy of our model. Besides, low computation cost of the proposed (codebook-free) object detector facilitates rather straightforward query detection in large data sets including movie videos. © 1979-2012 IEEE.","Fast Detection; Fourier transform; Graph based dimensionality reduction; One shot object detection"
"Khan S.H., Bennamoun M., Sohel F., Togneri R.","Automatic Shadow Detection and Removal from a Single Image",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","3", 7172555,"431","446",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962090411&doi=10.1109%2fTPAMI.2015.2462355&partnerID=40&md5=ce31bb2f47c3b641629d6f0809f81e60","We present a framework to automatically detect and remove shadows in real world scenes from a single image. Previous works on shadow detection put a lot of effort in designing shadow variant and invariant hand-crafted features. In contrast, our framework automatically learns the most relevant features in a supervised manner using multiple convolutional deep neural networks (ConvNets). The features are learned at the super-pixel level and along the dominant boundaries in the image. The predicted posteriors based on the learned features are fed to a conditional random field model to generate smooth shadow masks. Using the detected shadow masks, we propose a Bayesian formulation to accurately extract shadow matte and subsequently remove shadows. The Bayesian formulation is based on a novel model which accurately models the shadow generation process in the umbra and penumbra regions. The model parameters are efficiently estimated using an iterative optimization procedure. Our proposed framework consistently performed better than the state-of-the-art on all major shadow databases collected under a variety of conditions. © 1979-2012 IEEE.","Bayesian shadow removal; Conditional Random Field; ConvNets; Feature Learning; Shadow detection; Shadow matting"
"Munoz-Gonzalez L., Lazaro-Gredilla M., Figueiras-Vidal A.R.","Laplace Approximation for Divisive Gaussian Processes for Nonstationary Regression",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","3", 7150413,"618","624",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962045172&doi=10.1109%2fTPAMI.2015.2452914&partnerID=40&md5=0f6d75c4711e6092a0239cbe63343bae","The standard Gaussian Process regression (GP) is usually formulated under stationary hypotheses: The noise power is considered constant throughout the input space and the covariance of the prior distribution is typically modeled as depending only on the difference between input samples. These assumptions can be too restrictive and unrealistic for many real-world problems. Although nonstationarity can be achieved using specific covariance functions, they require a prior knowledge of the kind of nonstationarity, not available for most applications. In this paper we propose to use the Laplace approximation to make inference in a divisive GP model to perform nonstationary regression, including heteroscedastic noise cases. The log-concavity of the likelihood ensures a unimodal posterior and makes that the Laplace approximation converges to a unique maximum. The characteristics of the likelihood also allow to obtain accurate posterior approximations when compared to the Expectation Propagation (EP) approximations and the asymptotically exact posterior provided by a Markov Chain Monte Carlo implementation with Elliptical Slice Sampling (ESS), but at a reduced computational load with respect to both, EP and ESS. © 1979-2012 IEEE.","Gaussian Processes; Heteroscedastic Regression; Laplace approximation; Nonstationary Regression"
"Liu G., Xu H., Tang J., Liu Q., Yan S.","A Deterministic Analysis for LRR",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","3", 7152948,"417","430",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962053951&doi=10.1109%2fTPAMI.2015.2453969&partnerID=40&md5=e681e9046b726c483e992c99baff5e53","The recently proposed low-rank representation (LRR) method has been empirically shown to be useful in various tasks such as motion segmentation, image segmentation, saliency detection and face recognition. While potentially powerful, LRR depends heavily on the configuration of its key parameter, λ. In realistic environments where the prior knowledge about data is lacking, however, it is still unknown how to choose λ in a suitable way. Even more, there is a lack of rigorous analysis about the success conditions of the method, and thus the significance of LRR is a little bit vague. In this paper we therefore establish a theoretical analysis for LRR, striving for figuring out under which conditions LRR can be successful, and deriving a moderately good estimate to the key parameter λ as well. Simulations on synthetic data points and experiments on real motion sequences verify our claims. © 1979-2012 IEEE.","low-rank representation; outlier detection; parameter estimation"
"Chen L., Shen C., Vogelstein J.T., Priebe C.E.","Robust Vertex Classification",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","3", 7159084,"578","590",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962073615&doi=10.1109%2fTPAMI.2015.2456913&partnerID=40&md5=de6416b988c7c8007d108bc7684a7d77","For random graphs distributed according to stochastic blockmodels, a special case of latent position graphs, adjacency spectral embedding followed by appropriate vertex classification is asymptotically Bayes optimal; but this approach requires knowledge of and critically depends on the model dimension. In this paper, we propose a sparse representation vertex classifier which does not require information about the model dimension. This classifier represents a test vertex as a sparse combination of the vertices in the training set and uses the recovered coefficients to classify the test vertex. We prove consistency of our proposed classifier for stochastic blockmodels, and demonstrate that the sparse representation classifier can predict vertex labels with higher accuracy than adjacency spectral embedding approaches via both simulation studies and real data experiments. Our results demonstrate the robustness and effectiveness of our proposed vertex classifier when the model dimension is unknown. © 1979-2012 IEEE.","robustness; sparse representation; vertex classification"
"Fu Y., Hospedales T.M., Xiang T., Xiong J., Gong S., Wang Y., Yao Y.","Robust Subjective Visual Property Prediction from Crowdsourced Pairwise Labels",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","3", 7159107,"563","577",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962128645&doi=10.1109%2fTPAMI.2015.2456887&partnerID=40&md5=9364f7f5eb16572c41ef563f0fc562d2","The problem of estimating subjective visual properties from image and video has attracted increasing interest. A subjective visual property is useful either on its own (e.g. image and video interestingness) or as an intermediate representation for visual recognition (e.g. a relative attribute). Due to its ambiguous nature, annotating the value of a subjective visual property for learning a prediction model is challenging. To make the annotation more reliable, recent studies employ crowdsourcing tools to collect pairwise comparison labels. However, using crowdsourced data also introduces outliers. Existing methods rely on majority voting to prune the annotation outliers/errors. They thus require a large amount of pairwise labels to be collected. More importantly as a local outlier detection method, majority voting is ineffective in identifying outliers that can cause global ranking inconsistencies. In this paper, we propose a more principled way to identify annotation outliers by formulating the subjective visual property prediction task as a unified robust learning to rank problem, tackling both the outlier detection and learning to rank jointly. This differs from existing methods in that (1) the proposed method integrates local pairwise comparison labels together to minimise a cost that corresponds to global inconsistency of ranking order, and (2) the outlier detection and learning to rank problems are solved jointly. This not only leads to better detection of annotation outliers but also enables learning with extremely sparse annotations. © 1979-2012 IEEE.","outlier detection; robust ranking; Subjective visual properties"
"Painsky A., Rosset S.","Isotonic Modeling with Non-Differentiable Loss Functions with Application to Lasso Regularization",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","2", 7117430,"308","321",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962097067&doi=10.1109%2fTPAMI.2015.2441063&partnerID=40&md5=ccdb3a7b3ab3aee54a96b62542e5f0d5","In this paper we present an algorithmic approach for fitting isotonic models under convex, yet non-differentiable, loss functions. It is a generalization of the greedy non-regret approach proposed by Luss and Rosset (2014) for differentiable loss functions, taking into account the sub-gradiental extensions required. We prove that our suggested algorithm solves the isotonic modeling problem while maintaining favorable computational and statistical properties. As our suggested algorithm may be used for any non-differentiable loss function, we focus our interest on isotonic modeling for either regression or two-class classification with appropriate log-likelihood loss and lasso penalty on the fitted values. This combination allows us to maintain the non-parametric nature of isotonic modeling, while controlling model complexity through regularization. We demonstrate the efficiency and usefulness of this approach on both synthetic and real world data. An implementation of our suggested solution is publicly available from the first author's website (https://sites.google.com/site/amichaipainsky/software). © 1979-2012 IEEE.","convex optimization; GIRP; isotonic regression; nonparametric regression; regularization path"
"Liao S., Jain A.K., Li S.Z.","A Fast and Accurate Unconstrained Face Detector",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","2", 7130626,"211","223",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962088950&doi=10.1109%2fTPAMI.2015.2448075&partnerID=40&md5=3d8e875f2dc5d19b15ef0a66b4e1b0c9","We propose a method to address challenges in unconstrained face detection, such as arbitrary pose variations and occlusions. First, a new image feature called Normalized Pixel Difference (NPD) is proposed. NPD feature is computed as the difference to sum ratio between two pixel values, inspired by the Weber Fraction in experimental psychology. The new feature is scale invariant, bounded, and is able to reconstruct the original image. Second, we propose a deep quadratic tree to learn the optimal subset of NPD features and their combinations, so that complex face manifolds can be partitioned by the learned rules. This way, only a single soft-cascade classifier is needed to handle unconstrained face detection. Furthermore, we show that the NPD features can be efficiently obtained from a look up table, and the detection template can be easily scaled, making the proposed face detector very fast. Experimental results on three public face datasets (FDDB, GENKI, and CMU-MIT) show that the proposed method achieves state-of-the-art performance in detecting unconstrained faces with arbitrary pose variations and occlusions in cluttered scenes. © 1979-2012 IEEE.","AdaBoost; cascade classifier; normalized pixel difference; regression tree; Unconstrained face detection"
"Oxholm G., Nishino K.","Shape and Reflectance Estimation in the Wild",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","2", 7138642,"376","389",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962124771&doi=10.1109%2fTPAMI.2015.2450734&partnerID=40&md5=335f21e0904e14f4e06ba2b1c7bcab93","Our world is full of objects with complex reflectances situated in rich illumination environments. Though stunning, the diversity of appearance that arises from this complexity is also daunting. For this reason, past work on geometry recovery has tried to frame the problem into simplistic models of reflectance (such as Lambertian, mirrored, or dichromatic) or illumination (one or more distant point light sources). In this work, we directly tackle the problem of joint reflectance and geometry estimation under known but uncontrolled natural illumination by fully exploiting the surface orientation cues that become embedded in the appearance of the object. Intuitively, salient scene features (such as the sun or stained glass windows) act analogously to the point light sources of traditional geometry estimation frameworks by strongly constraining the possible orientations of the surface patches reflecting them. By jointly estimating the reflectance of the object, which modulates the illumination, the appearance of a surface patch can be used to derive a nonparametric distribution of its possible orientations. If only a single image exists, these strongly constrained surface patches may then be used to anchor the geometry estimation and give context to the less-descriptive regions. When multiple images exist, the distribution of possible surface orientations becomes tighter as additional context is given, though integrating the separate views poses additional challenges. In this paper we introduce two methods, one for the single image case, and another for the case of multiple images. The effectiveness of our methods is evaluated extensively on synthetic and real-world data sets that span the wide range of real-world environments and reflectances that lies between the extremes that have been the focus of past work. © 1979-2012 IEEE.","multiview stereo; reflectance estimation; shape estimation; shape from shadin"
"Dong C., Loy C.C., He K., Tang X.","Image Super-Resolution Using Deep Convolutional Networks",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","2", 7115171,"295","307",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962128851&doi=10.1109%2fTPAMI.2015.2439281&partnerID=40&md5=85dfb77242a07a971ab72603a1f105ba","We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality. © 1979-2012 IEEE.","deep convolutional neural networks; sparse coding; Super-resolution"
"Zhou Q., Zhao Q.","Flexible Clustered Multi-Task Learning by Learning Representative Tasks",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","2", 7150415,"266","278",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962053225&doi=10.1109%2fTPAMI.2015.2452911&partnerID=40&md5=ec1a1e0fb393d28324a7d47e8bf7d4ce","Multi-task learning (MTL) methods have shown promising performance by learning multiple relevant tasks simultaneously, which exploits to share useful information across relevant tasks. Among various MTL methods, clustered multi-task learning (CMTL) assumes that all tasks can be clustered into groups and attempts to learn the underlying cluster structure from the training data. In this paper, we present a new approach for CMTL, called flexible clustered multi-task (FCMTL), in which the cluster structure is learned by identifying representative tasks. The new approach allows an arbitrary task to be described by multiple representative tasks, effectively soft-assigning a task to multiple clusters with different weights. Unlike existing counterpart, the proposed approach is more flexible in that (a) it does not require clusters to be disjoint, (b) tasks within one particular cluster do not have to share information to the same extent, and (c) the number of clusters is automatically inferred from data. Computationally, the proposed approach is formulated as a row-sparsity pursuit problem. We validate the proposed FCMTL on both synthetic and real-world data sets, and empirical results demonstrate that it outperforms many existing MTL methods. © 1979-2012 IEEE.","Clustered Multi-Task Learning; Group Sparsity; Representative Task"
"Huang D., Cabral R., Torre F.D.","Robust Regression",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","2", 7130636,"363","375",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962076949&doi=10.1109%2fTPAMI.2015.2448091&partnerID=40&md5=55adeae5749322fd698667138ffebae6","Discriminative methods (e.g., kernel regression, SVM) have been extensively used to solve problems such as object recognition, image alignment and pose estimation from images. These methods typically map image features (X) to continuous (e.g., pose) or discrete (e.g., object category) values. A major drawback of existing discriminative methods is that samples are directly projected onto a subspace and hence fail to account for outliers common in realistic training sets due to occlusion, specular reflections or noise. It is important to notice that existing discriminative approaches assume the input variables X to be noise free. Thus, discriminative methods experience significant performance degradation when gross outliers are present. Despite its obvious importance, the problem of robust discriminative learning has been relatively unexplored in computer vision. This paper develops the theory of robust regression (RR) and presents an effective convex approach that uses recent advances on rank minimization. The framework applies to a variety of problems in computer vision including robust linear discriminant analysis, regression with missing data, and multi-label classification. Several synthetic and real examples with applications to head pose estimation from images, image and video classification and facial attribute classification with missing data are used to illustrate the benefits of RR. © 1979-2012 IEEE.","errors in variables; intra-sample outliers; missing data; Robust methods"
"Kooij J.F.P., Englebienne G., Gavrila D.M.","Mixture of Switching Linear Dynamics to Discover Behavior Patterns in Object Tracks",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","2", 7121003,"322","334",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962092322&doi=10.1109%2fTPAMI.2015.2443801&partnerID=40&md5=f7ff62e0e53c2de1420ca2448822f743","We present a novel non-parametric Bayesian model to jointly discover the dynamics of low-level actions and high-level behaviors of tracked objects. In our approach, actions capture both linear, low-level object dynamics, and an additional spatial distribution on where the dynamic occurs. Furthermore, behavior classes capture high-level temporal motion dependencies in Markov chains of actions, thus each learned behavior is a switching linear dynamical system. The number of actions and behaviors is discovered from the data itself using Dirichlet Processes. We are especially interested in cases where tracks can exhibit large kinematic and spatial variations, e.g. person tracks in open environments, as found in the visual surveillance and intelligent vehicle domains. The model handles real-valued features directly, so no information is lost by quantizing measurements into 'visual words', and variations in standing, walking and running can be discovered without discrete thresholds. We describe inference using Markov Chain Monte Carlo sampling and validate our approach on several artificial and real-world pedestrian track datasets from the surveillance and intelligent vehicle domain. We show that our model can distinguish between relevant behavior patterns that an existing state-of-the-art hierarchical model for clustering and simpler model variants cannot. The software and the artificial and surveillance datasets are made publicly available for benchmarking purposes. © 1979-2012 IEEE.","Hierarchical non-parametric graphical model; Human behavior analysis; Switching Linear Dynamical Systems"
"Vo M., Narasimhan S.G., Sheikh Y.","Texture Illumination Separation for Single-Shot Structured Light Reconstruction",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","2", 7121014,"390","404",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962069502&doi=10.1109%2fTPAMI.2015.2443775&partnerID=40&md5=4e9177dcb59fbdd46c89f1f6c67075de","Active illumination based methods have a trade-off between acquisition time and resolution of the estimated 3D shapes. Multi-shot approaches can generate dense reconstructions but require stationary scenes. Single-shot methods are applicable to dynamic objects but can only estimate sparse reconstructions and are sensitive to surface texture. We present a single-shot approach to produce dense shape reconstructions of highly textured objects illuminated by one or more projectors. The key to our approach is an image decomposition scheme that can recover the illumination image of different projectors and the texture images of the scene from their mixed appearances. We focus on three cases of mixed appearances: the illumination from one projector onto textured surface, illumination from multiple projectors onto a textureless surface, or their combined effect. Our method can accurately compute per-pixel warps from the illumination patterns and the texture template to the observed image. The texture template is obtained by interleaving the projection sequence with an all-white pattern. The estimated warps are reliable even with infrequent interleaved projection and strong object deformation. Thus, we obtain detailed shape reconstruction and dense motion tracking of the textured surfaces. The proposed method, implemented using a one camera and two projectors system, is validated on synthetic and real data containing subtle non-rigid surface deformations. © 1979-2012 IEEE.","decomposition; illumination; mixture; separation; Single-shot; texture"
"Ren W., Huang K., Tao D., Tan T.","Weakly Supervised Large Scale Object Localization with Multiple Instance Learning and Bag Splitting",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","2", 7159101,"405","416",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962122607&doi=10.1109%2fTPAMI.2015.2456908&partnerID=40&md5=17c963d72a43fd57520c2dbcd9190653","Localizing objects of interest in images when provided with only image-level labels is a challenging visual recognition task. Previous efforts have required carefully designed features and have difficulty in handling images with cluttered backgrounds. Up-scaling to large datasets also poses a challenge to applying these methods to real applications. In this paper, we propose an efficient and effective learning framework called MILinear, which is able to learn an object localization model from large-scale data without using bounding box annotations. We integrate rich general prior knowledge into a learning model using a large pre-trained convolutional network. Moreover, to reduce ambiguity in positive images, we present a bag-splitting algorithm that iteratively generates new negative bags from positive ones. We evaluate the proposed approach on the challenging Pascal VOC 2007 dataset, and our method outperforms other state-of-the-art methods by a large margin; some results are even comparable to fully supervised models trained with bounding box annotations. To further demonstrate scalability, we also present detection results on the ILSVRC 2013 detection dataset, and our method outperforms supervised deformable part-based model without using box annotations. © 1979-2012 IEEE.","Convolutional networks; Multiple instance learning; Weakly supervised localization"
"Tennakoon R.B., Bab-Hadiashar A., Cao Z., Hoseinnezhad R., Suter D.","Robust Model Fitting Using Higher Than Minimal Subset Sampling",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","2", 7130650,"350","362",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962045965&doi=10.1109%2fTPAMI.2015.2448103&partnerID=40&md5=ec329b81caa8b4461c648fa02537af2a","Identifying the underlying model in a set of data contaminated by noise and outliers is a fundamental task in computer vision. The cost function associated with such tasks is often highly complex, hence in most cases only an approximate solution is obtained by evaluating the cost function on discrete locations in the parameter (hypothesis) space. To be successful at least one hypothesis has to be in the vicinity of the solution. Due to noise hypotheses generated by minimal subsets can be far from the underlying model, even when the samples are from the said structure. In this paper we investigate the feasibility of using higher than minimal subset sampling for hypothesis generation. Our empirical studies showed that increasing the sample size beyond minimal size ($p$ ), in particular up to $p+2$ , will significantly increase the probability of generating a hypothesis closer to the true model when subsets are selected from inliers. On the other hand, the probability of selecting an all inlier sample rapidly decreases with the sample size, making direct extension of existing methods unfeasible. Hence, we propose a new computationally tractable method for robust model fitting that uses higher than minimal subsets. Here, one starts from an arbitrary hypothesis (which does not need to be in the vicinity of the solution) and moves until either a structure in data is found or the process is re-initialized. The method also has the ability to identify when the algorithm has reached a hypothesis with adequate accuracy and stops appropriately, thereby saving computational time. The experimental analysis carried out using synthetic and real data shows that the proposed method is both accurate and efficient compared to the state-of-the-art robust model fitting techniques. © 1979-2012 IEEE.","data segmentation; higher than minimal subset sampling; hypothesis generation; Model fitting; Robust Statistics"
"Zhou F., De La Torre F.","Generalized Canonical Time Warping",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","2", 7062899,"279","294",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962071073&doi=10.1109%2fTPAMI.2015.2414429&partnerID=40&md5=c35662277e424b5fbfcd2264026ada9d","Temporal alignment of human motion has been of recent interest due to its applications in animation, tele-rehabilitation and activity recognition. This paper presents generalized canonical time warping (GCTW), an extension of dynamic time warping (DTW) and canonical correlation analysis (CCA) for temporally aligning multi-modal sequences from multiple subjects performing similar activities. GCTW extends previous work on DTW and CCA in several ways: (1) it combines CCA with DTW to align multi-modal data (e.g., video and motion capture data); (2) it extends DTW by using a linear combination of monotonic functions to represent the warping path, providing a more flexible temporal warp. Unlike exact DTW, which has quadratic complexity, we propose a linear time algorithm to minimize GCTW. (3) GCTW allows simultaneous alignment of multiple sequences. Experimental results on aligning multi-modal data, facial expressions, motion capture data and video illustrate the benefits of GCTW. The code is available at http://humansensing.cs.cmu.edu/ctw. © 1979-2012 IEEE.","Canonical correlation analysis; Dynamic time warping; Multi-modal sequence alignment"
"Liu L., Wang L., Shen C.","A Generalized Probabilistic Framework for Compact Codebook Creation",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","2", 7117422,"224","237",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962091518&doi=10.1109%2fTPAMI.2015.2441069&partnerID=40&md5=c38d044407815d9a576a994065fc55c2","Compact and discriminative visual codebooks are preferred in many visual recognition tasks. In the literature, a number of works have taken the approach of hierarchically merging visual words of an initial large-sized codebook, but implemented this approach with different merging criteria. In this work, we propose a single probabilistic framework to unify these merging criteria, by identifying two key factors: the function used to model the class-conditional distribution and the method used to estimate the distribution parameters. More importantly, by adopting new distribution functions and/or parameter estimation methods, our framework can readily produce a spectrum of novel merging criteria. Three of them are specifically discussed in this paper. For the first criterion, we adopt the multinomial distribution with the Bayesian method; For the second criterion, we integrate the Gaussian distribution with maximum likelihood parameter estimation. For the third criterion, which shows the best merging performance, we propose a max-margin-based parameter estimation method and apply it with the multinomial distribution. Extensive experimental study is conducted to systematically analyze the performance of the above three criteria and compare them with existing ones. As demonstrated, the best criterion within our framework achieves the overall best merging performance among the compared merging criteria developed in the literature. © 1979-2012 IEEE.","Bag-of-features model; Compact codebook; Image recognition; Max-margin estimation; Probabilistic framework"
"Li A., Lin M., Wu Y., Yang M.-H., Yan S.","NUS-PRO: A New Visual Tracking Challenge",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","2", 7072555,"335","349",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962116001&doi=10.1109%2fTPAMI.2015.2417577&partnerID=40&md5=0af78b1cd3848678c3192378b350e4c6","Numerous approaches on object tracking have been proposed during the past decade with demonstrated success. However, most tracking algorithms are evaluated on limited video sequences and annotations. For thorough performance evaluation, we propose a large-scale database which contains 365 challenging image sequences of pedestrians and rigid objects. The database covers 12 kinds of objects, and most of the sequences are captured from moving cameras. Each sequence is annotated with target location and occlusion level for evaluation. A thorough experimental evaluation of 20 state-of-the-art tracking algorithms is presented with detailed analysis using different metrics. The database is publicly available and evaluation can be carried out online for fair assessments of visual tracking algorithms. © 1979-2012 IEEE.","benchmark database; Object tracking; performance evaluation"
"Ambikasaran S., Foreman-Mackey D., Greengard L., Hogg D.W., O'Neil M.","Fast Direct Methods for Gaussian Processes",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","2", 7130620,"252","265",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962053105&doi=10.1109%2fTPAMI.2015.2448083&partnerID=40&md5=b420bfec57972eef39e1bb5bf29051bc","A number of problems in probability and statistics can be addressed using the multivariate normal (Gaussian) distribution. In the one-dimensional case, computing the probability for a given mean and variance simply requires the evaluation of the corresponding Gaussian density. In the n -dimensional setting, however, it requires the inversion of an n × n covariance matrix, C , as well as the evaluation of its determinant, det (C). In many cases, such as regression using Gaussian processes, the covariance matrix is of the form C = σ2 I + K , where K is computed using a specified covariance kernel which depends on the data and additional parameters (hyperparameters). The matrix C is typically dense, causing standard direct methods for inversion and determinant evaluation to require O(n3) work. This cost is prohibitive for large-scale modeling. Here, we show that for the most commonly used covariance functions, the matrix C can be hierarchically factored into a product of block low-rank updates of the identity matrix, yielding an O (n2, n) algorithm for inversion. More importantly, we show that this factorization enables the evaluation of the determinant (C), permitting the direct calculation of probabilities in high dimensions under fairly broad assumptions on the kernel defining K. Our fast algorithm brings many problems in marginalization and the adaptation of hyperparameters within practical reach using a single CPU core. The combination of nearly optimal scaling in terms of problem size with high-performance computing resources will permit the modeling of previously intractable problems. We illustrate the performance of the scheme on standard covariance kernels. © 1979-2012 IEEE.","Bayesian analysis; Covariance matrix; Ction; Determinant; Direct solver; Fast multipole method; Hierarchical off-diagonal low-rank; Likelihood"
"Kolesov I., Lee J., Sharp G., Vela P., Tannenbaum A.","A Stochastic Approach to Diffeomorphic Point Set Registration with Landmark Constraints",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","2", 7130637,"238","251",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962071093&doi=10.1109%2fTPAMI.2015.2448102&partnerID=40&md5=3dde402d3b9325b00d6cfa803e49bc4a","This work presents a deformable point set registration algorithm that seeks an optimal set of radial basis functions to describe the registration. A novel, global optimization approach is introduced composed of simulated annealing with a particle filter based generator function to perform the registration. It is shown how constraints can be incorporated into this framework. A constraint on the deformation is enforced whose role is to ensure physically meaningful fields (i.e., invertible). Further, examples in which landmark constraints serve to guide the registration are shown. Results on 2D and 3D data demonstrate the algorithm's robustness to noise and missing information. © 1979-2012 IEEE.","constrained optimization; deformable registration; Point set"
"Domokos C., Kato Z.","Realigning 2D and 3D Object Fragments without Correspondences",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","1", 7138636,"195","202",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961572714&doi=10.1109%2fTPAMI.2015.2450726&partnerID=40&md5=11db09b53a6d6aab62e474a1bd2e9588","This paper addresses the problem of simultaneous estimation of different linear deformations, resulting in a global non-linear transformation, between an original object and its broken fragments. A general framework is proposed without using correspondences, where the solution of a polynomial system of equations directly provides the parameters of the alignment. We quantitatively evaluate the proposed algorithm on a large synthetic dataset containing 2D and 3D images, where linear (rigid-body and affine) transformations are considered. We also conduct an exhaustive analysis of the robustness against segmentation errors and the numerical stability of the proposed method. Moreover, we present experiments on 2D real images as well as on volumetric medical images. © 1979-2012 IEEE.","affine puzzle; Image registration; locally linear deformation; realignment of broken fragments"
"Girshick R., Donahue J., Darrell T., Malik J.","Region-Based Convolutional Networks for Accurate Object Detection and Segmentation",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","1", 7112511,"142","158",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961595279&doi=10.1109%2fTPAMI.2015.2437384&partnerID=40&md5=f5625ad24a55f8ec458eadf4201f5871","Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012 - achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at http://www.cs.Berkeley.edu/rbg/rcnn. © 1979-2012 IEEE.","Convolutional Networks; Deep Learning; Detection; Object Recognition; Semantic Segmentation; Transfer Learning"
"Zhu F., Chen G., Hao J., Heng P.-A.","Blind Image Denoising via Dependent Dirichlet Process Tree",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","PP","99", 7557070,"","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988499919&doi=10.1109%2fTPAMI.2016.2604816&partnerID=40&md5=3ac99665b0ce4752017931a87e367602","Most existing image denoising approaches assumed the noise to be homogeneous white Gaussian distributed with known intensity. However, in real noisy images, the noise models are usually unknown beforehand and can be much more complex. This paper addresses this problem and proposes a novel blind image denoising algorithm to recover the clean image from noisy one with the unknown noise model. To model the empirical noise of an image, our method introduces the mixture of Gaussian distribution, which is flexible enough to approximate different continuous distributions. The problem of blind image denoising is reformulated as a learning problem. The procedure is to first build a two-layer structural model for noisy patches and consider the clean ones as latent variable. To control the complexity of the noisy patch model, this work proposes a novel Bayesian nonparametric prior called 'Dependent Dirichlet Process Tree' to build the model. Then, this study derives a variational inference algorithm to estimate model parameters and recover clean patches. We apply our method on synthesis and real noisy images with different noise models. Comparing with previous approaches, ours achieves better performance. The experimental results indicate the efficiency of the proposed algorithm to cope with practical image denoising tasks. © 1979-2012 IEEE.","Bayesian nonparametrics; Blind image denoising; dependent Dirichlet process; noise modeling; patch modeling; variational inference"
"Tumpach A.B., Drira H., Daoudi M., Srivastava A.","Gauge Invariant Framework for Shape Analysis of Surfaces",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","1", 7102748,"46","59",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961661604&doi=10.1109%2fTPAMI.2015.2430319&partnerID=40&md5=6f66a72b5a96b027813ad921f37c51fb","This paper describes a novel framework for computing geodesic paths in shape spaces of spherical surfaces under an elastic Riemannian metric. The novelty lies in defining this Riemannian metric directly on the quotient (shape) space, rather than inheriting it from pre-shape space, and using it to formulate a path energy that measures only the normal components of velocities along the path. In other words, this paper defines and solves for geodesics directly on the shape space and avoids complications resulting from the quotient operation. This comprehensive framework is invariant to arbitrary parameterizations of surfaces along paths, a phenomenon termed as gauge invariance. Additionally, this paper makes a link between different elastic metrics used in the computer science literature on one hand, and the mathematical literature on the other hand, and provides a geometrical interpretation of the terms involved. Examples using real and simulated 3D objects are provided to help illustrate the main ideas. © 1979-2012 IEEE.","3D surfaces; geodesics; Riemannian metric"
"Brandao M., Ferreira R., Hashimoto K., Takanishi A., Santos-Victor J.","On Stereo Confidence Measures for Global Methods: Evaluation, New Model and Integration into Occupancy Grids",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","1", 7112529,"116","128",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961578459&doi=10.1109%2fTPAMI.2015.2437381&partnerID=40&md5=9a92c6cbdbd79bb992fd21d7839f63aa","Stereo confidence measures are important functions for global reconstruction methods and some applications of stereo. In this article we evaluate and compare several models of confidence which are defined at the whole disparity range. We propose a new stereo confidence measure to which we call the Histogram Sensor Model (HSM), and show how it is one of the best performing functions overall. We also introduce, for parametric models, a systematic method for estimating their parameters which is shown to lead to better performance when compared to parameters as computed in previous literature. All models were evaluated when applied to two different cost functions at different window sizes and model parameters. Contrary to previous stereo confidence measure benchmark literature, we evaluate the models with criteria important not only to winner-take-all stereo, but also to global applications. To this end, we evaluate the models on a real-world application using a recent formulation of 3D reconstruction through occupancy grids which integrates stereo confidence at all disparities. We obtain and discuss our results on both indoors' and outdoors' publicly available datasets. © 1979-2012 IEEE.","3D reconstruction; confidence; occupancy grids; stereo matching; Stereo vision; uncertainty"
"Haene C., Zach C., Cohen A., Pollefeys M.","Dense Semantic 3D Reconstruction",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","PP","99", 7575643,"","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991658393&doi=10.1109%2fTPAMI.2016.2613051&partnerID=40&md5=4948b8c064548391d8c6eb9510b8c3b0","Both image segmentation and dense 3D modeling from images represent an intrinsically ill-posed problem. Strong regularizers are therefore required to constrain the solutions from being 'too noisy'. These priors generally yield overly smooth reconstructions and/or segmentations in certain regions while they fail to constrain the solution sufficiently in other areas. In this paper, we argue that image segmentation and dense 3D reconstruction contribute valuable information to each other's task. As a consequence, we propose a mathematical framework to formulate and solve a joint segmentation and dense reconstruction problem. On the one hand knowing about the semantic class of the geometry provides information about the likelihood of the surface direction. On the other hand the surface direction provides information about the likelihood of the semantic class. Experimental results on several data sets highlight the advantages of our joint formulation. We show how weakly observed surfaces are reconstructed more faithfully compared to a geometry only reconstruction. Thanks to the volumetric nature of our formulation we also infer surfaces which cannot be directly observed for example the surface between the ground and a building. Finally, our method returns a semantic segmentation which is consistent across the whole dataset. © 2016 IEEE.","Convex Formulation; Multi-Label Segmentation; Semantic 3D Modeling; Semantic Labeling; Volumetric Reconstruction"
"Amor B.B., Su J., Srivastava A.","Action Recognition Using Rate-Invariant Analysis of Skeletal Shape Trajectories",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","1", 7115162,"1","13",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961618626&doi=10.1109%2fTPAMI.2015.2439257&partnerID=40&md5=3026a1646131f7b6af201f7923fe9318","We study the problem of classifying actions of human subjects using depth movies generated by Kinect or other depth sensors. Representing human body as dynamical skeletons, we study the evolution of their (skeletons') shapes as trajectories on Kendall's shape manifold. The action data is typically corrupted by large variability in execution rates within and across subjects and, thus, causing major problems in statistical analyses. To address that issue, we adopt a recently-developed framework of Su et al. [1] , [2] to this problem domain. Here, the variable execution rates correspond to re-parameterizations of trajectories, and one uses a parameterization-invariant metric for aligning, comparing, averaging, and modeling trajectories. This is based on a combination of transported square-root vector fields (TSRVFs) of trajectories and the standard Euclidean norm, that allows computational efficiency. We develop a comprehensive suite of computational tools for this application domain: smoothing and denoising skeleton trajectories using median filtering, up- and down-sampling actions in time domain, simultaneous temporal-registration of multiple actions, and extracting invertible Euclidean representations of actions. Due to invertibility these Euclidean representations allow both discriminative and generative models for statistical analysis. For instance, they can be used in a SVM-based classification of original actions, as demonstrated here using MSR Action-3D, MSR Daily Activity and 3D Action Pairs datasets. Using only the skeletal information, we achieve state-of-the-art classification results on these datasets. © 1979-2012 IEEE.","Action Recognition; Depth sensors; Manifold Trajectories; Riemannian geometry; Skeletal data"
"Ngo D.T., Ostlund J., Fua P.","Template-Based Monocular 3D Shape Recovery Using Laplacian Meshes",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","1", 7110605,"172","187",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961584100&doi=10.1109%2fTPAMI.2015.2435739&partnerID=40&md5=c5e95b2b2251de1542ae3f9928d22605","We show that by extending the Laplacian formalism, which was first introduced in the Graphics community to regularize 3D meshes, we can turn the monocular 3D shape reconstruction of a deformable surface given correspondences with a reference image into a much better-posed problem. This allows us to quickly and reliably eliminate outliers by simply solving a linear least squares problem. This yields an initial 3D shape estimate, which is not necessarily accurate, but whose 2D projections are. The initial shape is then refined by a constrained optimization problem to output the final surface reconstruction. Our approach allows us to reduce the dimensionality of the surface reconstruction problem without sacrificing accuracy, thus allowing for real-time implementations. © 1979-2012 IEEE.","Deformable surfaces; Laplacian formalism; Monocular shape recovery"
"Cho D., Kim S., Tai Y.-W., Kweon I.S.","Automatic Trimap Generation and Consistent Matting for Light-Field Images",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","PP","99", 7562518,"","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988312224&doi=10.1109%2fTPAMI.2016.2606397&partnerID=40&md5=5e152d2e9cb0e90472f4cdcd2a05c139","In this paper, we introduce an automatic approach to generate trimaps and consistent alpha mattes of foreground objects in a light-field image. Our method first performs binary segmentation to roughly segment a light-field image into foreground and background based on depth and color. Next, we estimate accurate trimaps through analyzing color distribution along the boundary of the segmentation using guided image filter and KL-divergence. In order to estimate consistent alpha mattes across sub-images, we utilize the epipolar plane image (EPI) where colors and alphas along the same epipolar line must be consistent. Since EPI of foreground and background are mixed in the matting area, we propagate the EPI from definite foreground/background regions to unknown regions by assuming depth variations within unknown regions are spatially smooth. Using the EPI constraint, we derive two solutions to estimate alpha when color samples along epipolar line are known, and unknown. To further enhance consistency, we refine the estimated alpha mattes by using the multi-image matting Laplacian with an additional EPI smoothness constraint. In experimental evaluations, we have created a dataset where the ground truth alpha mattes of light-field images were obtained by using the blue screen technique. A variety of experiments show that our proposed algorithm produces both visually and quantitatively high-quality alpha mattes for light-field images. © 1979-2012 IEEE.","Image Matting; Light-Field Image; Trimap"
"Lian W., Zhang L., Yang M.-H.","An Efficient Globally Optimal Algorithm for Asymmetric Point Matching",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","PP","99", 7555337,"","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988474851&doi=10.1109%2fTPAMI.2016.2603988&partnerID=40&md5=bf346ac1363446d694063b6e20152a62","Although the robust point matching algorithm has been demonstrated to be effective for non-rigid registration, there are several issues with the adopted deterministic annealing optimization technique. First, it is not globally optimal and regularization on the spatial transformation is needed for good matching results. Second, it tends to align the mass centers of two point sets. To address these issues, we propose a globally optimal algorithm for the robust point matching problem where each model point has a counterpart in scene set. By eliminating the transformation variables, we show that the original matching problem is reduced to a concave quadratic assignment problem where the objective function has a low rank Hessian matrix. This facilitates the use of large scale global optimization techniques. We propose a branch-and-bound algorithm based on rectangular subdivision where in each iteration, multiple rectangles are used to increase the chances of subdividing the one containing the global optimal solution. In addition, we present an efficient lower bounding scheme which has a linear assignment formulation and can be efficiently solved. Extensive experiments on synthetic and real datasets demonstrate the proposed algorithm performs favorably against the state-of-the-art methods in terms of robustness to outliers, matching accuracy, and run-time. © 1979-2012 IEEE.","branch and bound; concave optimization; linear assignment; point correspondence; robust point matching"
"Donahue J., Hendricks L.A., Rohrbach M., Venugopalan S., Guadarrama S., Saenko K., Darrell T.","Long-term recurrent convolutional networks for visual recognition and description",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","PP","99", 7558228,"","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988362667&doi=10.1109%2fTPAMI.2016.2599174&partnerID=40&md5=10afdaf946bc7e58d179fb39f5b0aac3","Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent are effective for tasks involving sequences, visual and otherwise. We describe a class of recurrent convolutional architectures which is end-to-end trainable and suitable for large-scale visual understanding tasks, and demonstrate the value of these models for activity recognition, image captioning, and video description. In contrast to previous models which assume a fixed visual representation or perform simple temporal averaging for sequential processing, recurrent convolutional models are 'doubly deep' in that they learn compositional representations in space and time. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Differentiable recurrent models are appealing in that they can directly map variable-length inputs (e.g., videos) to variable-length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent sequence models are directly connected to modern visual convolutional network models and can be jointly trained to learn temporal dynamics and convolutional perceptual representations. Our results show that such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined or optimized. © 1979-2012 IEEE.",
"Lyzinski V., Fishkind D.E., Fiori M., Vogelstein J.T., Priebe C.E., Sapiro G.","Graph Matching: Relax at Your Own Risk",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","1", 7091002,"60","73",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961627441&doi=10.1109%2fTPAMI.2015.2424894&partnerID=40&md5=c447b6364b848745b2be27a897783820","Graph matching - aligning a pair of graphs to minimize their edge disagreements - has received wide-spread attention from both theoretical and applied communities over the past several decades, including combinatorics, computer vision, and connectomics. Its attention can be partially attributed to its computational difficulty. Although many heuristics have previously been proposed in the literature to approximately solve graph matching, very few have any theoretical support for their performance. A common technique is to relax the discrete problem to a continuous problem, therefore enabling practitioners to bring gradient-descent-type algorithms to bear. We prove that an indefinite relaxation (when solved exactly) almost always discovers the optimal permutation, while a common convex relaxation almost always fails to discover the optimal permutation. These theoretical results suggest that initializing the indefinite algorithm with the convex optimum might yield improved practical performance. Indeed, experimental results illuminate and corroborate these theoretical findings, demonstrating that excellent results are achieved in both benchmark and real data problems by amalgamating the two approaches. © 1979-2012 IEEE.","Assignment problem; Convex optimization; Frank-Wolfe; Graph matching; Random graphs"
"Zhang Z., Torr P.H.S.","Object Proposal Generation Using Two-Stage Cascade SVMs",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","1", 7102767,"102","115",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961615159&doi=10.1109%2fTPAMI.2015.2430348&partnerID=40&md5=86a02420ff37fe5140125fc32000df84","Object proposal algorithms have shown great promise as a first step for object recognition and detection. Good object proposal generation algorithms require high object detection recall rate as well as low computational cost, because generating object proposals is usually utilized as a preprocessing step. The problem of how to accelerate the object proposal generation and evaluation process without decreasing recall is thus of great interest. In this paper, we propose a new object proposal generation method using two-stage cascade support vector machines (SVMs), where in the first stage linear filters are learned for predefined quantized scales/aspect-ratios independently, and in the second stage a global linear classifier is learned across all the quantized scales/aspect-ratios for calibration, so that all the windows from the first stage can be compared properly. The windows with highest scores from the second stage are kept as inputs to our new efficient proposal calibration algorithm to improve their localization quality significantly, resulting in our final object proposals. We explain our scale/aspect-ratio quantization scheme, and investigate the effects of combinations of ell-1 and ell-2 regularizers in cascade SVMs with/without ranking constraints in learning. Comprehensive experiments on VOC2007 dataset are conducted, and our method is comparable with the current state-of-the-art methods with much better computational efficiency. © 1979-2012 IEEE.","2D convolution; Cascade SVMs; Linear filters; Object proposal generation; Scale/Aspect-ratio quantization"
"Kan M., Shan S., Zhang H., Lao S., Chen X.","Multi-View Discriminant Analysis",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","1", 7110624,"188","194",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961625718&doi=10.1109%2fTPAMI.2015.2435740&partnerID=40&md5=4268bf1b1857438a63e564176c1af4e9","In many computer vision systems, the same object can be observed at varying viewpoints or even by different sensors, which brings in the challenging demand for recognizing objects from distinct even heterogeneous views. In this work we propose a Multi-view Discriminant Analysis (MvDA) approach, which seeks for a single discriminant common space for multiple views in a non-pairwise manner by jointly learning multiple view-specific linear transforms. Specifically, our MvDA is formulated to jointly solve the multiple linear transforms by optimizing a generalized Rayleigh quotient, i.e., maximizing the between-class variations and minimizing the within-class variations from both intra-view and inter-view in the common space. By reformulating this problem as a ratio trace problem, the multiple linear transforms are achieved analytically and simultaneously through generalized eigenvalue decomposition. Furthermore, inspired by the observation that different views share similar data structures, a constraint is introduced to enforce the view-consistency of the multiple linear transforms. The proposed method is evaluated on three tasks: face recognition across pose, photo versus. sketch face recognition, and visual light image versus near infrared image face recognition on Multi-PIE, CUFSF and HFB databases respectively. Extensive experiments show that our MvDA achieves significant improvements compared with the best known results. © 1979-2012 IEEE.","Common Space; Cross-view Recognition; Heterogeneous Recognition; Multi-view Discriminant Analysis"
"Zhou X., Zhu M., Leonardos S., Daniilidis K.","Sparse representation for 3D shape estimation: A convex relaxation approach",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","PP","99", 7558185,"","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988422031&doi=10.1109%2fTPAMI.2016.2605097&partnerID=40&md5=ce5bcfe7f825a9afd572a7a3b8be672e","We investigate the problem of estimating the 3D shape of an object defined by a set of 3D landmarks, given their 2D correspondences in a single image. A successful approach to alleviating the reconstruction ambiguity is the 3D deformable shape model and a sparse representation is often used to capture complex shape variability. But the model inference is still a challenge due to the nonconvexity in optimization resulted from the joint estimation of shape and viewpoint. In contrast to prior work that relies on an alternating scheme with solutions depending on initialization, we propose a convex approach to addressing this challenge and develop an efficient algorithm to solve the proposed convex program. We further propose a robust model to handle gross errors in the 2D correspondences. We demonstrate the exact recovery property of the proposed method, the advantage compared to nonconvex baseline methods and the applicability to recover 3D human poses and car models from single images. © 1979-2012 IEEE.","3D reconstruction; convex optimization; sparse representation"
"Lombardi S., Nishino K.","Reflectance and Illumination Recovery in the Wild",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","1", 7102772,"129","141",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961629089&doi=10.1109%2fTPAMI.2015.2430318&partnerID=40&md5=056cd0308e3c92c38a8ef57fc3242697","The appearance of an object in an image encodes invaluable information about that object and the surrounding scene. Inferring object reflectance and scene illumination from an image would help us decode this information: reflectance can reveal important properties about the materials composing an object; the illumination can tell us, for instance, whether the scene is indoors or outdoors. Recovering reflectance and illumination from a single image in the real world, however, is a difficult task. Real scenes illuminate objects from every visible direction and real objects vary greatly in reflectance behavior. In addition, the image formation process introduces ambiguities, like color constancy, that make reversing the process ill-posed. To address this problem, we propose a Bayesian framework for joint reflectance and illumination inference in the real world. We develop a reflectance model and priors that precisely capture the space of real-world object reflectance and a flexible illumination model that can represent real-world illumination with priors that combat the deleterious effects of image formation. We analyze the performance of our approach on a set of synthetic data and demonstrate results on real-world scenes. These contributions enable reliable reflectance and illumination inference in the real world. © 1979-2012 IEEE.","DSBRDF; natural illumination estimation; real-world reflectance; Reflectance estimation"
"Purkait P., Chin T.-J., Sadri A., Suter D.","Clustering with Hypergraphs: The Case for Large Hyperedges",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","PP","99", 7582510,"","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992076931&doi=10.1109%2fTPAMI.2016.2614980&partnerID=40&md5=b1830aedb2fcf0c1695d1df5ebbf1c38","The extension of conventional clustering to hypergraph clustering, which involves higher order similarities instead of pairwise similarities, is increasingly gaining attention in computer vision. This is due to the fact that many clustering problems require an affinity measure that must involve a subset of data of size more than two. In the context of hypergraph clustering, the calculation of such higher order similarities on data subsets gives rise to hyperedges. Almost all previous work on hypergraph clustering in computer vision, however, has considered the smallest possible hyperedge size, due to a lack of study into the potential benefits of large hyperedges and effective algorithms to generate them. In this paper, we show that large hyperedges are better from both a theoretical and an empirical standpoint. We then propose a novel guided sampling strategy for large hyperedges, based on the concept of random cluster models. Our method can generate large pure hyperedges that significantly improve grouping accuracy without exponential increases in sampling costs. We demonstrate the efficacy of our technique on various higher-order grouping problems. In particular, we show that our approach improves the accuracy and efficiency of motion segmentation from dense, long-term, trajectories. © 2016 IEEE.","Higher order grouping; hypergraph clustering; motion segmentation"
"Jalba A.C., Sobiecki A., Telea A.C.","An Unified Multiscale Framework for Planar, Surface, and Curve Skeletonization",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","1", 7066924,"30","45",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961620572&doi=10.1109%2fTPAMI.2015.2414420&partnerID=40&md5=d55aa47d6b2cf153559a2e97299cf519","Computing skeletons of 2D shapes, and medial surface and curve skeletons of 3D shapes, is a challenging task. In particular, there is no unified framework that detects all types of skeletons using a single model, and also produces a multiscale representation which allows to progressively simplify, or regularize, all skeleton types. In this paper, we present such a framework. We model skeleton detection and regularization by a conservative mass transport process from a shape's boundary to its surface skeleton, next to its curve skeleton, and finally to the shape center. The resulting density field can be thresholded to obtain a multiscale representation of progressively simplified surface, or curve, skeletons. We detail a numerical implementation of our framework which is demonstrably stable and has high computational efficiency. We demonstrate our framework on several complex 2D and 3D shapes. © 1979-2012 IEEE.","based shape processing; Medial axes; Physically; Skeleton regularization"
"Koppula H.S., Saxena A.","Anticipating Human Activities Using Object Affordances for Reactive Robotic Response",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","1", 7102751,"14","29",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961588783&doi=10.1109%2fTPAMI.2015.2430335&partnerID=40&md5=8b695548eb29e2ac583f1cde7c8b423d","An important aspect of human perception is anticipation, which we use extensively in our day-to-day activities when interacting with other humans as well as with our surroundings. Anticipating which activities will a human do next (and how) can enable an assistive robot to plan ahead for reactive responses. Furthermore, anticipation can even improve the detection accuracy of past activities. The challenge, however, is two-fold: We need to capture the rich context for modeling the activities and object affordances, and we need to anticipate the distribution over a large space of future human activities. In this work, we represent each possible future using an anticipatory temporal conditional random field (ATCRF) that models the rich spatial-temporal relations through object affordances. We then consider each ATCRF as a particle and represent the distribution over the potential futures using a set of particles. In extensive evaluation on CAD-120 human activity RGB-D dataset, we first show that anticipation improves the state-of-the-art detection results. We then show that for new subjects (not seen in the training set), we obtain an activity anticipation accuracy (defined as whether one of top three predictions actually happened) of 84.1, 74.4 and 62.2 percent for an anticipation time of 1, 3 and 10 seconds respectively. Finally, we also show a robot using our algorithm for performing a few reactive responses. © 1979-2012 IEEE.","3D Activity Understanding; Human Activity Anticipation; Machine Learning; RGBD Data; Robotics Perception"
"Dong J., Chen Q., Huang Z., Yang J., Yan S.","Parsing Based on Parselets: A Unified Deformable Mixture Model for Human Parsing",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","1", 7080924,"88","101",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961566804&doi=10.1109%2fTPAMI.2015.2420563&partnerID=40&md5=6a8d698599240adcb7183ab938faf63e","Human parsing, namely partitioning the human body into semantic regions, has drawn much attention recently for its wide applications in human-centric analysis. Previous works often consider solving the problem of human pose estimation as the prerequisite of human parsing. We argue that these approaches cannot obtain optimal pixel-level parsing due to the inconsistent targets between the different tasks. In this work, we directly address the problem of human parsing by using the novel Parselet representation as the building blocks of our parsing model. Parselets are a group of parsable segments which can generally be obtained by low-level over-segmentation algorithms and bear strong semantic meaning. We then build a deformable mixture parsing model (DMPM) for human parsing to simultaneously handle the deformation and multi-modalities of Parselets. The proposed model has two unique characteristics: (1) the possible numerous modalities of Parselet ensembles are exhibited as the 'And-Or' structure of sub-trees; (2) to further solve the practical problem of Parselet occlusion or absence, we directly model the visibility property at some leaf nodes. The DMPM thus directly solves the problem of human parsing by searching for the best graph configuration from a pool of Parselet hypotheses without intermediate tasks. Fast rejection based on hierarchical filtering is employed to ensure the overall efficiency. Comprehensive evaluations on a new large-scale human parsing dataset, which is crawled from the Internet, with high resolution and thoroughly annotated semantic labels at pixel-level, and also a benchmark dataset demonstrate the encouraging performance of the proposed approach. © 1979-2012 IEEE.","And Or Graph; Deformable Model; Human Parsing; Parselets"
"Henter G.E., Kleijn W.B.","Minimum entropy rate simplification of stochastic processes",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","PP","99", 7416224,"","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988424295&doi=10.1109%2fTPAMI.2016.2533382&partnerID=40&md5=5e5ee402dbf5356dc2117af880fd7b01","We propose minimum entropy rate simplification (MERS), an information-theoretic, parameterization-independent framework for simplifying generative models of stochastic processes. Applications include improving model quality for sampling tasks by concentrating the probability mass on the most characteristic and accurately described behaviors while de-emphasizing the tails, and obtaining clean models from corrupted data (nonparametric denoising). This is the opposite of the smoothing step commonly applied to classification models. Drawing on rate-distortion theory, MERS seeks the minimum entropy-rate process under a constraint on the dissimilarity between the original and simplified processes. We particularly investigate the Kullback-Leibler divergence rate as a dissimilarity measure, where, compatible with our assumption that the starting model is disturbed or inaccurate, the simplification rather than the starting model is used for the reference distribution of the divergence. This leads to analytic solutions for stationary and ergodic Gaussian processes and Markov chains. The same formulas are also valid for maximum-entropy smoothing under the same divergence constraint. In experiments, MERS successfully simplifies and denoises models from audio, text, speech, and meteorology. © 1979-2012 IEEE.","G.3.e Markov processes; G.3.p Stochastic processes; H.1.1.b Information theory; H.5.5.c Signal analysis, synthesis, and processing; I.2.7.b Language generation; I.5.1.e Statistical models"
"Mottaghi R., Fidler S., Yuille A., Urtasun R., Parikh D.","Human-Machine CRFs for Identifying Bottlenecks in Scene Understanding",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","1", 7115183,"74","87",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961639809&doi=10.1109%2fTPAMI.2015.2437377&partnerID=40&md5=5c0417c7139d64757f91848a751e9d38","Recent trends in image understanding have pushed for scene understanding models that jointly reason about various tasks such as object detection, scene recognition, shape analysis, contextual reasoning, and local appearance based classifiers. In this work, we are interested in understanding the roles of these different tasks in improved scene understanding, in particular semantic segmentation, object detection and scene recognition. Towards this goal, we 'plug-in' human subjects for each of the various components in a conditional random field model. Comparisons among various hybrid human-machine CRFs give us indications of how much 'head room' there is to improve scene understanding by focusing research efforts on various individual tasks. © 1979-2012 IEEE.","Human-Machine Hybrid; Object Detection; Scene Recognition; Scene Understanding; Semantic Segmentation"
"Yang B., Lei Y., Liu J., Li W.","Social collaborative filtering by trust",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","PP","99", 7558226,"","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988433518&doi=10.1109%2fTPAMI.2016.2605085&partnerID=40&md5=23db61b64ff753411a3f3e273193ad85","Recommender systems are used to accurately and actively provide users with potentially interesting information or services. Collaborative filtering is a widely adopted approach to recommendation, but sparse data and cold-start users are often barriers to providing high quality recommendations. To address such issues, we propose a novel method that works to improve the performance of collaborative filtering recommendations by integrating sparse rating data given by users and sparse social trust network among these same users. This is a model-based method that adopts matrix factorization technique that maps users into low-dimensional latent feature spaces in terms of their trust relationship, and aims to more accurately reflect the users reciprocal influence on the formation of their own opinions and to learn better preferential patterns of users for high-quality recommendations. We use four large-scale datasets to show that the proposed method performs much better, especially for cold start users, than state-of-the-art recommendation algorithms for social collaborative filtering based on trust. © 1979-2012 IEEE.","collaborative filtering; matrix factorization; Recommender system; trust network"
"Yang B., Pei H., Chen H., Liu J., Xia S.","Characterizing and discovering spatiotemporal social contact patterns for healthcare",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","PP","99", 7558120,"","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988428119&doi=10.1109%2fTPAMI.2016.2605095&partnerID=40&md5=f00071c2d11d7ca4656dfbc8bb721df5","During an epidemic, the spatial, temporal and demographic patterns of disease transmission are determined by multiple factors. In addition to the physiological properties of the pathogens and hosts, the social contact of the host population, which characterizes the reciprocal exposures of individuals to infection according to their demographic structure and various social activities, are also pivotal to understanding and predicting the prevalence of infectious diseases. How social contact is measured will affect the extent to which we can forecast the dynamics of infections in the real world. Most current work focuses on modeling the spatial patterns of static social contact. In this work, we use a novel perspective to address the problem of how to characterize and measure dynamic social contact during an epidemic. We propose an epidemic-model-based tensor deconvolution framework in which the spatiotemporal patterns of social contact are represented by the factors of the tensors. These factors can be discovered using a tensor deconvolution procedure with the integration of epidemic models based on rich types of data, mainly heterogeneous outbreak surveillance data, socio-demographic census data and physiological data from medical reports. Using reproduction models that include SIR/SIS/SEIR/SEIS models as case studies, the efficacy and applications of the proposed framework are theoretically analyzed, empirically validated and demonstrated through a set of rigorous experiments using both synthetic and real-world data. © 1979-2012 IEEE.","Epidemic modeling; Healthcare; Heterogeneous data mining; Spatiotemporal social contact; Tensor deconvolution"
"Zhou W., Yang M., Wang X., Li H., Lin Y., Tian Q.","Scalable Feature Matching by Dual Cascaded Scalar Quantization for Image Retrieval",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence","38","1", 7102746,"159","171",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961620164&doi=10.1109%2fTPAMI.2015.2430329&partnerID=40&md5=5347809ba5c227c6c3739307a4641bb6","In this paper, we investigate the problem of scalable visual feature matching in large-scale image search and propose a novel cascaded scalar quantization scheme in dual resolution. We formulate the visual feature matching as a range-based neighbor search problem and approach it by identifying hyper-cubes with a dual-resolution scalar quantization strategy. Specifically, for each dimension of the PCA-transformed feature, scalar quantization is performed at both coarse and fine resolutions. The scalar quantization results at the coarse resolution are cascaded over multiple dimensions to index an image database. The scalar quantization results over multiple dimensions at the fine resolution are concatenated into a binary super-vector and stored into the index list for efficient verification. The proposed cascaded scalar quantization (CSQ) method is free of the costly visual codebook training and thus is independent of any image descriptor training set. The index structure of the CSQ is flexible enough to accommodate new image features and scalable to index large-scale image database. We evaluate our approach on the public benchmark datasets for large-scale image retrieval. Experimental results demonstrate the competitive retrieval performance of the proposed method compared with several recent retrieval algorithms on feature quantization. © 1979-2012 IEEE.","cascaded scalar quantization; codebook training-free; dual resolution quantization; Large scale image retrieval"
"Cao Y., Brubaker M.A., Fleet D.J., Hertzmann A.","Efficient Optimization for Sparse Gaussian Process Regression",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7089279,"2415","2427",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960910592&doi=10.1109%2fTPAMI.2015.2424873&partnerID=40&md5=9338841848fac65dc3e57adece014046","We propose an efficient optimization algorithm to select a subset of training data as the inducing set for sparse Gaussian process regression. Previous methods either use different objective functions for inducing set and hyperparameter selection, or else optimize the inducing set by gradient-based continuous optimization. The former approaches are harder to interpret and suboptimal, whereas the latter cannot be applied to discrete input domains or to kernel functions that are not differentiable with respect to the input. The algorithm proposed in this work estimates an inducing set and the hyperparameters using a single objective. It can be used to optimize either the marginal likelihood or a variational free energy. Space and time complexity are linear in training set size, and the algorithm can be applied to large regression problems on discrete or continuous domains. Empirical evaluation shows state-of-art performance in discrete cases, competitive prediction results as well as a favorable trade-off between training and test time in continuous cases. © 2015 IEEE.","Gaussian process regression; low rank; matrix factorization; sparsity"
"Li J., Duan L.-Y., Chen X., Huang T., Tian Y.","Finding the Secret of Image Saliency in the Frequency Domain",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7089301,"2428","2440",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960474040&doi=10.1109%2fTPAMI.2015.2424870&partnerID=40&md5=05022eaa4fc58207563153452b01eb98","There are two sides to every story of visual saliency modeling in the frequency domain. On the one hand, image saliency can be effectively estimated by applying simple operations to the frequency spectrum. On the other hand, it is still unclear which part of the frequency spectrum contributes the most to popping-out targets and suppressing distractors. Toward this end, this paper tentatively explores the secret of image saliency in the frequency domain. From the results obtained in several qualitative and quantitative experiments, we find that the secret of visual saliency may mainly hide in the phases of intermediate frequencies. To explain this finding, we reinterpret the concept of discrete Fourier transform from the perspective of template-based contrast computation and thus develop several principles for designing the saliency detector in the frequency domain. Following these principles, we propose a novel approach to design the saliency detector under the assistance of prior knowledge obtained through both unsupervised and supervised learning processes. Experimental results on a public image benchmark show that the learned saliency detector outperforms 18 state-of-the-art approaches in predicting human fixations. © 2015 IEEE.","experimental study; fixation prediction; Fourier transform; Image saliency; learning-based; spectral analysis"
"Attanasi A., Cavagna A., Del Castello L., Giardina I., Jelić A., Melillo S., Parisi L., Pellacini F., Shen E., Silvestri E., Viale M.","GReTA-A Novel Global and Recursive Tracking Algorithm in Three Dimensions",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7062911,"2451","2463",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960471981&doi=10.1109%2fTPAMI.2015.2414427&partnerID=40&md5=cf09de15f208b9c8d2af594110917068","Tracking multiple moving targets allows quantitative measure of the dynamic behavior in systems as diverse as animal groups in biology, turbulence in fluid dynamics and crowd and traffic control. In three dimensions, tracking several targets becomes increasingly hard since optical occlusions are very likely, i.e., two featureless targets frequently overlap for several frames. Occlusions are particularly frequent in biological groups such as bird flocks, fish schools, and insect swarms, a fact that has severely limited collective animal behavior field studies in the past. This paper presents a 3D tracking method that is robust in the case of severe occlusions. To ensure robustness, we adopt a global optimization approach that works on all objects and frames at once. To achieve practicality and scalability, we employ a divide and conquer formulation, thanks to which the computational complexity of the problem is reduced by orders of magnitude. We tested our algorithm with synthetic data, with experimental data of bird flocks and insect swarms and with public benchmark datasets, and show that our system yields high quality trajectories for hundreds of moving targets with severe overlap. The results obtained on very heterogeneous data show the potential applicability of our method to the most diverse experimental situations. © 2015 IEEE.","3D; branching; divide and conquer; global optimization; multi-object; multi-path; recursion; Tracking"
"Shen X., Yan Q., Xu L., Ma L., Jia J.","Multispectral Joint Image Restoration via Optimizing a Scale Map",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7081751,"2518","2530",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960903952&doi=10.1109%2fTPAMI.2015.2417569&partnerID=40&md5=810a52dfdb5ba7afa52c9bdcc1a3ccbd","Color, infrared and flash images captured in different fields can be employed to effectively eliminate noise and other visual artifacts. We propose a two-image restoration framework considering input images from different fields, for example, one noisy color image and one dark-flashed near-infrared image. The major issue in such a framework is to handle all structure divergence and find commonly usable edges and smooth transitions for visually plausible image reconstruction. We introduce a novel scale map as a competent representation to explicitly model derivative-level confidence and propose new functions and a numerical solver to effectively infer it following our important structural observations. Multispectral shadow detection is also used to make our system more robust. Our method is general and shows a principled way to solve multispectral restoration problems. © 2015 IEEE.","depth enhancement; image denoise; image restoration; joint filtering; multispectral image; shadow detection"
"Eynard D., Kovnatsky A., Bronstein M.M., Glashoff K., Bronstein A.M.","Multimodal Manifold Analysis by Simultaneous Diagonalization of Laplacians",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7053905,"2505","2517",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939541806&doi=10.1109%2fTPAMI.2015.2408348&partnerID=40&md5=5eeb72dbf800810058170fbc4c8623f4","We construct an extension of spectral and diffusion geometry to multiple modalities through simultaneous diagonalization of Laplacian matrices. This naturally extends classical data analysis tools based on spectral geometry, such as diffusion maps and spectral clustering. We provide several synthetic and real examples of manifold learning, object classification, and clustering, showing that the joint spectral geometry better captures the inherent structure of multi-modal data. We also show the relation of many previous approaches for multimodal manifold analysis to our framework. © 2015 IEEE.","diffusion distances; dimensionality reduction; Joint diagonalization; Laplace-Beltrami operator; manifold alignment; manifold learning; multimodal clustering; multimodal data"
"Bai X., Bai S., Zhu Z., Latecki L.J.","3D Shape Matching via Two Layer Coding",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7089309,"2361","2373",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960895286&doi=10.1109%2fTPAMI.2015.2424863&partnerID=40&md5=f1604122fab188f784f0141d415c79d1","View-based 3D shape retrieval is a popular branch in 3D shape analysis owing to the high discriminative property of 2D views. However, many previous works do not scale up to large 3D shape databases. We propose a two layer coding (TLC) framework to conduct shape matching much more efficiently. The first layer coding is applied to pairs of views represented as depth images. The spatial relationship of each view pair is captured with so-called eigen-angle, which is the planar angle between the two views measured at the center of the 3D shape. Prior to the second layer coding, the view pairs are divided into subsets according to their eigen-angles. Consequently, view pairs that differ significantly in their eigen-angles are encoded with different codewords, which implies that spatial arrangement of views is preserved in the second layer coding. The final feature vector of a 3D shape is the concatenation of all the encoded features from different subsets, which is used for efficient indexing directly. TLC is not limited to encode the local features from 2D views, but can be also applied to encoding 3D features. Exhaustive experimental results confirm that TLC achieves state-of-the-art performance in both retrieval accuracy and efficiency. © 2015 IEEE.","3D shape matching; bag of features; large scale; shape retrieval; two layer coding"
"Wang S., Wang Y., Zhu S.-C.","Learning Hierarchical Space Tiling for Scene Modeling, Parsing and Attribute Tagging",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7091016,"2478","2491",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960935850&doi=10.1109%2fTPAMI.2015.2424880&partnerID=40&md5=723048b3bbab64f8f98109c4952d6b2c","A typical scene category contains an enormous number of distinct scene configurations that are composed of objects and regions of varying shapes in different layouts. In this paper, we first propose a representation named hierarchical space tiling (HST) to quantize the huge and continuous scene configuration space. Then, we augment the HST with attributes (nouns and adjectives) to describe the semantics of the objects and regions inside a scene. We present a weakly supervised method for simultaneously learning the scene configurations and attributes from a collection of natural images associated with descriptive text. The precise locations of attributes are unknown in the input and are mapped to the HST nodes through learning. Starting with a full HST, we iteratively estimate the HST model under a learning-by-parsing framework. Given a test image, we compute the most probable parse tree with the associated attributes by dynamic programming. We quantitatively analyze the representative efficiency of HST, show the learned representation is less ambiguous and has semantically meaningful inner concepts. In applications, we apply our model to four tasks: scene classification, attribute recognition, attribute localization, and pixel-wise scene labeling, and show the performance improvements as well as higher efficiency. © 2015 IEEE.","hierarchical space tiling; scene attributes; Scene representation"
"Ulén J., Strandmark P., Kahl F.","Shortest Paths with Higher-Order Regularization",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7054488,"2588","2600",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960943376&doi=10.1109%2fTPAMI.2015.2409869&partnerID=40&md5=0991eba6e64a13d9f764342be15c5d76","This paper describes a new method of finding thin, elongated structures in images and volumes. We use shortest paths to minimize very general functionals of higher-order curve properties, such as curvature and torsion. Our method uses line graphs to find the optimal path on a given discretization, often in the order of seconds on a single computer. The curves are then refined using local optimization making it possible to recover very smooth curves. We are able to place constraints on our curves such as maximum integrated curvature, or a maximum curvature at any point of the curve. To our knowledge, we are the first to perform experiments in three dimensions with curvature and torsion regularization. The largest graphs we process have over a hundred billion arcs. Experiments on medical images and in multi-view reconstruction show the significance and practical usefulness of higher order regularization. © 2015 IEEE.","curvature; Curve extraction; line graphs; optimization; torsion"
"Jayasumana S., Hartley R., Salzmann M., Li H., Harandi M.","Kernel Methods on Riemannian Manifolds with Gaussian RBF Kernels",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7063231,"2464","2477",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957828695&doi=10.1109%2fTPAMI.2015.2414422&partnerID=40&md5=3c92a7c1f0e6c09bddddc21087c9e9a6","In this paper, we develop an approach to exploiting kernel methods with manifold-valued data. In many computer vision problems, the data can be naturally represented as points on a Riemannian manifold. Due to the non-Euclidean geometry of Riemannian manifolds, usual Euclidean computer vision and machine learning algorithms yield inferior results on such data. In this paper, we define Gaussian radial basis function (RBF)-based positive definite kernels on manifolds that permit us to embed a given manifold with a corresponding metric in a high dimensional reproducing kernel Hilbert space. These kernels make it possible to utilize algorithms developed for linear spaces on nonlinear manifold-valued data. Since the Gaussian RBF defined with any given metric is not always positive definite, we present a unified framework for analyzing the positive definiteness of the Gaussian RBF on a generic metric space. We then use the proposed framework to identify positive definite kernels on two specific manifolds commonly encountered in computer vision: the Riemannian manifold of symmetric positive definite matrices and the Grassmann manifold, i.e., the Riemannian manifold of linear subspaces of a Euclidean space. We show that many popular algorithms designed for Euclidean spaces, such as support vector machines, discriminant analysis and principal component analysis can be generalized to Riemannian manifolds with the help of such positive definite Gaussian kernels. © 2015 IEEE.","Gaussian RBF kernels; Grassmann manifolds; kernel methods; positive definite kernels; Riemannian manifolds; symmetric positive definite matrices"
"Perina A., Jojic N.","Capturing Spatial Interdependence in Image Features: The Counting Grid, an Epitomic Representation for Bags of Features",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7089289,"2374","2387",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960897213&doi=10.1109%2fTPAMI.2015.2424864&partnerID=40&md5=28192da467f8c5ea067b8c253a8a3c65","In recent scene recognition research images or large image regions are often represented as disorganized 'bags' of features which can then be analyzed using models originally developed to capture co-variation of word counts in text. However, image feature counts are likely to be constrained in different ways than word counts in text. For example, as a camera pans upwards from a building entrance over its first few floors and then further up into the sky Fig. 1 Fig. 1. Feature counts change slightly as the field of view moves. For example, the abundance of the 'car' features is reduced, but the counts of the features found on building facades are increased. The counting grid model accounts for such changes naturally, and it can also account for images of different scenes. © 2015 IEEE.","Bag of features; scene analysis; spatial layout"
"Liang X., Liu S., Shen X., Yang J., Liu L., Dong J., Lin L., Yan S.","Deep Human Parsing with Active Template Regression",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7053923,"2402","2414",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960902550&doi=10.1109%2fTPAMI.2015.2408360&partnerID=40&md5=afa7447f6f279c541d27d0d3ef1a6ec7","In this work, the human parsing task, namely decomposing a human image into semantic fashion/body regions, is formulated as an active template regression (ATR) problem, where the normalized mask of each fashion/body item is expressed as the linear combination of the learned mask templates, and then morphed to a more precise mask with the active shape parameters, including position, scale and visibility of each semantic region. The mask template coefficients and the active shape parameters together can generate the human parsing results, and are thus called the structure outputs for human parsing. The deep Convolutional Neural Network (CNN) is utilized to build the end-to-end relation between the input human image and the structure outputs for human parsing. More specifically, the structure outputs are predicted by two separate networks. The first CNN network is with max-pooling, and designed to predict the template coefficients for each label mask, while the second CNN network is without max-pooling to preserve sensitivity to label mask position and accurately predict the active shape parameters. For a new image, the structure outputs of the two networks are fused to generate the probability of each label for each pixel, and super-pixel smoothing is finally used to refine the human parsing result. Comprehensive evaluations on a large dataset well demonstrate the significant superiority of the ATR framework over other state-of-the-arts for human parsing. In particular, the F1-score reaches 64.38 percent by our ATR framework, significantly higher than 44.76 percent based on the state-of-the-art algorithm [28]. © 2015 IEEE.","active shape network; active template network; Active template regression; CNN; human parsing"
"Zhang S., Yang M., Wang X., Lin Y., Tian Q.","Semantic-Aware Co-Indexing for Image Retrieval",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7072494,"2573","2587",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960501739&doi=10.1109%2fTPAMI.2015.2417573&partnerID=40&md5=3bda7cd7400876d392410613de4ae1f7","In content-based image retrieval, inverted indexes allow fast access to database images and summarize all knowledge about the database. Indexing multiple clues of image contents allows retrieval algorithms search for relevant images from different perspectives, which is appealing to deliver satisfactory user experiences. However, when incorporating diverse image features during online retrieval, it is challenging to ensure retrieval efficiency and scalability. In this paper, for large-scale image retrieval, we propose a semantic-aware co-indexing algorithm to jointly embed two strong cues into the inverted indexes: 1) local invariant features that are robust to delineate low-level image contents, and 2) semantic attributes from large-scale object recognition that may reveal image semantic meanings. Specifically, for an initial set of inverted indexes of local features, we utilize semantic attributes to filter out isolated images and insert semantically similar images to this initial set. Encoding these two distinct and complementary cues together effectively enhances the discriminative capability of inverted indexes. Such co-indexing operations are totally off-line and introduce small computation overhead to online retrieval, because only local features but no semantic attributes are employed for the query. Hence, this co-indexing is different from existing image retrieval methods fusing multiple features or retrieval results. Extensive experiments and comparisons with recent retrieval methods manifest the competitive performance of our method. © 2015 IEEE.","Deep CNN; Inverted Indexing; Large-scale Image Retrieval; Semantic Attributes; Vocabulary Trees"
"Xu C., Tao D., Xu C.","Multi-View Intact Space Learning",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7072521,"2531","2544",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960497436&doi=10.1109%2fTPAMI.2015.2417578&partnerID=40&md5=b48d66fe7ddee794aa7dfc5b58e6ac96","It is practical to assume that an individual view is unlikely to be sufficient for effective multi-view learning. Therefore, integration of multi-view information is both valuable and necessary. In this paper, we propose the Multi-view Intact Space Learning (MISL) algorithm, which integrates the encoded complementary information in multiple views to discover a latent intact representation of the data. Even though each view on its own is insufficient, we show theoretically that by combing multiple views we can obtain abundant information for latent intact space learning. Employing the Cauchy loss (a technique used in statistical learning) as the error measurement strengthens robustness to outliers. We propose a new definition of multi-view stability and then derive the generalization error bound based on multi-view stability and Rademacher complexity, and show that the complementarity between multiple views is beneficial for the stability and generalization. MISL is efficiently optimized using a novel Iteratively Reweight Residuals (IRR) technique, whose convergence is theoretically analyzed. Experiments on synthetic data and real-world datasets demonstrate that MISL is an effective and promising algorithm for practical applications. © 2015 IEEE.","Multi-view learning; robust algorithms"
"Chen H.-Y., Lin Y.-Y., Chen B.-Y.","Co-Segmentation Guided Hough Transform for Robust Feature Matching",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7080945,"2388","2401",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960495543&doi=10.1109%2fTPAMI.2015.2420556&partnerID=40&md5=d2da9f1b1544a1c4ecaffff3a7d225a7","We present an algorithm that integrates image co-segmentation into feature matching, and can robustly yield accurate and dense feature correspondences. Inspired by the fact that correct feature correspondences on the same object typically have coherent transformations, we cast the task of feature matching as a density estimation problem in the homography space. Specifically, we project the homographies of correspondence candidates into the parametric Hough space, in which geometric verification of correspondences can be activated by voting. The precision of matching is then boosted. On the other hand, we leverage image co-segmentation, which discovers object boundaries, to determine relevant voters and speed up Hough voting. In addition, correspondence enrichment can be achieved by inferring the concerted homographies that are propagated between the features within the same segments. The recall is hence increased. In our approach, feature matching and image co-segmentation are tightly coupled. Through an iterative optimization process, more and more correct correspondences are detected owing to object boundaries revealed by co-segmentation. The proposed approach is comprehensively evaluated. Promising experimental results on four datasets manifest its effectiveness. © 2015 IEEE.","co-segmentation; correspondence problems; energy minimization; Hough transform; Image feature matching"
"Behl A., Mohapatra P., Jawahar C.V., Kumar M.P.","Optimizing Average Precision Using Weakly Supervised Data",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7063224,"2545","2557",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960533407&doi=10.1109%2fTPAMI.2015.2414435&partnerID=40&md5=a2c6d1ebcb3fd42522dfb1783ab17605","Many tasks in computer vision, such as action classification and object detection, require us to rank a set of samples according to their relevance to a particular visual category. The performance of such tasks is often measured in terms of the average precision (ap). Yet it is common practice to employ the support vector machine ( svm) classifier, which optimizes a surrogate 0-1 loss. The popularity of svmcan be attributed to its empirical performance. Specifically, in fully supervised settings, svm tends to provide similar accuracy to ap-svm, which directly optimizes an ap-based loss. However, we hypothesize that in the significantly more challenging and practically useful setting of weakly supervised learning, it becomes crucial to optimize the right accuracy measure. In order to test this hypothesis, we propose a novel latent ap-svm that minimizes a carefully designed upper bound on the ap-based loss function over weakly supervised samples. Using publicly available datasets, we demonstrate the advantage of our approach over standard loss-based learning frameworks on three challenging problems: action classification, character recognition and object detection. © 2015 IEEE.","average precision; Latent SVM; Weakly supervised learning"
"Jiang H., Tian T.-P., Sclaroff S.","Scale and Rotation Invariant Matching Using Linearly Augmented Trees",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7054480,"2558","2572",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960882496&doi=10.1109%2fTPAMI.2015.2409880&partnerID=40&md5=60dfbc80e124e5a6445385ca427e5c85","We propose a novel linearly augmented tree method for efficient scale and rotation invariant object matching. The proposed method enforces pairwise matching consistency defined on trees, and high-order constraints on all the sites of a template. The pairwise constraints admit arbitrary metrics while the high-order constraints use L1 norms and therefore can be linearized. Such a linearly augmented tree formulation introduces hyperedges and loops into the basic tree structure. But, different from a general loopy graph, its special structure allows us to relax and decompose the optimization into a sequence of tree matching problems that are efficiently solvable by dynamic programming. The proposed method also works on continuous scale and rotation parameters; we can match with a scale up to any large value with the same efficiency. Our experiments on ground truth data and a variety of real images and videos show that the proposed method is efficient, accurate and reliable. © 2015 IEEE.","decomposition method; high-order model; linear optimization; linearly augmented tree; Object matching; scale and rotation invariance"
"Mirzaei H., Funt B.","Gaussian-Based Hue Descriptors",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7080920,"2441","2450",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960475498&doi=10.1109%2fTPAMI.2015.2420560&partnerID=40&md5=40fe183b41adc6509d0ca01c8f63e82a","A robust and accurate hue descriptor that is useful in modeling human color perception and for computer vision applications is explored. The hue descriptor is based on the peak wavelength of a Gaussian-like function (called a wraparound Gaussian) and is shown to correlate as well as CIECAM02 hue to the hue designators of papers from the Munsell and Natural Color System color atlases and to the hue names found in Moroney's Color Thesaurus. The new hue descriptor is also shown to be significantly more stable under a variety of illuminants than CIECAM02. The use of wraparound Gaussians as a hue model is similar in spirit to the use of subtractive Gaussians proposed by Mizokami et al., but overcomes many of their limitations. © 2015 IEEE.","Color; color atlas; Gaussian reflectance; hue; wraparound Gaussian"
"Piuze E., Sporring J., Siddiqi K.","Maurer-Cartan Forms for Fields on Surfaces: Application to Heart Fiber Geometry",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","12", 7053934,"2492","2504",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960941720&doi=10.1109%2fTPAMI.2015.2408352&partnerID=40&md5=b6777440c3d2cba85d043d2d808083de","We study the space of first order models of smooth frame fields using the method of moving frames. By exploiting the Maurer-Cartan matrix of connection forms we develop geometrical embeddings for frame fields which lie on spherical, ellipsoidal and generalized helicoid surfaces. We design methods for optimizing connection forms in local neighborhoods and apply these to a statistical analysis of heart fiber geometry, using diffusion magnetic resonance imaging. This application of moving frames corroborates and extends recent characterizations of muscle fiber orientation in the heart wall, but also provides for a rich geometrical interpretation. In particular, we can now obtain direct local measurements of the variation of the helix and transverse angles, of fiber fanning and twisting, and of the curvatures of the heart wall in which these fibers lie. © 2015 IEEE.","Differential geometry; diffusion MRI; heart wall myofibers; Maurer-Cartan form; moving frames"
"Lobel H., Vidal R., Soto A.","Learning Shared, Discriminative, and Compact Representations for Visual Recognition",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","11", 7053941,"2218","2231",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960938263&doi=10.1109%2fTPAMI.2015.2408349&partnerID=40&md5=41c95a6177e25a0e61e2e6d3fdb964c8","Dictionary-based and part-based methods are among the most popular approaches to visual recognition. In both methods, a mid-level representation is built on top of low-level image descriptors and high-level classifiers are trained on top of the mid-level representation. While earlier methods built the mid-level representation without supervision, there is currently great interest in learning both representations jointly to make the mid-level representation more discriminative. In this work we propose a new approach to visual recognition that jointly learns a shared, discriminative, and compact mid-level representation and a compact high-level representation. By using a structured output learning framework, our approach directly handles the multiclass case at both levels of abstraction. Moreover, by using a group-sparse prior in the structured output learning framework, our approach encourages sharing of visual words and thus reduces the number of words used to represent each class. We test our proposed method on several popular benchmarks. Our results show that, by jointly learning mid- and high-level representations, and fostering the sharing of discriminative visual words among target classes, we are able to achieve state-of-the-art recognition performance using far less visual words than previous approaches. © 2015 IEEE.","Dictionary Learning; Group Sparsity; Image Categorization; Max-margin Learning; Structural SVMs"
"Su Z., Wang Y., Shi R., Zeng W., Sun J., Luo F., Gu X.","Optimal Mass Transport for Shape Matching and Comparison",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","11", 7053911,"2246","2259",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960913566&doi=10.1109%2fTPAMI.2015.2408346&partnerID=40&md5=fdf30510a82df792e81a9602a97b887b","Surface based 3D shape analysis plays a fundamental role in computer vision and medical imaging. This work proposes to use optimal mass transport map for shape matching and comparison, focusing on two important applications including surface registration and shape space. The computation of the optimal mass transport map is based on Monge-Brenier theory, in comparison to the conventional method based on Monge-Kantorovich theory, this method significantly improves the efficiency by reducing computational complexity from O(n2) to O(n). For surface registration problem, one commonly used approach is to use conformal map to convert the shapes into some canonical space. Although conformal mappings have small angle distortions, they may introduce large area distortions which are likely to cause numerical instability thus resulting failures of shape analysis. This work proposes to compose the conformal map with the optimal mass transport map to get the unique area-preserving map, which is intrinsic to the Riemannian metric, unique, and diffeomorphic. For shape space study, this work introduces a novel Riemannian framework, Conformal Wasserstein Shape Space, by combing conformal geometry and optimal mass transport theory. In our work, all metric surfaces with the disk topology are mapped to the unit planar disk by a conformal mapping, which pushes the area element on the surface to a probability measure on the disk. The optimal mass transport provides a map from the shape space of all topological disks with metrics to the Wasserstein space of the disk and the pullback Wasserstein metric equips the shape space with a Riemannian metric. We validate our work by numerous experiments and comparisons with prior approaches and the experimental results demonstrate the efficiency and efficacy of our proposed approach. © 2015 IEEE.","optimal mass transport; shape representation; shape space; surface matching"
"Galimzianova A., Pernuš F., Likar B., Špiclin Ž.","Robust Estimation of Unbalanced Mixture Models on Samples with Outliers",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","11", 7045499,"2273","2285",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944901764&doi=10.1109%2fTPAMI.2015.2404835&partnerID=40&md5=cfc57149262fb6f5cfc41f640f191854","Mixture models are often used to compactly represent samples from heterogeneous sources. However, in real world, the samples generally contain an unknown fraction of outliers and the sources generate different or unbalanced numbers of observations. Such unbalanced and contaminated samples may, for instance, be obtained by high density data sensors such as imaging devices. Estimation of unbalanced mixture models from samples with outliers requires robust estimation methods. In this paper, we propose a novel robust mixture estimator incorporating trimming of the outliers based on component-wise confidence level ordering of observations. The proposed method is validated and compared to the state-of-the-art FAST-TLE method on two data sets, one consisting of synthetic samples with a varying fraction of outliers and a varying balance between mixture weights, while the other data set contained structural magnetic resonance images of the brain with tumors of varying volumes. The results on both data sets clearly indicate that the proposed method is capable to robustly estimate unbalanced mixtures over a broad range of outlier fractions. As such, it is applicable to real-world samples, in which the outlier fraction cannot be estimated in advance. © 2015 IEEE.","brain structure segmentation; expectation-maximization; magnetic resonance imaging (MRI); Mixture model; outlier detection; robust estimation; trimmed likelihood estimation"
"Heo J.-P., Lee Y., He J., Chang S.-F., Yoon S.-E.","Spherical Hashing: Binary Code Embedding with Hyperspheres",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","11", 7051256,"2304","2316",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948404548&doi=10.1109%2fTPAMI.2015.2408363&partnerID=40&md5=1656fd394c3a44fc12ed6ab5104bb446","Many binary code embedding schemes have been actively studied recently, since they can provide efficient similarity search, and compact data representations suitable for handling large scale image databases. Existing binary code embedding techniques encode high-dimensional data by using hyperplane-based hashing functions. In this paper we propose a novel hypersphere-based hashing function, spherical hashing, to map more spatially coherent data points into a binary code compared to hyperplane-based hashing functions. We also propose a new binary code distance function, spherical Hamming distance, tailored for our hypersphere-based binary coding scheme, and design an efficient iterative optimization process to achieve both balanced partitioning for each hash function and independence between hashing functions. Furthermore, we generalize spherical hashing to support various similarity measures defined by kernel functions. Our extensive experiments show that our spherical hashing technique significantly outperforms state-of-the-art techniques based on hyperplanes across various benchmarks with sizes ranging from one to 75 million of GIST, BoW and VLAD descriptors. The performance gains are consistent and large, up to 100 percent improvements over the second best method among tested methods. These results confirm the unique merits of using hyperspheres to encode proximity regions in high-dimensional spaces. Finally, our method is intuitive and easy to implement. © 2015 IEEE.","binary codes; Hashing; large-scale image search"
"Dal Mutto C., Zanuttigh P., Cortelazzo G.M.","Probabilistic ToF and Stereo Data Fusion Based on Mixed Pixels Measurement Models",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","11", 7053914,"2260","2272",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960937061&doi=10.1109%2fTPAMI.2015.2408361&partnerID=40&md5=eb799d420a97da903406e3839db500e1","This paper proposes a method for fusing data acquired by a ToF camera and a stereo pair based on a model for depth measurement by ToF cameras which accounts also for depth discontinuity artifacts due to the mixed pixel effect. Such model is exploited within both a ML and a MAP-MRF frameworks for ToF and stereo data fusion. The proposed MAP-MRF framework is characterized by site-dependent range values, a rather important feature since it can be used both to improve the accuracy and to decrease the computational complexity of standard MAP-MRF approaches. This paper, in order to optimize the site dependent global cost function characteristic of the proposed MAP-MRF approach, also introduces an extension to Loopy Belief Propagation which can be used in other contexts. Experimental data validate the proposed ToF measurements model and the effectiveness of the proposed fusion techniques. © 2015 IEEE.","Data Fusion; Loopy Belief Propagation; MAP-MRF; Mixed Pixels; Stereo; ToF"
"Fukui K., Maki A.","Difference Subspace and Its Generalization for Subspace-Based Methods",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","11", 7053916,"2164","2177",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960850495&doi=10.1109%2fTPAMI.2015.2408358&partnerID=40&md5=8bffe693846fe1056cb2cd6b18d33412","Subspace-based methods are known to provide a practical solution for image set-based object recognition. Based on the insight that local shape differences between objects offer a sensitive cue for recognition, this paper addresses the problem of extracting a subspace representing the difference components between class subspaces generated from each set of object images independently of each other. We first introduce the difference subspace (DS), a novel geometric concept between two subspaces as an extension of a difference vector between two vectors, and describe its effectiveness in analyzing shape differences. We then generalize it to the generalized difference subspace (GDS) for multi-class subspaces, and show the benefit of applying this to subspace and mutual subspace methods, in terms of recognition capability. Furthermore, we extend these methods to kernel DS (KDS) and kernel GDS (KGDS) by a nonlinear kernel mapping to deal with cases involving larger changes in viewing direction. In summary, the contributions of this paper are as follows: 1) a DS/KDS between two class subspaces characterizes shape differences between the two respectively corresponding objects, 2) the projection of an input vector onto a DS/KDS realizes selective visualization of shape differences between objects, and 3) the projection of an input vector or subspace onto a GDS/KGDS is extremely effective at extracting differences between multiple subspaces, and therefore improves object recognition performance. We demonstrate validity through shape analysis on synthetic and real images of 3D objects as well as extensive comparison of performance on classification tests with several related methods; we study the performance in face image classification on the Yale face database B+ and the CMU Multi-PIE database, and hand shape classification of multi-view images. © 2015 IEEE.","3D object recognition; canonical angles; difference subspace; mutual subspace method; Subspace method"
"Fu Y., Hospedales T.M., Xiang T., Gong S.","Transductive Multi-View Zero-Shot Learning",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","11", 7053935,"2332","2345",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941001216&doi=10.1109%2fTPAMI.2015.2408354&partnerID=40&md5=c5847840c8097a4816259269f929d539","Most existing zero-shot learning approaches exploit transfer learning via an intermediate semantic representation shared between an annotated auxiliary dataset and a target dataset with different classes and no annotation. A projection from a low-level feature space to the semantic representation space is learned from the auxiliary dataset and applied without adaptation to the target dataset. In this paper we identify two inherent limitations with these approaches. First, due to having disjoint and potentially unrelated classes, the projection functions learned from the auxiliary dataset/domain are biased when applied directly to the target dataset/domain. We call this problem the projection domain shift problem and propose a novel framework, transductive multi-view embedding, to solve it. The second limitation is the prototype sparsity problem which refers to the fact that for each target class, only a single prototype is available for zero-shot learning given a semantic representation. To overcome this problem, a novel heterogeneous multi-view hypergraph label propagation method is formulated for zero-shot learning in the transductive embedding space. It effectively exploits the complementary information offered by different semantic representations and takes advantage of the manifold structures of multiple representation spaces in a coherent manner. We demonstrate through extensive experiments that the proposed approach (1) rectifies the projection shift between the auxiliary and target domains, (2) exploits the complementarity of multiple semantic representations, (3) significantly outperforms existing methods for both zero-shot and N-shot recognition on three image and video benchmark datasets, and (4) enables novel cross-view annotation tasks. © 2015 IEEE.","heterogeneous hypergraph; multi-view Learning; Transducitve learning; transfer Learning; zero-shot Learning"
"Pepik B., Stark M., Gehler P., Schiele B.","Multi-View and 3D Deformable Part Models",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","11", 7053926,"2232","2245",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960851889&doi=10.1109%2fTPAMI.2015.2408347&partnerID=40&md5=9ada278c95b00da950dfe9c0280eef2b","As objects are inherently 3D, they have been modeled in 3D in the early days of computer vision. Due to the ambiguities arising from mapping 2D features to 3D models, 3D object representations have been neglected and 2D feature-based models are the predominant paradigm in object detection nowadays. While such models have achieved outstanding bounding box detection performance, they come with limited expressiveness, as they are clearly limited in their capability of reasoning about 3D shape or viewpoints. In this work, we bring the worlds of 3D and 2D object representations closer, by building an object detector which leverages the expressive power of 3D object representations while at the same time can be robustly matched to image evidence. To that end, we gradually extend the successful deformable part model [1] to include viewpoint information and part-level 3D geometry information, resulting in several different models with different level of expressiveness. We end up with a 3D object model, consisting of multiple object parts represented in 3D and a continuous appearance model. We experimentally verify that our models, while providing richer object hypotheses than the 2D object models, provide consistently better joint object localization and viewpoint estimation than the state-of-the-art multi-view and 3D object detectors on various benchmarks (KITTI [2], 3D object classes [3], Pascal3D+ [4], Pascal VOC 2007 [5], EPFL multi-view cars [6]). © 2015 IEEE.","3D object models; deformable part models; Object detection; structured output learning"
"Evangelidis G.D., Hansard M., Horaud R.","Fusion of Range and Stereo Data for High-Resolution Scene-Modeling",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","11", 7031946,"2178","2192",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960934939&doi=10.1109%2fTPAMI.2015.2400465&partnerID=40&md5=53385327d54495d821af3733bd91bfd4","This paper addresses the problem of range-stereo fusion, for the construction of high-resolution depth maps. In particular, we combine low-resolution depth data with high-resolution stereo data, in a maximum a posteriori (MAP) formulation. Unlike existing schemes that build on MRF optimizers, we infer the disparity map from a series of local energy minimization problems that are solved hierarchically, by growing sparse initial disparities obtained from the depth data. The accuracy of the method is not compromised, owing to three properties of the data-term in the energy function. First, it incorporates a new correlation function that is capable of providing refined correlations and disparities, via subpixel correction. Second, the correlation scores rely on an adaptive cost aggregation step, based on the depth data. Third, the stereo and depth likelihoods are adaptively fused, based on the scene texture and camera geometry. These properties lead to a more selective growing process which, unlike previous seed-growing methods, avoids the tendency to propagate incorrect disparities. The proposed method gives rise to an intrinsically efficient algorithm, which runs at 3FPS on 2.0 MP images on a standard desktop computer. The strong performance of the new method is established both by quantitative comparisons with state-of-the-art methods, and by qualitative comparisons using real depth-stereo data-sets. © 2015 IEEE.","maximum a posteriori; range data; seed-growing; sensor fusion; Stereo; time-of-flight camera"
"Shi Y., Gao Y., Liao S., Zhang D., Gao Y., Shen D.","Semi-Automatic Segmentation of Prostate in CT Images via Coupled Feature Representation and Spatial-Constrained Transductive Lasso",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","11", 7089297,"2286","2303",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960852320&doi=10.1109%2fTPAMI.2015.2424869&partnerID=40&md5=353819a874c158ff2d7db47dda7ab7f8","Conventional learning-based methods for segmenting prostate in CT images ignore the relations among the low-level features by assuming all these features are independent. Also, their feature selection steps usually neglect the image appearance changes in different local regions of CT images. To this end, we present a novel semi-automatic learning-based prostate segmentation method in this article. For segmenting the prostate in a certain treatment image, the radiation oncologist will be first asked to take a few seconds to manually specify the first and last slices of the prostate. Then, prostate is segmented with the following two steps: (i) Estimation of 3D prostate-likelihood map to predict the likelihood of each voxel being prostate by employing the coupled feature representation, and the proposed Spatial-COnstrained Transductive LassO (SCOTO); (ii) Multi-atlases based label fusion to generate the final segmentation result by using the prostate shape information obtained from both planning and previous treatment images. The major contribution of the proposed method mainly includes: (i) incorporating radiation oncologist's manual specification to aid segmentation, (ii) adopting coupled features to relax previous assumption of feature independency for voxel representation, and (iii) developing SCOTO for joint feature selection across different local regions. The experimental result shows that the proposed method outperforms the state-of-the-art methods in a real-world prostate CT dataset, consisting of 24 patients with totally 330 images, all of which were manually delineated by the radiation oncologist for performance evaluation. Moreover, our method is also clinically feasible, since the segmentation performance can be improved by just requiring the radiation oncologist to spend only a few seconds for manual specification of ending slices in the current treatment CT image. © 2015 IEEE.","feature representation; feature selection; label fusion; Prostate segmentation"
"Taneja A., Ballan L., Pollefeys M.","Geometric Change Detection in Urban Environments Using Images",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","11", 7045528,"2193","2206",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960486799&doi=10.1109%2fTPAMI.2015.2404834&partnerID=40&md5=8c3d434935abc1ebd5ee78db3e74c989","We propose a method to detect changes in the geometry of a city using panoramic images captured by a car driving around the city. The proposed method can be used to significantly optimize the process of updating the 3D model of an urban environment that is changing over time, by restricting this process to only those areas where changes are detected. With this application in mind, we designed our algorithm to specifically detect only structural changes in the environment, ignoring any changes in its appearance, and ignoring also all the changes which are not relevant for update purposes such as cars, people etc. The approach also accounts for the challenges involved in a large scale application of change detection, such as inaccuracies in the input geometry, errors in the geo-location data of the images as well as the limited amount of information due to sparse imagery. We evaluated our approach on a small scale setup using high resolution, densely captured images and a large scale setup covering an entire city using instead the more realistic scenario of low resolution, sparsely captured images. A quantitative evaluation was also conducted for the large scale setup consisting of 14,000 images. © 2015 IEEE.","Change detection; image registration; Streetview image application"
"Lin G., Shen C., Van Den Hengel A.","Supervised Hashing Using Graph Cuts and Boosted Decision Trees",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","11", 7044591,"2317","2331",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960485177&doi=10.1109%2fTPAMI.2015.2404776&partnerID=40&md5=6c92628c5b1d9a695ecd0946fc256549","To build large-scale query-by-example image retrieval systems, embedding image features into a binary Hamming space provides great benefits. Supervised hashing aims to map the original features to compact binary codes that are able to preserve label based similarity in the binary Hamming space. Most existing approaches apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of those methods, and can result in complex optimization problems that are difficult to solve. In this work we proffer a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. The proposed framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problem-specific hashing methods. Our framework decomposes the hashing learning problem into two steps: binary code (hash bit) learning and hash function learning. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training a standard binary classifier. For solving large-scale binary code inference, we show how it is possible to ensure that the binary quadratic problems are submodular such that efficient graph cut methods may be used. To achieve efficiency as well as efficacy on large-scale high-dimensional data, we propose to use boosted decision trees as the hash functions, which are nonlinear, highly descriptive, and are very fast to train and evaluate. Experiments demonstrate that the proposed method significantly outperforms most state-of-the-art methods, especially on high-dimensional data. © 2015 IEEE.","binary codes; decision trees; graph cuts; Hashing; image retrieval; nearest neighbour search"
"Zhang X., Qu Y., Yang D., Wang H., Kymer J.","Laplacian Scale-Space Behavior of Planar Curve Corners",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","11", 7018927,"2207","2217",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960916402&doi=10.1109%2fTPAMI.2015.2396074&partnerID=40&md5=16e433167a58961d01f1bcbc415ebd38","Scale-space behavior of corners is important for developing an efficient corner detection algorithm. In this paper, we analyze the scale-space behavior with the Laplacian of Gaussian (LoG) operator on a planar curve which constructs Laplacian Scale Space (LSS). The analytical expression of a Laplacian Scale-Space map (LSS map) is obtained, demonstrating the Laplacian Scale-Space behavior of the planar curve corners, based on a newly defined unified corner model. With this formula, some Laplacian Scale-Space behavior is summarized. Although LSS demonstrates some similarities to Curvature Scale Space (CSS), there are still some differences. First, no new extreme points are generated in the LSS. Second, the behavior of different cases of a corner model is consistent and simple. This makes it easy to trace the corner in a scale space. At last, the behavior of LSS is verified in an experiment on a digital curve. © 2015 IEEE.","Corner Detection; Laplacian of Gaussian; Planar Curve; Scale Space"
"Taha A.A., Hanbury A.","An Efficient Algorithm for Calculating the Exact Hausdorff Distance",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","11", 7053955,"2153","2163",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960862116&doi=10.1109%2fTPAMI.2015.2408351&partnerID=40&md5=c55000b1259ceadf83442e490f818249","The Hausdorff distance (HD) between two point sets is a commonly used dissimilarity measure for comparing point sets and image segmentations. Especially when very large point sets are compared using the HD, for example when evaluating magnetic resonance volume segmentations, or when the underlying applications are based on time critical tasks, like motion detection, then the computational complexity of HD algorithms becomes an important issue. In this paper we propose a novel efficient algorithm for computing the exact Hausdorff distance. In a runtime analysis, the proposed algorithm is demonstrated to have nearly-linear complexity. Furthermore, it has efficient performance for large point set sizes as well as for large grid size; performs equally for sparse and dense point sets; and finally it is general without restrictions on the characteristics of the point set. The proposed algorithm is tested against the HD algorithm of the widely used national library of medicine insight segmentation and registration toolkit (ITK) using magnetic resonance volumes with extremely large size. The proposed algorithm outperforms the ITK HD algorithm both in speed and memory required. In an experiment using trajectories from a road network, the proposed algorithm significantly outperforms an HD algorithm based on R-Trees. © 2015 IEEE.","Algorithm; Computational Complexity; Evaluation; Hausdorff distance; Runtime Analysis"
"Torii A., Sivic J., Okutomi M., Pajdla T.","Visual Place Recognition with Repetitive Structures",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","11", 7054472,"2346","2359",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960876812&doi=10.1109%2fTPAMI.2015.2409868&partnerID=40&md5=7e54cd1a4b0d7af6ac9bb627fc8da8ed","Repeated structures such as building facades, fences or road markings often represent a significant challenge for place recognition. Repeated structures are notoriously hard for establishing correspondences using multi-view geometry. They violate the feature independence assumed in the bag-of-visual-words representation which often leads to over-counting evidence and significant degradation of retrieval performance. In this work we show that repeated structures are not a nuisance but, when appropriately represented, they form an important distinguishing feature for many places. We describe a representation of repeated structures suitable for scalable retrieval and geometric verification. The retrieval is based on robust detection of repeated image structures and a suitable modification of weights in the bag-of-visual-word model. We also demonstrate that the explicit detection of repeated patterns is beneficial for robust visual word matching for geometric verification. Place recognition results are shown on datasets of street-level imagery from Pittsburgh and San Francisco demonstrating significant gains in recognition performance compared to the standard bag-of-visual-words baseline as well as the more recently proposed burstiness weighting and Fisher vector encoding. © 2015 IEEE.","bag of visual words; geometric verification; image retrieval; Place recognition"
"Gao S.-B., Yang K.-F., Li C.-Y., Li Y.-J.","Color Constancy Using Double-Opponency",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","10", 7018983,"1973","1985",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929347808&doi=10.1109%2fTPAMI.2015.2396053&partnerID=40&md5=b89d9c17791af88256198b4504c5d47a","The double-opponent (DO) color-sensitive cells in the primary visual cortex (V1) of the human visual system (HVS) have long been recognized as the physiological basis of color constancy. In this work we propose a new color constancy model by imitating the functional properties of the HVS from the single-opponent (SO) cells in the retina to the DO cells in V1 and the possible neurons in the higher visual cortexes. The idea behind the proposed double-opponency based color constancy (DOCC) model originates from the substantial observation that the color distribution of the responses of DO cells to the color-biased images coincides well with the vector denoting the light source color. Then the illuminant color is easily estimated by pooling the responses of DO cells in separate channels in LMS space with the pooling mechanism of sum or max. Extensive evaluations on three commonly used datasets, including the test with the dataset dependent optimal parameters, as well as the intra- and inter-dataset cross validation, show that our physiologically inspired DOCC model can produce quite competitive results in comparison to the state-of-the-art approaches, but with a relative simple implementation and without requiring fine-tuning of the method for each different dataset. © 2015 IEEE.","color constancy; double opponency; human visual system; pooling mechanism"
"Bartoli A., Gerard Y., Chadebecq F., Collins T., Pizarro D.","Shape-from-Template",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","10", 07010934,"2099","2118",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941219513&doi=10.1109%2fTPAMI.2015.2392759&partnerID=40&md5=36e1d9a2b33e6fdf3e6ac617685a523d","We study a problem that we call Shape-from-Template, which is the problem of reconstructing the shape of a deformable surface from a single image and a 3D template. Current methods in the literature address the case of isometric deformations, and relax the isometry constraint to the convex inextensibility constraint, solved using the so-called maximum depth heuristic. We call these methods zeroth-order since they use image point locations (the zeroth-order differential structure) to solve the shape inference problem from a perspective image. We propose a novel class of methods that we call first-order. The key idea is to use both image point locations and their first-order differential structure. The latter can be easily extracted from a warp between the template and the input image. We give a unified problem formulation as a system of PDEs for isometric and conformal surfaces that we solve analytically. This has important consequences. First, it gives the first analytical algorithms to solve this type of reconstruction problems. Second, it gives the first algorithms to solve for the exact constraints. Third, it allows us to study the well-posedness of this type of reconstruction: we establish that isometric surfaces can be reconstructed unambiguously and that conformal surfaces can be reconstructed up to a few discrete ambiguities and a global scale. In the latter case, the candidate solution surfaces are obtained analytically. Experimental results on simulated and real data show that our isometric methods generally perform as well as or outperform state of the art approaches in terms of reconstruction accuracy, while our conformal methods largely outperform all isometric methods for extensible deformations. © 1979-2012 IEEE.",
"Lu F., Matsushita Y., Sato I., Okabe T., Sato Y.","From Intensity Profile to Surface Normal: Photometric Stereo for Unknown Light Sources and Isotropic Reflectances",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","10", 7006767,"1999","2012",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941079841&doi=10.1109%2fTPAMI.2015.2389841&partnerID=40&md5=c6c5af6a6548ed566e09314e94e1a397","We propose an uncalibrated photometric stereo method that works with general and unknown isotropic reflectances. Our method uses a pixel intensity profile, which is a sequence of radiance intensities recorded at a pixel under unknown varying directional illumination. We show that for general isotropic materials and uniformly distributed light directions, the geodesic distance between intensity profiles is linearly related to the angular difference of their corresponding surface normals, and that the intensity distribution of the intensity profile reveals reflectance properties. Based on these observations, we develop two methods for surface normal estimation; one for a general setting that uses only the recorded intensity profiles, the other for the case where a BRDF database is available while the exact BRDF of the target scene is still unknown. Quantitative and qualitative evaluations are conducted using both synthetic and real-world scenes, which show the state-of-the-art accuracy of smaller than 10 degree without using reference data and 5 degree with reference data for all 100 materials in MERL database. © 2015 IEEE.","BRDF; general reflectance; intensity profile; Uncalibrated photometric stereo"
"Han L., Wilson R.C., Hancock E.R.","Generative Graph Prototypes from Information Theory",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","10", 7031923,"2013","2027",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941079746&doi=10.1109%2fTPAMI.2015.2400451&partnerID=40&md5=2e580a1b0927298a504ad107a99887c1","In this paper we present a method for constructing a generative prototype for a set of graphs by adopting a minimum description length approach. The method is posed in terms of learning a generative supergraph model from which the new samples can be obtained by an appropriate sampling mechanism. We commence by constructing a probability distribution for the occurrence of nodes and edges over the supergraph. We encode the complexity of the supergraph using an approximate Von Neumann entropy. A variant of the EM algorithm is developed to minimize the description length criterion in which the structure of the supergraph and the node correspondences between the sample graphs and the supergraph are treated as missing data. To generate new graphs, we assume that the nodes and edges of graphs arise under independent Bernoulli distributions and sample new graphs according to their node and edge occurrence probabilities. Empirical evaluations on real-world databases demonstrate the practical utility of the proposed algorithm and show the effectiveness of the generative model for the tasks of graph classification, graph clustering and generating new sample graphs. © 2015 IEEE.","Generative prototype; Jensen-Shannon divergence; Minimum description length criterion; Supergraph; Von Neumann entropy"
"Chakraborty S., Balasubramanian V., Sun Q., Panchanathan S., Ye J.","Active Batch Selection via Convex Relaxations with Guaranteed Solution Bounds",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","10", 7006697,"1945","1958",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941194771&doi=10.1109%2fTPAMI.2015.2389848&partnerID=40&md5=478e402b04c8d5876e4300f11a0a8d7b","Active learning techniques have gained popularity to reduce human effort in labeling data instances for inducing a classifier. When faced with large amounts of unlabeled data, such algorithms automatically identify the exemplar instances for manual annotation. More recently, there have been attempts towards a batch mode form of active learning, where a batch of data points is simultaneously selected from an unlabeled set. In this paper, we propose two novel batch mode active learning (BMAL) algorithms: BatchRank and BatchRand. We first formulate the batch selection task as an NP-hard optimization problem; we then propose two convex relaxations, one based on linear programming and the other based on semi-definite programming to solve the batch selection problem. Finally, a deterministic bound is derived on the solution quality for the first relaxation and a probabilistic bound for the second. To the best of our knowledge, this is the first research effort to derive mathematical guarantees on the solution quality of the BMAL problem. Our extensive empirical studies on $15$ binary, multi-class and multi-label challenging datasets corroborate that the proposed algorithms perform at par with the state-of-the-art techniques, deliver high quality solutions and are robust to real-world issues like label noise and class imbalance. © 1979-2012 IEEE.","Batch Mode Active Learning; Optimization"
"Harel M., Mannor S.","The Perturbed Variation",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","10", 7045555,"2119","2130",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941037492&doi=10.1109%2fTPAMI.2015.2404836&partnerID=40&md5=b9f9380d38ecd455af4a5353ef76aea6","We introduce a new discrepancy measure between two distributions that gives an indication on their similarity. The new measure, termed the Perturbed Variation (PV), gives an intuitive interpretation of similarity; it optimally perturbs the distributions so that they best fit each other. The PV is defined between continuous and discrete distributions, and can be efficiently estimated from samples. We provide bounds on the convergence of the estimated score to its distributional value, as well as robustness analysis of the PV to outliers. A number of possible applications of the score are presented, and its ability to detect similarity is compared with that of other known measures on real data. We also present a new visual tracking algorithm based on the PV, and compare its performance with known tracking algorithms. © 2015 IEEE.","discrepancy; distance; Distributional similarity; homogeneity testing"
"Shi Z., Hospedales T.M., Xiang T.","Bayesian Joint Modelling for Object Localisation in Weakly Labelled Images",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","10", 7010951,"1959","1972",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941089808&doi=10.1109%2fTPAMI.2015.2392769&partnerID=40&md5=5a67aae236987eb0ccd5b42301125d69","We address the problem of localisation of objects as bounding boxes in images and videos with weak labels. This weakly supervised object localisation problem has been tackled in the past using discriminative models where each object class is localised independently from other classes. In this paper, a novel framework based on Bayesian joint topic modelling is proposed, which differs significantly from the existing ones in that: (1) All foreground object classes are modelled jointly in a single generative model that encodes multiple object co-existence so that ""explaining away"" inference can resolve ambiguity and lead to better learning and localisation. (2) Image backgrounds are shared across classes to better learn varying surroundings and ""push out"" objects of interest. (3) Our model can be learned with a mixture of weakly labelled and unlabelled data, allowing the large volume of unlabelled images on the Internet to be exploited for learning. Moreover, the Bayesian formulation enables the exploitation of various types of prior knowledge to compensate for the limited supervision offered by weakly labelled data, as well as Bayesian domain adaptation for transfer learning. Extensive experiments on the PASCAL VOC, ImageNet and YouTube-Object videos datasets demonstrate the effectiveness of our Bayesian joint model for weakly supervised object localisation. © 2015 IEEE.","Bayesian Domain Transfer; Object Detection; Probabilistic Modelling; Topic Modelling; Weakly Supervised Learning"
"Xu Y., Qiu P., Roysam B.","Unsupervised Discovery of Subspace Trends",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","10", 7015603,"2131","2145",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941060074&doi=10.1109%2fTPAMI.2015.2394475&partnerID=40&md5=e5e40e7ceec297d554840144a615ce2e","This paper presents unsupervised algorithms for discovering previously unknown subspace trends in high-dimensional data sets without the benefit of prior information. A subspace trend is a sustained pattern of gradual/progressive changes within an unknown subset of feature dimensions. A fundamental challenge to subspace trend discovery is the presence of irrelevant data dimensions, noise, outliers, and confusion from multiple subspace trends driven by independent factors that are mixed in with each other. These factors can obscure the trends in conventional dimension reduction & projection based data visualizations. To overcome these limitations, we propose a novel graph-theoretic neighborhood similarity measure for detecting concordant progressive changes across data dimensions. Using this measure, we present an unsupervised algorithm for trend-relevant feature selection, subspace trend discovery, quantification of trend strength, and validation. Our method successfully identified verifiable subspace trends in diverse synthetic and real-world biomedical datasets. Visualizations derived from the selected trend-relevant features revealed biologically meaningful hidden subspace trend(s) that were obscured by irrelevant features and noise. Although our examples are drawn from the biological domain, the proposed algorithm is broadly applicable to exploratory analysis of high-dimensional data including visualization, hypothesis generation, knowledge discovery, and prediction in diverse other applications. © 2015 IEEE.","Multivariate Data Visualization; Subspace Trend Discovery; Trend-relevant Feature Selection"
"Rivera A.R., Chae O.","Spatiotemporal Directional Number Transitional Graph for Dynamic Texture Recognition",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","10", 7010973,"2146","2152",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941096844&doi=10.1109%2fTPAMI.2015.2392774&partnerID=40&md5=b44d7654dc16e0209eaa33434db90b5e","Spatiotemporal image descriptors are gaining attention in the image research community for better representation of dynamic textures. In this paper, we introduce a dynamic-micro-texture descriptor, i.e., spatiotemporal directional number transitional graph (DNG), which describes both the spatial structure and motion of each local neighborhood by capturing the direction of natural flow in the temporal domain. We use the structure of the local neighborhood, given by its principal directions, and compute the transition of such directions between frames. Moreover, we present the statistics of the direction transitions in a transitional graph, which acts as a signature for a given spatiotemporal region in the dynamic texture. Furthermore, we create a sequence descriptor by dividing the spatiotemporal volume into several regions, computing a transitional graph for each of them, and represent the sequence as a set of graphs. Our results validate the robustness of the proposed descriptor in different scenarios for expression recognition and dynamic texture analysis. © 2015 IEEE.","Directional number; dynamic texture; facial expression; spatiotemporal descriptors; transitional graph"
"Lu J., Liong V.E., Zhou X., Zhou J.","Learning Compact Binary Face Descriptor for Face Recognition",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","10", 7053942,"2041","2056",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941055050&doi=10.1109%2fTPAMI.2015.2408359&partnerID=40&md5=fda99bd380234f8fa485f6ddaf5ace21","Binary feature descriptors such as local binary patterns (LBP) and its variations have been widely used in many face recognition systems due to their excellent robustness and strong discriminative power. However, most existing binary face descriptors are hand-crafted, which require strong prior knowledge to engineer them by hand. In this paper, we propose a compact binary face descriptor (CBFD) feature learning method for face representation and recognition. Given each face image, we first extract pixel difference vectors (PDVs) in local patches by computing the difference between each pixel and its neighboring pixels. Then, we learn a feature mapping to project these pixel difference vectors into low-dimensional binary vectors in an unsupervised manner, where 1) the variance of all binary codes in the training set is maximized, 2) the loss between the original real-valued codes and the learned binary codes is minimized, and 3) binary codes evenly distribute at each learned bin, so that the redundancy information in PDVs is removed and compact binary codes are obtained. Lastly, we cluster and pool these binary codes into a histogram feature as the final representation for each face image. Moreover, we propose a coupled CBFD (C-CBFD) method by reducing the modality gap of heterogeneous faces at the feature level to make our method applicable to heterogeneous face recognition. Extensive experimental results on five widely used face datasets show that our methods outperform state-of-the-art face descriptors. © 2015 IEEE.","binary feature; biometrics; compact feature; Face recognition; feature learning; heterogeneous face matching"
"Idrees H., Soomro K., Shah M.","Detecting humans in dense crowds using locally-consistent scale prior and global occlusion reasoning",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","10", 7018985,"1986","1998",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941194361&doi=10.1109%2fTPAMI.2015.2396051&partnerID=40&md5=dd326b825ecaa156dae8a05f4149c57f","Human detection in dense crowds is an important problem, as it is a prerequisite to many other visual tasks, such as tracking, counting, action recognition or anomaly detection in behaviors exhibited by individuals in a dense crowd. This problem is challenging due to the large number of individuals, small apparent size, severe occlusions and perspective distortion. However, crowded scenes also offer contextual constraints that can be used to tackle these challenges. In this paper, we explore context for human detection in dense crowds in the form of a locally-consistent scale prior which captures the similarity in scale in local neighborhoods and its smooth variation over the image. Using the scale and confidence of detections obtained from an underlying human detector, we infer scale and confidence priors using Markov Random Field. In an iterative mechanism, the confidences of detection hypotheses are modified to reflect consistency with the inferred priors, and the priors are updated based on the new detections. The final set of detections obtained are then reasoned for occlusion using Binary Integer Programming where overlaps and relations between parts of individuals are encoded as linear constraints. Both human detection and occlusion reasoning in proposed approach are solved with local neighbor-dependent constraints, thereby respecting the inter-dependence between individuals characteristic to dense crowd analysis. In addition, we propose a mechanism to detect different combinations of body parts without requiring annotations for individual combinations. We performed experiments on a new and extremely challenging dataset of dense crowd images showing marked improvement over the underlying human detector. © 1979-2012 IEEE.","combinations-of-parts detection; crowd analysis; deformable parts model; dense crowds; global occlusion reasoning; human detection; locally-consistent scale prior; Markov Random Field; scale context; spatial priors"
"Orchard G., Meyer C., Etienne-Cummings R., Posch C., Thakor N., Benosman R.","HFirst: A Temporal Approach to Object Recognition",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","10", 7010933,"2028","2040",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941029030&doi=10.1109%2fTPAMI.2015.2392947&partnerID=40&md5=fa57f22d76b8fc1fbd2fab341de66171","This paper introduces a spiking hierarchical model for object recognition which utilizes the precise timing information inherently present in the output of biologically inspired asynchronous address event representation (AER) vision sensors. The asynchronous nature of these systems frees computation and communication from the rigid predetermined timing enforced by system clocks in conventional systems. Freedom from rigid timing constraints opens the possibility of using true timing to our advantage in computation. We show not only how timing can be used in object recognition, but also how it can in fact simplify computation. Specifically, we rely on a simple temporal-winner-take-all rather than more computationally intensive synchronous operations typically used in biologically inspired neural networks for object recognition. This approach to visual computation represents a major paradigm shift from conventional clocked systems and can find application in other sensory modalities and computational tasks. We showcase effectiveness of the approach by achieving the highest reported accuracy to date (97.5% ± 3.5%) for a previously published four class card pip recognition task and an accuracy of 84.9% ± 1.9% for a new more difficult 36 class character recognition task. © 2015 IEEE.","computer vision; neural nets; Neuromorphic computing; object recognition"
"Wang X., Yang M., Zhu S., Lin Y.","Regionlets for Generic Object Detection",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","10", 7005538,"2071","2084",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941038243&doi=10.1109%2fTPAMI.2015.2389830&partnerID=40&md5=ba349a3bf75ce6e8dabda066a0ab412c","Generic object detection is confronted by dealing with different degrees of variations, caused by viewpoints or deformations in distinct object classes, with tractable computations. This demands for descriptive and flexible object representations which can be efficiently evaluated in many locations. We propose to model an object class with a cascaded boosting classifier which integrates various types of features from competing local regions, each of which may consist of a group of subregions, named as regionlets. A regionlet is a base feature extraction region defined proportionally to a detection window at an arbitrary resolution (i.e., size and aspect ratio). These regionlets are organized in small groups with stable relative positions to be descriptive to delineate fine-grained spatial layouts inside objects. Their features are aggregated into a one-dimensional feature within one group so as to be flexible to tolerate deformations. The most discriminative regionlets for each object class are selected through a boosting learning procedure. Our regionlet approach achieves very competitive performance on popular multi-class detection benchmark datasets with a single method, without any context. It achieves a detection mean average precision of 41.7 percent on the PASCAL VOC 2007 dataset, and 39.7 percent on the VOC 2010 for 20 object categories. We further develop support pixel integral images to efficiently augment regionlet features with the responses learned by deep convolutional neural networks. Our regionlet based method won second place in the ImageNet Large Scale Visual Object Recognition Challenge (ILSVRC 2013). © 2015 IEEE.","Boosting; Deep Convolutional Neural Network; Object Detection; Object Proposals; Regionlet; Selective Search"
"Li Z., Liu J., Tang J., Lu H.","Robust structured subspace learning for data representation",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","10", 7031960,"2085","2098",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930951838&doi=10.1109%2fTPAMI.2015.2400461&partnerID=40&md5=34185db028023f322743b7df4377c489","To uncover an appropriate latent subspace for data representation, in this paper we propose a novel Robust Structured Subspace Learning (RSSL) algorithm by integrating image understanding and feature learning into a joint learning framework. The learned subspace is adopted as an intermediate space to reduce the semantic gap between the low-level visual features and the high-level semantics. To guarantee the subspace to be compact and discriminative, the intrinsic geometric structure of data, and the local and global structural consistencies over labels are exploited simultaneously in the proposed algorithm. Besides, we adopt the ℓ<inf>2,1</inf> -norm for the formulations of loss function and regularization respectively to make our algorithm robust to the outliers and noise. An efficient algorithm is designed to solve the proposed optimization problem. It is noted that the proposed framework is a general one which can leverage several well-known algorithms as special cases and elucidate their intrinsic relationships. To validate the effectiveness of the proposed method, extensive experiments are conducted on diversity datasets for different image understanding tasks, i.e., image tagging, clustering, and classification, and the more encouraging results are achieved compared with some state-of-the-art approaches. © 1979-2012 IEEE.","Data Representation; Feature Learning; Image Understanding; Latent Subspace; Structure Preserving"
"Luo Y., Jiang M., Wong Y., Zhao Q.","Multi-Camera Saliency",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","10", 7010978,"2057","2070",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941110546&doi=10.1109%2fTPAMI.2015.2392783&partnerID=40&md5=abff733fb83da71aefafe8ecf8e5da98","A significant body of literature on saliency modeling predicts where humans look in a single image or video. Besides the scientific goal of understanding how information is fused from multiple visual sources to identify regions of interest in a holistic manner, there are tremendous engineering applications of multi-camera saliency due to the widespread of cameras. This paper proposes a principled framework to smoothly integrate visual information from multiple views to a global scene map, and to employ a saliency algorithm incorporating high-level features to identify the most important regions by fusing visual information. The proposed method has the following key distinguishing features compared with its counterparts: (1) the proposed saliency detection is global (salient regions from one local view may not be important in a global context), (2) it does not require special ways for camera deployment or overlapping field of view, and (3) the key saliency algorithm is effective in highlighting interesting object regions though not a single detector is used. Experiments on several data sets confirm the effectiveness of the proposed principled framework. © 2015 IEEE.","Global Saliency; High-Level Feature Saliency; Label Consistent K-SVD; Multi-Camera Eye Tracking Dataset; Multi-Camera Saliency; Region Competition"
"Liu Y.-J.","Semi-Continuity of Skeletons in Two-Manifold and Discrete Voronoi Approximation",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","9", 7102776,"1938","1944",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939239450&doi=10.1109%2fTPAMI.2015.2430342&partnerID=40&md5=e0328d84d3179b8b81a082e9c26e717e","The skeleton of a 2D shape is an important geometric structure in pattern analysis and computer vision. In this paper we study the skeleton of a 2D shape in a two-manifold M, based on a geodesic metric. We present a formal definition of the skeleton Sω for a shape V inMand show several properties that make Sω distinct from its Euclidean counterpart in R<inf>2</inf>. We further prove that for a shape sequence R that converge to a shape V inM, the mapping V ! Sω is lower semi-continuous. A direct application of this result is that we can use a set P of sample points to approximate the boundary of a 2D shape V in M, and the Voronoi diagram of ω inside VωMgives a good approximation to the skeleton Sω. Examples of skeleton computation in topography and brain morphometry are illustrated. © 1979-2012 IEEE.","2-manifold; 2D shape sequence; geodesic; Voronoi skeleton"
"Bousmalis K., Zafeiriou S., Morency L.-P., Pantic M., Ghahramani Z.","Variational infinite hidden conditional random fields",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","9", 7001103,"1917","1929",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939238717&doi=10.1109%2fTPAMI.2014.2388228&partnerID=40&md5=0b347b361f67289ed38897a39257b475","Hidden conditional random fields (HCRFs) are discriminative latent variable models which have been shown to successfully learn the hidden structure of a given classification problem. An Infinite hidden conditional random field is a hidden conditional random field with a countably infinite number of hidden states, which rids us not only of the necessity to specify a priori a fixed number of hidden states available but also of the problem of overfitting. Markov chain Monte Carlo (MCMC) sampling algorithms are often employed for inference in such models. However, convergence of such algorithms is rather difficult to verify, and as the complexity of the task at hand increases the computational cost of such algorithms often becomes prohibitive. These limitations can be overcome by variational techniques. In this paper, we present a generalized framework for infinite HCRF models, and a novel variational inference approach on a model based on coupled Dirichlet Process Mixtures, the HCRF-DPM. We show that the variational HCRF-DPM is able to converge to a correct number of represented hidden states, and performs as well as the best parametric HCRFs - chosen via cross-validation - for the difficult tasks of recognizing instances of agreement, disagreement, and pain in audiovisual sequences. © 1979-2012 IEEE.","dirichlet processes; discriminative models; hidden conditional random fields; nonparametric models; variational inference"
"Ouyang W., Zeng X., Wang X.","Single-pedestrian detection aided by two-pedestrian detection",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","9", 6994306,"1875","1889",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939219300&doi=10.1109%2fTPAMI.2014.2377734&partnerID=40&md5=252273990b3c23df68004fc6526fc11e","In this paper, we address the challenging problem of detecting pedestrians who appear in groups. A new approach is proposed for single-pedestrian detection aided by two-pedestrian detection. A mixture model of two-pedestrian detectors is designed to capture the unique visual cues which are formed by nearby pedestrians but cannot be captured by single-pedestrian detectors. A probabilistic framework is proposed to model the relationship between the configurations estimated by single- and two-pedestrian detectors, and to refine the single-pedestrian detection result using two-pedestrian detection. The two-pedestrian detector can integrate with any single-pedestrian detector. Twenty-five state-of-the-art single-pedestrian detection approaches are combined with the two-pedestrian detector on three widely used public datasets: Caltech, TUD-Brussels, and ETH. Experimental results show that our framework improves all these approaches. The average improvement is 9 percent on the Caltech-Test dataset, 11 percent on the TUD-Brussels dataset and 17 percent on the ETH dataset in terms of average miss rate. The lowest average miss rate is reduced from 37 to percent on the Caltech-Test dataset, from 55 to 50 percent on the TUD-Brussels dataset and from 43 to 38 percent on the ETH dataset. © 1979-2012 IEEE.","contextual information; discriminative model; human detection; object detection; Part based model; pedestrian detection"
"Wu Y., Lim J., Yang M.-H.","Object tracking benchmark",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","9", 7001050,"1834","1848",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939235624&doi=10.1109%2fTPAMI.2014.2388226&partnerID=40&md5=a18dddba9d374892cfd29d04c8d49e2b","Object tracking has been one of the most important and active research areas in the field of computer vision. A large number of tracking algorithms have been proposed in recent years with demonstrated success. However, the set of sequences used for evaluation is often not sufficient or is sometimes biased for certain types of algorithms. Many datasets do not have common ground-truth object positions or extents, and this makes comparisons among the reported quantitative results difficult. In addition, the initial conditions or parameters of the evaluated tracking algorithms are not the same, and thus, the quantitative results reported in literature are incomparable or sometimes contradictory. To address these issues, we carry out an extensive evaluation of the state-of-the-art online object-tracking algorithms with various evaluation criteria to understand how these methods perform within the same framework. In this work, we first construct a large dataset with ground-truth object positions and extents for tracking and introduce the sequence attributes for the performance analysis. Second, we integrate most of the publicly available trackers into one code library with uniform input and output formats to facilitate large-scale performance evaluation. Third, we extensively evaluate the performance of 31 algorithms on 100 sequences with different initialization settings. By analyzing the quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field. © 1979-2012 IEEE.","benchmark dataset; Object tracking; performance evaluation"
"Djelouah A., Franco J.-S., Boyer E., Le Clerc F., Perez P.","Sparse Multi-View Consistency for Object Segmentation",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","9", 6996026,"1890","1903",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939226626&doi=10.1109%2fTPAMI.2014.2385704&partnerID=40&md5=0c1bb1a1c4d415b9d15309bdd649bd1e","Multiple view segmentation consists in segmenting objects simultaneously in several views. A key issue in that respect and compared to monocular settings is to ensure propagation of segmentation information between views while minimizing complexity and computational cost. In this work, we first investigate the idea that examining measurements at the projections of a sparse set of 3D points is sufficient to achieve this goal. The proposed algorithm softly assigns each of these 3D samples to the scene background if it projects on the background region in at least one view, or to the foreground if it projects on foreground region in all views. Second, we show how other modalities such as depth may be seamlessly integrated in the model and benefit the segmentation. The paper exposes a detailed set of experiments used to validate the algorithm, showing results comparable with the state of art, with reduced computational complexity. We also discuss the use of different modalities for specific situations, such as dealing with a low number of viewpoints or a scene with color ambiguities between foreground and background. © 1979-2012 IEEE.","Scene analysis; Segmentation"
"Kwon J., Lee K.M.","A Unified Framework for Event Summarization and Rare Event Detection from Multiple Views",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","9", 6995977,"1737","1750",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939250287&doi=10.1109%2fTPAMI.2014.2385695&partnerID=40&md5=04342922a42ae537b9260a7ec3d1f173","A novel approach for event summarization and rare event detection is proposed. Unlike conventional methods that deal with event summarization and rare event detection independently, our method solves them in a single framework by transforming them into a graph editing problem. In our approach, a video is represented by a graph, each node of which indicates an event obtained by segmenting the video spatially and temporally. The edges between nodes describe the relationship between events. Based on the degree of relations, edges have different weights. After learning the graph structure, our method finds subgraphs that represent event summarization and rare events in the video by editing the graph, that is, merging its subgraphs or pruning its edges. The graph is edited to minimize a predefined energy model with the Markov Chain Monte Carlo (MCMC) method. The energy model consists of several parameters that represent the causality, frequency, and significance of events. We design a specific energy model that uses these parameters to satisfy each objective of event summarization and rare event detection. The proposed method is extended to obtain event summarization and rare event detection results across multiple videos captured from multiple views. For this purpose, the proposed method independently learns and edits each graph of individual videos for event summarization or rare event detection. Then, the method matches the extracted multiple graphs to each other, and constructs a single composite graph that represents event summarization or rare events from multiple views. Experimental results show that the proposed approach accurately summarizes multiple videos in a fully unsupervised manner. Moreover, the experiments demonstrate that the approach is advantageous in detecting rare transition of events. © 1979-2012 IEEE.","Event Summarization; Rare Event Detection; Video Structure Editing; Video Structure Learning; Video Structure Matching"
"Ayed I.B., Punithakumar K., Li S.","Distribution Matching with the Bhattacharyya Similarity: A Bound Optimization Framework",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","9", 6987271,"1777","1791",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939241029&doi=10.1109%2fTPAMI.2014.2382104&partnerID=40&md5=48a033b6000033a609228c463abd52e1","We present efficient graph cut algorithms for three problems: (1) finding a region in an image, so that the histogram (or distribution) of an image feature within the region most closely matches a given model; (2) co-segmentation of image pairs and (3) interactive image segmentation with a user-provided bounding box. Each algorithm seeks the optimum of a global cost function based on the Bhattacharyya measure, a convenient alternative to other matching measures such as the Kullback-Leibler divergence. Our functionals are not directly amenable to graph cut optimization as they contain non-linear functions of fractional terms, which make the ensuing optimization problems challenging. We first derive a family of parametric bounds of the Bhattacharyya measure by introducing an auxiliary labeling. Then, we show that these bounds are auxiliary functions of the Bhattacharyya measure, a result which allows us to solve each problem efficiently via graph cuts. We show that the proposed optimization procedures converge within very few graph cut iterations. Comprehensive and various experiments, including quantitative and comparative evaluations over two databases, demonstrate the advantages of the proposed algorithms over related works in regard to optimality, computational load, accuracy and flexibility. © 1979-2012 IEEE.","auxiliary functions; Bhattacharyya measure; bound optimization; Graph cuts"
"Yin X.-C., Pei W.-Y., Zhang J., Hao H.-W.","Multi-Orientation Scene Text Detection with Adaptive Clustering",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","9", 7001081,"1930","1937",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939244993&doi=10.1109%2fTPAMI.2014.2388210&partnerID=40&md5=5ef6792fc8649da496521eca8bbbbb29","Text detection in natural scene images is an important prerequisite for many content-based image analysis tasks, while most current research efforts only focus on horizontal or near horizontal scene text. In this paper, first we present a unified distance metric learning framework for adaptive hierarchical clustering, which can simultaneously learn similarity weights (to adaptively combine different feature similarities) and the clustering threshold (to automatically determine the number of clusters). Then, we propose an effective multi-orientation scene text detection system, which constructs text candidates by grouping characters based on this adaptive clustering. Our text candidates construction method consists of several sequential coarse-To-fine grouping steps: morphology-based grouping via single-link clustering, orientation-based grouping via divisive hierarchical clustering, and projection-based grouping also via divisive clustering. The effectiveness of our proposed system is evaluated on several public scene text databases, e.g., ICDAR Robust Reading Competition data sets (2011 and 2013), MSRA-TD500 and NEOCR. Specifically, on the multi-orientation text data set MSRA-TD500, the f measure of our system is 71 percent, much better than the state-of-The-Art performance. We also construct and release a practical challenging multi-orientation scene text data set (USTB-SV1K), which is available at http://prir.ustb.edu.cn/TexStar/MOMV-Text-detection/. © 1979-2012 IEEE.","adaptive hierarchical clustering; coarse-To-fine grouping; multi-orientation; Scene text detection"
"He K., Zhang X., Ren S., Sun J.","Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","9", 7005506,"1904","1916",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939247735&doi=10.1109%2fTPAMI.2015.2389824&partnerID=40&md5=bb98a04474f7ebe9514a5c0ed24fd0d6","Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224\times 224) input image. This requirement is 'artificial' and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, 'spatial pyramid pooling', to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-The-Art classification results using a single full-image representation and no fine-Tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 \times faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition. © 1979-2012 IEEE.","Convolutional Neural Networks; Image Classification; Object Detection; Spatial Pyramid Pooling"
"Haines O., Calway A.","Recognising Planes in a Single Image",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","9", 6987324,"1849","1861",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939241069&doi=10.1109%2fTPAMI.2014.2382097&partnerID=40&md5=32c25f2d241f53f5938d18ba1d719e6d","We present a novel method to recognise planar structures in a single image and estimate their 3D orientation. This is done by exploiting the relationship between image appearance and 3D structure, using machine learning methods with supervised training data. As such, the method does not require specific features or use geometric cues, such as vanishing points. We employ general feature representations based on spatiograms of gradients and colour, coupled with relevance vector machines for classification and regression. We first show that using hand-labelled training data, we are able to classify pre-segmented regions as being planar or not, and estimate their 3D orientation. We then incorporate the method into a segmentation algorithm to detect multiple planar structures from a previously unseen image. © 1979-2012 IEEE.","learning; Planar structure; recognition; single images"
"Kwon Y., Kim K.I., Tompkin J., Kim J.H., Theobalt C.","Efficient Learning of Image Super-Resolution and Compression Artifact Removal with Semi-Local Gaussian Processes",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","9", 7005530,"1792","1805",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939248816&doi=10.1109%2fTPAMI.2015.2389797&partnerID=40&md5=95d57e322943b5f3ffd258a22c24c81f","Improving the quality of degraded images is a key problem in image processing, but the breadth of the problem leads to domain-specific approaches for tasks such as super-resolution and compression artifact removal. Recent approaches have shown that a general approach is possible by learning application-specific models from examples; however, learning models sophisticated enough to generate high-quality images is computationally expensive, and so specific per-Application or per-dataset models are impractical. To solve this problem, we present an efficient semi-local approximation scheme to large-scale Gaussian processes. This allows efficient learning of task-specific image enhancements from example images without reducing quality. As such, our algorithm can be easily customized to specific applications and datasets, and we show the efficiency and effectiveness of our approach across five domains: single-image super-resolution for scene, human face, and text images, and artifact removal in JPEG- and JPEG 2000-encoded images. © 1979-2012 IEEE.","Gaussian process; image compression; Image enhancement; regression; super-resolution"
"Zhao Q., Zhang L., Cichocki A.","Bayesian CP factorization of incomplete tensors with automatic rank determination",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","9", 7010937,"1751","1763",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939249854&doi=10.1109%2fTPAMI.2015.2392756&partnerID=40&md5=153f2534a760cc6fd57aba6cdbca5a5a","CANDECOMP/PARAFAC (CP) tensor factorization of incomplete data is a powerful technique for tensor completion through explicitly capturing the multilinear latent factors. The existing CP algorithms require the tensor rank to be manually specified, however, the determination of tensor rank remains a challenging problem especially for CP rank. In addition, existing approaches do not take into account uncertainty information of latent factors, as well as missing entries. To address these issues, we formulate CP factorization using a hierarchical probabilistic model and employ a fully Bayesian treatment by incorporating a sparsity-inducing prior over multiple latent factors and the appropriate hyperpriors over all hyperparameters, resulting in automatic rank determination. To learn the model, we develop an efficient deterministic Bayesian inference algorithm, which scales linearly with data size. Our method is characterized as a tuning parameter-free approach, which can effectively infer underlying multilinear factors with a low-rank constraint, while also providing predictive distributions over missing entries. Extensive simulations on synthetic data illustrate the intrinsic capability of our method to recover the ground-truth of CP rank and prevent the overfitting problem, even when a large amount of entries are missing. Moreover, the results from real-world applications, including image inpainting and facial image synthesis, demonstrate that our method outperforms state-of-the-art approaches for both tensor factorization and tensor completion in terms of predictive performance. © 1979-2012 IEEE.","Bayesian inference; image synthesis; tensor completion; Tensor factorization; tensor rank determination"
"Sprechmann P., Bronstein A.M., Sapiro G.","Learning Efficient Sparse and Low Rank Models",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","9", 7010964,"1821","1833",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939228352&doi=10.1109%2fTPAMI.2015.2392779&partnerID=40&md5=583f14f588962dee680d8d2b996e570f","Parsimony, including sparsity and low rank, has been shown to successfully model data in numerous machine learning and signal processing tasks. Traditionally, such modeling approaches rely on an iterative algorithm that minimizes an objective function with parsimony-promoting terms. The inherently sequential structure and data-dependent complexity and latency of iterative optimization constitute a major limitation in many applications requiring real-Time performance or involving large-scale data. Another limitation encountered by these modeling techniques is the difficulty of their inclusion in discriminative learning scenarios. In this work, we propose to move the emphasis from the model to the pursuit algorithm, and develop a process-centric view of parsimonious modeling, in which a learned deterministic fixed-complexity pursuit process is used in lieu of iterative optimization. We show a principled way to construct learnable pursuit process architectures for structured sparse and robust low rank models, derived from the iteration of proximal descent algorithms. These architectures learn to approximate the exact parsimonious representation at a fraction of the complexity of the standard optimization methods. We also show that appropriate training regimes allow to naturally extend parsimonious models to discriminative settings. State-of-The-Art results are demonstrated on several challenging problems in image and audio processing with several orders of magnitude speed-up compared to the exact optimization algorithms. © 1979-2012 IEEE.","big data; deep learning; NMF; parsimonious modeling; proximal methods; real-Time implementations; sparse and low-rank models"
"Wang J., Fan W., Ye J.","Fused Lasso Screening Rules via the Monotonicity of Subdifferentials",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","9", 7001682,"1806","1820",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939240553&doi=10.1109%2fTPAMI.2014.2388203&partnerID=40&md5=c1a827be4827e3401a16c36fbe21f1f3","Fused Lasso is a popular regression technique that encodes the smoothness of the data. It has been applied successfully to many applications with a smooth feature structure. However, the computational cost of the existing solvers for fused Lasso is prohibitive when the feature dimension is extremely large. In this paper, we propose novel screening rules that are able to quickly identity the adjacent features with the same coefficients. As a result, the number of variables to be estimated can be significantly reduced, leading to substantial savings in computational cost and memory usage. To the best of our knowledge, the proposed approach is the first attempt to develop screening methods for the fused Lasso problem with general data matrix. Our major contributions are: 1) we derive a new dual formulation of fused Lasso that comes with several desirable properties; 2) we show that the new dual formulation of fused Lasso is equivalent to that of the standard Lasso by two affine transformations; 3) we propose a novel framework for developing effective and efficient screening rules for fused La sso via the monotonicity of the subdifferentials (FLAMS). Some appealing features of FLAMS are: 1) our methods are safe in the sense that the detected adjacent features are guaranteed to have the same coefficients; 2) the dataset needs to be scanned only once to run the screening, whose computational cost is negligible compared to that of solving the fused Lasso; (3) FLAMS is independent of the solvers and can be integrated with any existing solvers. We have evaluated the proposed FLAMS rules on both synthetic and real datasets. The experiments indicate that FLAMS is very effective in identifying the adjacent features with the same coefficients. The speedup gained by FLAMS can be orders of magnitude. © 1979-2012 IEEE.","'1 regularization; Fused Lasso; Screening"
"Yamada M., Sigal L., Raptis M., Toyoda M., Chang Y., Sugiyama M.","Cross-Domain Matching with Squared-Loss Mutual Information",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","9", 7001064,"1764","1776",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939247303&doi=10.1109%2fTPAMI.2014.2388235&partnerID=40&md5=af5f30b902ea900968d01d50fcf40069","The goal of cross-domain matching (CDM) is to find correspondences between two sets of objects in different domains in an unsupervised way. CDM has various interesting applications, including photo album summarization where photos are automatically aligned into a designed frame expressed in the Cartesian coordinate system, and temporal alignment which aligns sequences such as videos that are potentially expressed using different features. In this paper, we propose an information-Theoretic CDM framework based on squared-loss mutual information (SMI). The proposed approach can directly handle non-linearly related objects/sequences with different dimensions, with the ability that hyper-parameters can be objectively optimized by cross-validation. We apply the proposed method to several real-world problems including image matching, unpaired voice conversion, photo album summarization, cross-feature video and cross-domain video-To-mocap alignment, and Kinect-based action recognition, and experimentally demonstrate that the proposed method is a promising alternative to state-of-The-Art CDM methods. © 1979-2012 IEEE.","Cross-Domain Object Matching; Cross-Domain Temporal Alignment; Squared-Loss Mutual Information"
"Lindner C., Bromiley P.A., Ionita M.C., Cootes T.F.","Robust and Accurate Shape Model Matching Using Random Forest Regression-Voting",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","9", 6987312,"1862","1874",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939248714&doi=10.1109%2fTPAMI.2014.2382106&partnerID=40&md5=8293751171fd294473f309bbd916a618","A widely used approach for locating points on deformable objects in images is to generate feature response images for each point, and then to fit a shape model to these response images. We demonstrate that Random Forest regression-voting can be used to generate high quality response images quickly. Rather than using a generative or a discriminative model to evaluate each pixel, a regressor is used to cast votes for the optimal position of each point. We show that this leads to fast and accurate shape model matching when applied in the Constrained Local Model framework. We evaluate the technique in detail, and compare it with a range of commonly used alternatives across application areas: the annotation of the joints of the hands in radiographs and the detection of feature points in facial images. We show that our approach outperforms alternative techniques, achieving what we believe to be the most accurate results yet published for hand joint annotation and state-of-the-art performance for facial feature point detection. © 1979-2012 IEEE.","Computer vision; Constrained Local Models; Random Forests"
"Barron J.T., Malik J.","Shape, illumination, and reflectance from shading",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","8", 2377712,"1670","1687",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947217753&doi=10.1109%2fTPAMI.2014.2377712&partnerID=40&md5=f35d34b99022dc74a3e7c20c672601fe","A fundamental problem in computer vision is that of inferring the intrinsic, 3D structure of the world from flat, 2D images of that world. Traditional methods for recovering scene properties such as shape, reflectance, or illumination rely on multiple observations of the same scene to overconstrain the problem. Recovering these same properties from a single image seems almost impossible in comparison-there are an infinite number of shapes, paint, and lights that exactly reproduce a single image. However, certain explanations are more likely than others: surfaces tend to be smooth, paint tends to be uniform, and illumination tends to be natural. We therefore pose this problem as one of statistical inference, and define an optimization problem that searches for the most likely explanation of a single image. Our technique can be viewed as a superset of several classic computer vision problems (shape-from-shading, intrinsic images, color constancy, illumination estimation, etc) and outperforms all previous solutions to those constituent problems. © 2014 IEEE.","Color constancy; Computer vision; Intrinsic images; Machine learning; Shape estimation; Shape from shading"
"Fernandez J.A., Boddeti V.N., Rodriguez A., Kumar B.V.K.V.","Zero-aliasing correlation filters for object recognition",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","8", 2375215,"1702","1715",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947799197&doi=10.1109%2fTPAMI.2014.2375215&partnerID=40&md5=eca4d8d79ee4cd9bbb0088828e415ea6","Correlation filters (CFs) are a class of classifiers that are attractive for object localization and tracking applications. Traditionally, CFs have been designed in the frequency domain using the discrete Fourier transform (DFT), where correlation is efficiently implemented. However, existing CF designs do not account for the fact that the multiplication of two DFTs in the frequency domain corresponds to a circular correlation in the time/spatial domain. Because this was previously unaccounted for, prior CF designs are not truly optimal, as their optimization criteria do not accurately quantify their optimization intention. In this paper, we introduce new zero-aliasing constraints that completely eliminate this aliasing problem by ensuring that the optimization criterion for a given CF corresponds to a linear correlation rather than a circular correlation. This means that previous CF designs can be significantly improved by this reformulation. We demonstrate the benefits of this new CF design approach with several important CFs. We present experimental results on diverse data sets and present solutions to the computational challenges associated with computing these CFs. Code for the CFs described in this paper and their respective zero-aliasing versions is available at http://vishnu.boddeti.net/projects/correlation-filters.html © 2014 IEEE. Person.","Correlation filters; Discrete Fourier transform; Object detection; Object localization; Object recognition"
"Liang S., Luo J., Liu W., Wei Y.","Sketch matching on topology product graph",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","8", 2369031,"1723","1729",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947782626&doi=10.1109%2fTPAMI.2014.2369031&partnerID=40&md5=ebfe3c60473806b7cce6cee640d2c4de","Sketch matching is the fundamental problem in sketch based interfaces. After years of study, it remains challenging when there exists large irregularity and variations in the hand drawn sketch shapes. While most existing works exploit topology relations and graph representations for this problem, they are usually limited by the coarse topology exploration and heuristic (thus suboptimal) similarity metrics between graphs. We present a new sketch matching method with two novel contributions. We introduce a comprehensive definition of topology relations, which results in a rich and informative graph representation of sketches. For graph matching, we propose topology product graph that retains the full correspondence for matching two graphs. Based on it, we derive an intuitive sketch similarity metric whose exact solution is easy to compute. In addition, the graph representation and new metric naturally support partial matching, an important practical problem that received less attention in the literature. Extensive experimental results on a real challenging dataset and the superior performance of our method show that it outperforms the state-of-the-art. © 2014 IEEE.","Similarity metrics; Sketch matching; Topology relations"
"Naghibi T., Hoffmann S., Pfister B.","A semidefinite programming based search strategy for feature selection with mutual information measure",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","8", 2372791,"1529","1540",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937811946&doi=10.1109%2fTPAMI.2014.2372791&partnerID=40&md5=53aaa00f3c0fdf347697d27bba70448c","Feature subset selection, as a special case of the general subset selection problem, has been the topic of a considerable number of studies due to the growing importance of data-mining applications. In the feature subset selection problem there are two main issues that need to be addressed: (i) Finding an appropriate measure function than can be fairly fast and robustly computed for high-dimensional data. (ii) A search strategy to optimize the measure over the subset space in a reasonable amount of time. In this article mutual information between features and class labels is considered to be the measure function. Two series expansions for mutual information are proposed, and it is shown that most heuristic criteria suggested in the literature are truncated approximations of these expansions. It is well-known that searching the whole subset space is an NP-hard problem. Here, instead of the conventional sequential search algorithms, we suggest a parallel search strategy based on semidefinite programming (SDP) that can search through the subset space in polynomial time. By exploiting the similarities between the proposed algorithm and an instance of the maximum-cut problem in graph theory, the approximation ratio of this algorithm is derived and is compared with the approximation ratio of the backward elimination method. The experiments show that it can be misleading to judge the quality of a measure solely based on the classification accuracy, without taking the effect of the non-optimum search strategy into account. © 2014 IEEE.","Approximation ratio; Convex objective; Feature selection; Mutual information"
"Ni B., Moulin P., Yan S.","Order preserving sparse coding",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","8", 2362935,"1615","1628",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947758519&doi=10.1109%2fTPAMI.2014.2362935&partnerID=40&md5=d1be162ec90eda47081a3268885f8832","In this paper, we investigate order-preserving sparse coding for classifying structured data whose atomic features possess ordering relationships. Examples include time sequences where individual frame-wise features are temporally ordered, as well as still images (landscape, street view, etc.) where different regions of the image are spatially ordered. Classification of these structured data is often tackled by first decomposing the input data into individual atomic features, then performing sparse coding or other processing for each atomic feature vector independently, and finally aggregating individual responses to classify the input data. However, this heuristic approach ignores the underlying order of the individual atomic features within the input data, and results in suboptimal discriminative capability. In this work, we introduce an order preserving regularizer which aims to preserve the ordering structure of the reconstruction coefficients within the sparse coding framework. An efficient Nesterov-type smooth approximation method is developed for optimization of the new regularization criterion, with theoretically guaranteed error bound. We perform extensive experiments for time series classification on a synthetic dataset, several machine learning benchmarks, and an RGB-D human activity dataset. We also report experiments for scene classification on a benchmark image dataset. The encoded representation is discriminative and robust, and our classifier outperforms state-of-the-art methods on these tasks. © 2014 IEEE.","Order preserving; Scene classification; Sparse coding; Time sequence classification"
"Estrada R., Tomasi C., Schmidler S.C., Farsiu S.","Tree topology estimation",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","8", 2382116,"1688","1701",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947800662&doi=10.1109%2fTPAMI.2014.2382116&partnerID=40&md5=74bfb4546b923f290eb9d0ef5ff5df55","Tree-like structures are fundamental in nature, and it is often useful to reconstruct the topology of a tree-what connects to what-from a two-dimensional image of it. However, the projected branches often cross in the image: the tree projects to a planar graph, and the inverse problem of reconstructing the topology of the tree from that of the graph is ill-posed. We regularize this problem with a generative, parametric tree-growth model. Under this model, reconstruction is possible in linear time if one knows the direction of each edge in the graph-which edge endpoint is closer to the root of the tree-but becomes NP-hard if the directions are not known. For the latter case, we present a heuristic search algorithm to estimate the most likely topology of a rooted, three-dimensional tree from a single two-dimensional image. Experimental results on retinal vessel, plant root, and synthetic tree data sets show that our methodology is both accurate and efficient. © 2014 IEEE.","Computer vision; Graph theory; Image analysis; Stochastic processes; Tree topology"
"Shih K.J., Endres I., Hoiem D.","Learning discriminative collections of part detectors for object recognition",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","8", 2366122,"1571","1584",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947812978&doi=10.1109%2fTPAMI.2014.2366122&partnerID=40&md5=1d50f09d749e527b089a1dda49b6cf1b","We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC2010, we evaluate the part detectors' ability to discriminate and localize annotated keypoints and their effectiveness in detecting object categories. © 2014 IEEE.","Discriminative parts; Object recognition; Part sharing"
"Romero A., Radeva P., Gatta C.","Meta-parameter free unsupervised sparse feature learning",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","8", 2366129,"1716","1722",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947803081&doi=10.1109%2fTPAMI.2014.2366129&partnerID=40&md5=3e751f36f189c0ebbbef4f1d2cc66e83","We propose a meta-parameter free, off-the-shelf, simple and fast unsupervised feature learning algorithm, which exploits a new way of optimizing for sparsity. Experiments on CIFAR-10, STL-10 and UCMerced show that the method achieves the state-of-the-art performance, providing discriminative features that generalize well. © 2014 IEEE.","Pre-training of deep networks; Representation learning; Sparse visual features; Unsupervised feature learning"
"Hu H., Feng J., Zhou J.","Exploiting unsupervised and supervised constraints for subspace clustering",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","8", 2377740,"1542","1557",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947777715&doi=10.1109%2fTPAMI.2014.2377740&partnerID=40&md5=174f5884dcb06994b30f8b87fb8188a5","Data in many image and video analysis tasks can be viewed as points drawn from multiple low-dimensional subspaces with each subspace corresponding to one category or class. One basic task for processing such kind of data is to separate the points according to the underlying subspace, referred to as subspace clustering. Extensive studies have been made on this subject, and nearly all of them use unconstrained subspace models, meaning the points can be drawn from everywhere of a subspace, to represent the data. In this paper, we attempt to do subspace clustering based on a constrained subspace assumption that the data is further restricted in the corresponding subspaces, e.g., belonging to a submanifold or satisfying the spatial regularity constraint. This assumption usually describes the real data better, such as differently moving objects in a video scene and face images of different subjects under varying illumination. A unified integer linear programming optimization framework is used to approach subspace clustering, which can be efficiently solved by a branch-and-bound (BB) method. We also show that various kinds of supervised information, such as subspace number, outlier ratio, pairwise constraints, size prior and etc., can be conveniently incorporated into the proposed framework. Experiments on real data show that the proposed method outperforms the state-of-the-art algorithms significantly in clustering accuracy. The effectiveness of the proposed method in exploiting supervised information is also demonstrated. © 2014 IEEE.","Branch and bound; Constrained clustering; Face clustering; Linear programming; Motion segmentation; Subspace clustering"
"Dubrovina-Karni A., Rosman G., Kimmel R.","Multi-region active contours with a single level set function",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","8", 2385708,"1585","1601",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947812492&doi=10.1109%2fTPAMI.2014.2385708&partnerID=40&md5=c37996e2959f1972797610c5fd773100","Segmenting an image into an arbitrary number of coherent regions is at the core of image understanding. Many formulations of the segmentation problem have been suggested over the past years. These formulations include, among others, axiomatic functionals, which are hard to implement and analyze, and graph-based alternatives, which impose a non-geometric metric on the problem. We propose a novel method for segmenting an image into an arbitrary number of regions using an axiomatic variational approach. The proposed method allows to incorporate various generic region appearance models, while avoiding metrication errors. In the suggested framework, the segmentation is performed by level set evolution. Yet, contrarily to most existing methods, here, multiple regions are represented by a single non-negative level set function. The level set function evolution is efficiently executed through the Voronoi Implicit Interface Method for multi-phase interface evolution. The proposed approach is shown to obtain accurate segmentation results for various natural 2D and 3D images, comparable to state-of-the-art image segmentation algorithms. © 2014 IEEE.","Active contours; Level sets; Multi-region; Segmentation"
"Dollár P., Zitnick C.L.","Fast edge detection using structured forests",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","8", 2377715,"1558","1570",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947781852&doi=10.1109%2fTPAMI.2014.2377715&partnerID=40&md5=e7e47cfb39e1487ecfd829be57b19712","Edge detection is a critical component of many vision systems, including object detectors and image segmentation algorithms. Patches of edges exhibit well-known forms of local structure, such as straight lines or T-junctions. In this paper we take advantage of the structure present in local image patches to learn both an accurate and computationally efficient edge detector. We formulate the problem of predicting local edge masks in a structured learning framework applied to random decision forests. Our novel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain measures may be evaluated. The result is an approach that obtains realtime performance that is orders of magnitude faster than many competing state-of-the-art approaches, while also achieving state-of-the-art edge detection results on the BSDS500 Segmentation dataset and NYU Depth dataset. Finally, we show the potential of our approach as a general purpose edge detector by showing our learned edge models generalize well across datasets. © 2014 IEEE.","Edge detection; Real-time systems; Segmentation; Structured random forests; Visual features"
"Seguin G., Alahari K., Sivic J., Laptev I.","Pose estimation and segmentation of multiple people in stereoscopic movies",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","8", 2369050,"1643","1655",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947792989&doi=10.1109%2fTPAMI.2014.2369050&partnerID=40&md5=3b2d223963f03a81a56d1f20b0b5cb67","We describe a method to obtain a pixel-wise segmentation and pose estimation of multiple people in stereoscopic videos. This task involves challenges such as dealing with unconstrained stereoscopic video, non-stationary cameras, and complex indoor and outdoor dynamic scenes with multiple people. We cast the problem as a discrete labelling task involving multiple person labels, devise a suitable cost function, and optimize it efficiently. The contributions of our work are two-fold: First, we develop a segmentation model incorporating person detections and learnt articulated pose segmentation masks, as well as colour, motion, and stereo disparity cues. The model also explicitly represents depth ordering and occlusion. Second, we introduce a stereoscopic dataset with frames extracted from feature-length movies ""StreetDance 3D"" and ""Pina"". The dataset contains 587 annotated human poses, 1,158 bounding box annotations and 686 pixel-wise segmentations of people. The dataset is composed of indoor and outdoor scenes depicting multiple people with frequent occlusions. We demonstrate results on our new challenging dataset, as well as on the H2view dataset from (Sheasby et al. ACCV 2012). © 2014 IEEE.","3D data; Person detection; Pose estimation; Segmentation; Stereo movies"
"Cohen A.R., Vitányi P.M.B.","Normalized compression distance of multisets with applications",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","8", 2375175,"1602","1614",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947747978&doi=10.1109%2fTPAMI.2014.2375175&partnerID=40&md5=5f60c214ccaa765e682ca672aa6690f6","Pairwise normalized compression distance (NCD) is a parameter-free, feature-free, alignment-free, similarity metric based on compression. We propose an NCD of multisets that is also metric. Previously, attempts to obtain such an NCD failed. For classification purposes it is superior to the pairwise NCD in accuracy and implementation complexity. We cover the entire trajectory from theoretical underpinning to feasible practice. It is applied to biological (stem cell, organelle transport) and OCR classification questions that were earlier treated with the pairwise NCD. With the new method we achieved significantly better results. The theoretic foundation is Kolmogorov complexity. © 2014 IEEE.","Classification; Data mining; Handwritten character recognition; Kolmogorov complexity; Multisets or multiples; Normalized compression distance; Organelle transport; Pattern recognition; Retinal progenitor cells; Similarity; Synthetic data"
"Lisanti G., Masi I., Bagdanov A.D., Del Bimbo A.","Person re-identification by iterative re-weighted sparse ranking",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","8", 2369055,"1629","1642",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942420568&doi=10.1109%2fTPAMI.2014.2369055&partnerID=40&md5=c76ef463d6fc73ea09805ab6191645fa","In this paper we introduce a method for person re-identification based on discriminative, sparse basis expansions of targets in terms of a labeled gallery of known individuals. We propose an iterative extension to sparse discriminative classifiers capable of ranking many candidate targets. The approach makes use of soft- and hard-re-weighting to redistribute energy among the most relevant contributing elements and to ensure that the best candidates are ranked at each iteration. Our approach also leverages a novel visual descriptor which we show to be discriminative while remaining robust to pose and illumination variations. An extensive comparative evaluation is given demonstrating that our approach achieves state-of-the-art performance on single- and multi-shot person re-identification scenarios on the VIPeR, i-LIDS, ETHZ, and CAVIAR4REID datasets. The combination of our descriptor and iterative sparse basis expansion improves state-of-the-art rank-1 performance by six percentage points on VIPeR and by 20 on CAVIAR4REID compared to other methods with a single gallery image per person. With multiple gallery and probe images per person our approach improves by 17 percentage points the state-of-the-art on i-LIDS and by 72 on CAVIAR4REID at rank-1. The approach is also quite efficient, capable of single-shot person re-identification over galleries containing hundreds of individuals at about 30 re-identifications per second. © 2014 IEEE.","Person re-identification; Sparse methods; Video surveillance"
"Zhang L., Shen Y., Li H., Lu J.","3D palmprint identification using block-wise features and collaborative representation",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","8", 2372764,"1730","1736",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947729938&doi=10.1109%2fTPAMI.2014.2372764&partnerID=40&md5=79e7ff4f90a86d36ecd598a8fa6f4e07","Developing 3D palmprint recognition systems has recently begun to draw attention of researchers. Compared with its 2D counterpart, 3D palmprint has several unique merits. However, most of the existing 3D palmprint matching methods are designed for one-to-one verification and they are not efficient to cope with the one-to-many identification case. In this paper, we fill this gap by proposing a collaborative representation (CR) based framework with l1-norm or l2-norm regularizations for 3D palmprint identification. The effects of different regularization terms have been evaluated in experiments. To use the CR-based classification framework, one key issue is how to extract feature vectors. To this end, we propose a block-wise statistics based feature extraction scheme. We divide a 3D palmprint ROI into uniform blocks and extract a histogram of surface types from each block; histograms from all blocks are then concatenated to form a feature vector. Such feature vectors are highly discriminative and are robust to mere misalignment. Experiments demonstrate that the proposed CR-based framework with an l2-norm regularization term can achieve much better recognition accuracy than the other methods. More importantly, its computational complexity is extremely low, making it quite suitable for the large-scale identification application. Source codes are available at http://sse.tongji.edu.cn/linzhang/cr3dpalm/cr3dpalm.htm. © 2014 IEEE.","3D palmprint; Collaborative representation; L1-minimization; Sparse representation; Surface type"
"Martinel N., Das A., Micheloni C., Roy-Chowdhury A.K.","Re-identification in the function space of feature warps",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","8", 2377748,"1656","1669",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945244363&doi=10.1109%2fTPAMI.2014.2377748&partnerID=40&md5=e7579155b4bfe9f7eaf49333e68ff110","Person re-identification in a non-overlapping multicamera scenario is an open challenge in computer vision because of the large changes in appearances caused by variations in viewing angle, lighting, background clutter, and occlusion over multiple cameras. As a result of these variations, features describing the same person get transformed between cameras. To model the transformation of features, the feature space is nonlinearly warped to get the ""warp functions"". The warp functions between two instances of the same target form the set of feasible warp functions while those between instances of different targets form the set of infeasible warp functions. In this work, we build upon the observation that feature transformations between cameras lie in a nonlinear function space of all possible feature transformations. The space consisting of all the feasible and infeasible warp functions is the warp function space (WFS). We propose to learn a discriminating surface separating these two sets of warp functions in the WFS and to re-identify persons by classifying a test warp function as feasible or infeasible. Towards this objective, a Random Forest (RF) classifier is employed which effectively chooses the warp function components according to their importance in separating the feasible and the infeasible warp functions in the WFS. Extensive experiments on five datasets are carried out to show the superior performance of the proposed approach over state-of-the-art person re-identification methods. We show that our approach outperforms all other methods when large illumination variations are considered. At the same time it has been shown that our method reaches the best average performance over multiple combinations of the datasets, thus, showing that our method is not designed only to address a specific challenge posed by a particular dataset. © 2014 IEEE.","Feature transformation; Person re-identification; Warp function space"
"Alahari K., Batra D., Ramalingam S., Paragios N., Zemel R.","Guest Editors' Introduction: Special Section on Higher Order Graphical Models in Computer Vision",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 7116679,"1321","1322",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959856309&doi=10.1109%2fTPAMI.2015.2434651&partnerID=40&md5=b7c221c609a1cd61bb557bd2b09771c4","The papers in this special section address the programs and services supported by graphical models in computer vision. This section explores the main challenges in this framework-modeling novel priors, learning, inference-and presents innovative solutions. The papers cover the aspects of modeling novel priors, inference algorithms and parameter learning methods in the context of higher order graphical models. © 2015 IEEE.",
"Bratières S., Quadrianto N., Ghahramani Z.","GPstruct: Bayesian Structured Prediction Using Gaussian Processes",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 6942234,"1514","1520",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961631731&doi=10.1109%2fTPAMI.2014.2366151&partnerID=40&md5=80d16ac0a2b9c59c858c2ca08ad208ab","We introduce a conceptually novel structured prediction model, GPstruct, which is kernelized, non-parametric and Bayesian, by design. We motivate the model with respect to existing approaches, among others, conditional random fields (CRFs), maximum margin Markov networks (M3 N), and structured support vector machines (SVMstruct), which embody only a subset of its properties. We present an inference procedure based on Markov Chain Monte Carlo. The framework can be instantiated for a wide range of structured objects such as linear chains, trees, grids, and other general graphs. As a proof of concept, the model is benchmarked on several natural language processing tasks and a video gesture segmentation task involving a linear chain structure. We show prediction accuracies for GPstruct which are comparable to or exceeding those of CRFs and SVMstruct. © 2014 IEEE.","Gaussion processes; Markov random fields; natural language processing; Statistical learning; structured prediction"
"Shabat G., Shmueli Y., Bermanis A., Averbuch A.","Accelerating Particle Filter Using Randomized Multiscale and Fast Multipole Type Methods",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 7010941,"1396","1407",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960487191&doi=10.1109%2fTPAMI.2015.2392754&partnerID=40&md5=edac8008e6cb7562ff0bc7a44f58c6d5","Particle filter is a powerful tool for state tracking using non-linear observations. We present a multiscale based method that accelerates the tracking computation by particle filters. Unlike the conventional way, which calculates weights over all particles in each cycle of the algorithm, we sample a small subset from the source particles using matrix decomposition methods. Then, we apply a function extension algorithm that uses a particle subset to recover the density function for all the rest of the particles not included in the chosen subset. The computational effort is substantial especially when multiple objects are tracked concurrently. The proposed algorithm significantly reduces the computational load. By using the Fast Gaussian Transform, the complexity of the particle selection step is reduced to a linear time in n and k, where n is the number of particles and k is the number of particles in the selected subset. We demonstrate our method on both simulated and on real data such as object tracking in video sequences. © 2015 IEEE.","fast multipole method; multiscale methods; nonlinear tracking; Particle filter"
"Takahashi T., Kurita T.","Mixture of Subspaces Image Representation and Compact Coding for Large-Scale Image Retrieval",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 6987339,"1469","1479",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960436199&doi=10.1109%2fTPAMI.2014.2382092&partnerID=40&md5=3ecdec8c2e6ab595d2607977dcf4bfaf","There are two major approaches to content-based image retrieval using local image descriptors. One is descriptor-by-descriptor matching and the other is based on comparison of global image representation that describes the set of local descriptors of each image. In large-scale problems, the latter is preferred due to its smaller memory requirements; however, it tends to be inferior to the former in terms of retrieval accuracy. To achieve both low memory cost and high accuracy, we investigate an asymmetric approach in which the probability distribution of local descriptors is modeled for each individual database image while the local descriptors of a query are used as is. We adopt a mixture model of probabilistic principal component analysis. The model parameters constitute a global image representation to be stored in database. Then the likelihood function is employed to compute a matching score between each database image and a query. We also propose an algorithm to encode our image representation into more compact codes. Experimental results demonstrate that our method can represent each database image in less than several hundred bytes achieving higher retrieval accuracy than the state-of-the-art method using Fisher vectors. © 2014 IEEE.","Image retrieval; image search; likelihood function; principal component analysis"
"Gould S.","Learning Weighted Lower Linear Envelope Potentials in Binary Markov Random Fields",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 6945904,"1336","1346",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961653976&doi=10.1109%2fTPAMI.2014.2366760&partnerID=40&md5=7e948b1e93a42f7cb49d269fe47137c5","Markov random fields containing higher-order terms are becoming increasingly popular due to their ability to capture complicated relationships as soft constraints involving many output random variables. In computer vision an important class of constraints encode a preference for label consistency over large sets of pixels and can be modeled using higher-order terms known as lower linear envelope potentials. In this paper we develop an algorithm for learning the parameters of binary Markov random fields with weighted lower linear envelope potentials. We first show how to perform exact energy minimization on these models in time polynomial in the number of variables and number of linear envelope functions. Then, with tractable inference in hand, we show how the parameters of the lower linear envelope potentials can be estimated from labeled training data within a max-margin learning framework. We explore three variants of the lower linear envelope parameterization and demonstrate results on both synthetic and real-world problems. © 2015 IEEE.","higher-order MRFs; lower linear envelope potentials; max-margin learning"
"Dikmen O., Yang Z., Oja E.","Learning the Information Divergence",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 6942194,"1442","1454",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961572671&doi=10.1109%2fTPAMI.2014.2366144&partnerID=40&md5=a1c8274e296ddac14892fb27cb4b4f42","Information divergence that measures the difference between two nonnegative matrices or tensors has found its use in a variety of machine learning problems. Examples are Nonnegative Matrix/Tensor Factorization, Stochastic Neighbor Embedding, topic models, and Bayesian network optimization. The success of such a learning task depends heavily on a suitable divergence. A large variety of divergences have been suggested and analyzed, but very few results are available for an objective choice of the optimal divergence for a given task. Here we present a framework that facilitates automatic selection of the best divergence among a given family, based on standard maximum likelihood estimation. We first propose an approximated Tweedie distribution for the β-divergence family. Selecting the best β then becomes a machine learning problem solved by maximum likelihood. Next, we reformulate α-divergence in terms of β-divergence, which enables automatic selection of α by maximum likelihood with reuse of the learning principle for β-divergence. Furthermore, we show the connections between γ- and β-divergences as well as Rényi- and α-divergences, such that our automatic selection framework is extended to non-separable divergences. Experiments on both synthetic and real-world data demonstrate that our method can quite accurately select information divergence across different learning problems and various divergence families. © 2014 IEEE.","Information divergence; maximum likelihood; nonnegative matrix factorization; stochastic neighbor embedding; tweedie distribution"
"Fix A., Gruber A., Boros E., Zabih R.","A Hypergraph-Based Reduction for Higher-Order Binary Markov Random Fields",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 6994862,"1387","1395",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960492083&doi=10.1109%2fTPAMI.2014.2382109&partnerID=40&md5=710e5fc4fbe9beefeed3fd16d35b8bfb","Higher-order Markov Random Fields, which can capture important properties of natural images, have become increasingly important in computer vision. While graph cuts work well for first-order MRF's, until recently they have rarely been effective for higher-order MRF's. Ishikawa's graph cut technique [1], [2] shows great promise for many higher-order MRF's. His method transforms an arbitrary higher-order MRF with binary labels into a first-order one with the same minima. If all the terms are submodular the exact solution can be easily found; otherwise, pseudoboolean optimization techniques can produce an optimal labeling for a subset of the variables. We present a new transformation with better performance than [1], [2], both theoretically and experimentally. While [1], [2] transforms each higher-order term independently, we use the underlying hypergraph structure of the MRF to transform a group of terms at once. For n binary variables, each of which appears in terms with k other variables, at worst we produce n non-submodular terms, while [1], [2] produces O(nk). We identify a local completeness property under which our method perform even better, and show that under certain assumptions several important vision problems (including common variants of fusion moves) have this property. We show experimentally that our method produces smaller weight of non-submodular edges, and that this metric is directly related to the effectiveness of QPBO [3]. Running on the same field of experts dataset used in [1], [2] we optimally label significantly more variables (96 versus 80 percent) and converge more rapidly to a lower energy. Preliminary experiments suggest that some other higher-order MRF's used in stereo [4] and segmentation [5] are also locally complete and would thus benefit from our work. © 2014 IEEE.","computer vision; Graph cuts; higher order priors; Markov random fields"
"Ye Q., Doermann D.","Text Detection and Recognition in Imagery: A Survey",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 6945320,"1480","1500",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961595333&doi=10.1109%2fTPAMI.2014.2366765&partnerID=40&md5=5e6ef89a6e1385771dfe584d28a92579","This paper analyzes, compares, and contrasts technical challenges, methods, and the performance of text detection and recognition research in color imagery. It summarizes the fundamental problems and enumerates factors that should be considered when addressing these problems. Existing techniques are categorized as either stepwise or integrated and sub-problems are highlighted including text localization, verification, segmentation and recognition. Special issues associated with the enhancement of degraded text and the processing of video text, multi-oriented, perspectively distorted and multilingual text are also addressed. The categories and sub-categories of text are illustrated, benchmark datasets are enumerated, and the performance of the most representative approaches is compared. This review provides a fundamental comparison and analysis of the remaining problems in the field. © 2015 IEEE.","Survey; Text Detection; Text Localization; Text Recognition"
"Guan Y., Li C.-T., Roli F.","On Reducing the Effect of Covariate Factors in Gait Recognition: A Classifier Ensemble Method",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 6945828,"1521","1528",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951875604&doi=10.1109%2fTPAMI.2014.2366766&partnerID=40&md5=37f04a1574cefcdcb4fae6d76de4741c","Robust human gait recognition is challenging because of the presence of covariate factors such as carrying condition, clothing, walking surface, etc. In this paper, we model the effect of covariates as an unknown partial feature corruption problem. Since the locations of corruptions may differ for different query gaits, relevant features may become irrelevant when walking condition changes. In this case, it is difficult to train one fixed classifier that is robust to a large number of different covariates. To tackle this problem, we propose a classifier ensemble method based on the random subspace nethod (RSM) and majority voting (MV). Its theoretical basis suggests it is insensitive to locations of corrupted features, and thus can generalize well to a large number of covariates. We also extend this method by proposing two strategies, i.e., local enhancing (LE) and hybrid decision-level fusion (HDF) to suppress the ratio of false votes to true votes (before MV). The performance of our approach is competitive against the most challenging covariates like clothing, walking surface, and elapsed time. We evaluate our method on the USF dataset and OU-ISIR-B dataset, and it has much higher performance than other state-of-the-art algorithms. © 2015 IEEE.","biometrics; Classifier ensemble; covariate factors; gait recognition; hybrid decision-level fusion; local enhancing; random subspace method"
"Komodakis N., Xiang B., Paragios N.","A Framework for Efficient Structured Max-Margin Learning of High-Order MRF Models",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 6951487,"1425","1441",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961589477&doi=10.1109%2fTPAMI.2014.2368990&partnerID=40&md5=4bd52d900ff833b280d6235e8a6b63a9","We present a very general algorithm for structured prediction learning that is able to efficiently handle discrete MRFs/CRFs (including both pairwise and higher-order models) so long as they can admit a decomposition into tractable subproblems. At its core, it relies on a dual decomposition principle that has been recently employed in the task of MRF optimization. By properly combining such an approach with a max-margin learning method, the proposed framework manages to reduce the training of a complex high-order MRF to the parallel training of a series of simple slave MRFs that are much easier to handle. This leads to a very efficient and general learning scheme that relies on solid mathematical principles. We thoroughly analyze its theoretical properties, and also show that it can yield learning algorithms of increasing accuracy since it naturally allows a hierarchy of convex relaxations to be used for loss-augmented MAP-MRF inference within a max-margin learning approach. Furthermore, it can be easily adapted to take advantage of the special structure that may be present in a given class of MRFs. We demonstrate the generality and flexibility of our approach by testing it on a variety of scenarios, including training of pairwise and higher-order MRFs, training by using different types of regularizers and/or different types of dissimilarity loss functions, as well as by learning of appropriate models for a variety of vision tasks (including high-order models for compact pose-invariant shape priors, knowledge-based segmentation, image denoising, stereo matching as well as high-order Potts MRFs). © 2015 IEEE.","Analytical models; Computational modeling; Estimation; Learning systems; Prediction algorithms; Predictive models; Training"
"Jung J., Lee J.-Y., Jeong Y., Kweon I.S.","Time-of-Flight Sensor Calibration for a Color and Depth Camera Pair",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 6928464,"1501","1513",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960928854&doi=10.1109%2fTPAMI.2014.2363827&partnerID=40&md5=7c4faaf1d2c61e5feb63561ddebd36bd","We present a calibration method of a time-of-flight (ToF) sensor and a color camera pair to align the 3D measurements with the color image correctly. We have designed a 2.5D pattern board with irregularly placed holes to be accurately detected from low resolution depth images of a ToF camera as well as from high resolution color images. In order to improve the accuracy of the 3D measurements of a ToF camera, we propose to perform ray correction and range bias correction. We reset the transformation of the ToF sensor which transforms the radial distance into the scene depth in Cartesian coordinate through ray correction. Then we capture a planar scene from different depths to correct the distance error that is shown to be dependent not only on the distance but also on the pixel location. The range error profiles along the calibrated distance are classified according to their wiggling shapes and each cluster of profiles with similar shape are separately estimated using a B-spline function. The standard deviation of the remaining random noise is recorded as an uncertainty information of distance measurements. We show the performance of our calibration method quantitatively and qualitatively on various datasets, and validate the impact of our method by demonstrating an RGB-D shape refinement application. © 2015 IEEE.","color-depth camera fusion; Kinect; time-of-flight range error analysis; Time-of-flight sensor calibration"
"Arora C., Banerjee S., Kalra P.K., Maheshwari S.N.","Generalized Flows for Optimal Inference in Higher Order MRF-MAP",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 7001049,"1323","1335",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954101538&doi=10.1109%2fTPAMI.2014.2388218&partnerID=40&md5=4bbfab875f8971bd186dd034b8e81a81","Use of higher order clique potentials in MRF-MAP problems has been limited primarily because of the inefficiencies of the existing algorithmic schemes. We propose a new combinatorial algorithm for computing optimal solutions to 2 label MRF-MAP problems with higher order clique potentials. The algorithm runs in time O(2kn3) in the worst case (k is size of clique and n is the number of pixels). A special gadget is introduced to model flows in a higher order clique and a technique for building a flow graph is specified. Based on the primal dual structure of the optimization problem, the notions of the capacity of an edge and a cut are generalized to define a flow problem. We show that in this flow graph, when the clique potentials are submodular, the max flow is equal to the min cut, which also is the optimal solution to the problem. We show experimentally that our algorithm provides significantly better solutions in practice and is hundreds of times faster than solution schemes like Dual Decomposition [1], TRWS [2] and Reduction [3], [4], [5]. The framework represents a significant advance in handling higher order problems making optimal inference practical for medium sized cliques. © 2014 IEEE.","higher order cliques; Markov random field (MRF); maximum a posteriori (MAP); optimal inference"
"Mathe S., Sminchisescu C.","Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for Visual Recognition",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 6942210,"1408","1424",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961654805&doi=10.1109%2fTPAMI.2014.2366154&partnerID=40&md5=6d8e468271abb154c8ef6dea9e3e5df7","Systems based on bag-of-words models from image features collected at maxima of sparse interest point operators have been used successfully for both computer visual object and action recognition tasks. While the sparse, interest-point based approach to recognition is not inconsistent with visual processing in biological systems that operate in 'saccade and fixate' regimes, the methodology and emphasis in the human and the computer vision communities remains sharply distinct. Here, we make three contributions aiming to bridge this gap. First, we complement existing state-of-the art large scale dynamic computer vision annotated datasets like Hollywood-2 [1] and UCF Sports [2] with human eye movements collected under the ecological constraints of visual action and scene context recognition tasks. To our knowledge these are the first large human eye tracking datasets to be collected and made publicly available for video, vision.imar.ro/eyetracking (497,107 frames, each viewed by 19 subjects), unique in terms of their (a) large scale and computer vision relevance, (b) dynamic, video stimuli, (c) task control, as well as free-viewing. Second, we introduce novel dynamic consistency and alignment measures, which underline the remarkable stability of patterns of visual search among subjects. Third, we leverage the significant amount of collected data in order to pursue studies and build automatic, end-to-end trainable computer vision systems based on human eye movements. Our studies not only shed light on the differences between computer vision spatio-temporal interest point image sampling strategies and the human fixations, as well as their impact for visual recognition performance, but also demonstrate that human fixations can be accurately predicted, and when used in an end-to-end automatic system, leveraging some of the advanced computer vision practice, can lead to state of the art results. © 2014 IEEE.","consistency analysis; human eye-movements; large scale learning; saliency prediction; Visual action recognition"
"Kumar M.P., Turki H., Preston D., Koller D.","Parameter Estimation and Energy Minimization for Region-Based Semantic Segmentation",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 6971190,"1373","1386",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960453929&doi=10.1109%2fTPAMI.2014.2372766&partnerID=40&md5=2046ae7c305631d296699d8ce8da8640","We consider the problem of parameter estimation and energy minimization for a region-based semantic segmentation model. The model divides the pixels of an image into non-overlapping connected regions, each of which is to a semantic class. In the context of energy minimization, the main problem we face is the large number of putative pixel-to-region assignments. We address this problem by designing an accurate linear programming based approach for selecting the best set of regions from a large dictionary. The dictionary is constructed by merging and intersecting segments obtained from multiple bottom-up over-segmentations. The linear program is solved efficiently using dual decomposition. In the context of parameter estimation, the main problem we face is the lack of fully supervised data. We address this issue by developing a principled framework for parameter estimation using diverse data. More precisely, we propose a latent structural support vector machine formulation, where the latent variables model any missing information in the human annotation. Of particular interest to us are three types of annotations: (i) images segmented using generic foreground or background classes; (ii) images with bounding boxes specified for objects; and (iii) images labeled to indicate the presence of a class. Using large, publicly available datasets we show that our methods are able to significantly improve the accuracy of the region-based model. © 2014 IEEE.","energy minimization; LP relaxation; Semantic segmentation; weakly supervised learning"
"Werner T.","Marginal Consistency: Upper-Bounding Partition Functions over Commutative Semirings",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 6928491,"1455","1468",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960915981&doi=10.1109%2fTPAMI.2014.2363833&partnerID=40&md5=79c7d07d9708684a7ec753c0de699fca","Many inference tasks in pattern recognition and artificial intelligence lead to partition functions in which addition and multiplication are abstract binary operations forming a commutative semiring. By generalizing max-sum diffusion (one of convergent message passing algorithms for approximate MAP inference in graphical models), we propose an iterative algorithm to upper bound such partition functions over commutative semirings. The iteration of the algorithm is remarkably simple: change any two factors of the partition function such that their product remains the same and their overlapping marginals become equal. In many commutative semirings, repeating this iteration for different pairs of factors converges to a fixed point when the overlapping marginals of every pair of factors coincide. We call this state marginal consistency. During that, an upper bound on the partition function monotonically decreases. This abstract algorithm unifies several existing algorithms, including max-sum diffusion and basic costraint propagation (or local consistency) algorithms in constraint programming. We further construct a hierarchy of marginal consistencies of increasingly higher levels and show than any such level can be enforced by adding identity factors of higher arity (order). Finally, we discuss instances of the framework for several semirings, including the distributive lattice and the max-sum and sum-product semirings. © 2015 IEEE.","commutative semiring; constraint propagation; graphical model; linear programming relaxation; local consistency; Markov random field; max-sum diffusion; message passing; partition function; soft constraint satisfaction"
"Zhu Y., Nayak N.M., Roy-Chowdhury A.K.","Context-Aware Activity Modeling Using Hierarchical Conditional Random Fields",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 6951455,"1360","1372",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961644257&doi=10.1109%2fTPAMI.2014.2369044&partnerID=40&md5=55016b89627b49e3792bd12aba226c81","In this paper, rather than modeling activities in videos individually, we jointly model and recognize related activities in a scene using both motion and context features. This is motivated from the observations that activities related in space and time rarely occur independently and can serve as the context for each other. We propose a two-layer conditional random field model, that represents the action segments and activities in a hierarchical manner. The model allows the integration of both motion and various context features at different levels and automatically learns the statistics that capture the patterns of the features. With weakly labeled training data, the learning problem is formulated as a max-margin problem and is solved by an iterative algorithm. Rather than generating activity labels for individual activities, our model simultaneously predicts an optimum structural label for the related activities in the scene. We show promising results on the UCLA Office Dataset and VIRAT Ground Dataset that demonstrate the benefit of hierarchical modeling of related activities using both motion and context features. © 2015 IEEE.","Activity localization and recognition; Context-aware activity model; Hierarchical Conditional Random Field"
"Osokin A., Vetrov D.P.","Submodular Relaxation for Inference in Markov Random Fields",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","7", 6951467,"1347","1359",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961675307&doi=10.1109%2fTPAMI.2014.2369046&partnerID=40&md5=1e360cbbbad2ba9143044d6530b37db1","In this paper we address the problem of finding the most probable state of a discrete Markov random field (MRF), also known as the MRF energy minimization problem. The task is known to be NP-hard in general and its practical importance motivates numerous approximate algorithms. We propose a submodular relaxation approach (SMR) based on a Lagrangian relaxation of the initial problem. Unlike the dual decomposition approach of Komodakis et al. [29] SMR does not decompose the graph structure of the initial problem but constructs a submodular energy that is minimized within the Lagrangian relaxation. Our approach is applicable to both pairwise and high-order MRFs and allows to take into account global potentials of certain types. We study theoretical properties of the proposed approach and evaluate it experimentally. © 2015 IEEE.","combinatorial algorithms; energy minimization; graph cuts; Markov random fields; relaxation"
"Sariyanidi E., Gunes H., Cavallaro A.","Automatic analysis of facial affect: A survey of registration, representation, and recognition",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","6", 6940284,"1113","1133",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929192741&doi=10.1109%2fTPAMI.2014.2366127&partnerID=40&md5=a4ebb8427808a6325e33ea44b23e0cd0","Automatic affect analysis has attracted great interest in various contexts including the recognition of action units and basic or non-basic emotions. In spite of major efforts, there are several open questions on what the important cues to interpret facial expressions are and how to encode them. In this paper, we review the progress across a range of affect recognition applications to shed light on these fundamental questions. We analyse the state-of-the-art solutions by decomposing their pipelines into fundamental components, namely face registration, representation, dimensionality reduction and recognition. We discuss the role of these components and highlight the models and new trends that are followed in their design. Moreover, we provide a comprehensive analysis of facial representations by uncovering their advantages and limitations; we elaborate on the type of information they encode and discuss how they deal with the key challenges of illumination variations, registration errors, head-pose variations, occlusions, and identity bias. This survey allows us to identify open issues and to define future directions for designing real-world affect recognition systems. © 2014 IEEE.","Affect sensing and analysis; facial expressions; facial representations; registration; survey"
"Tagare H.D., Rao M.","Why does mutual-information work for image registration? A deterministic explanation",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","6", 6915906,"1286","1296",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929206999&doi=10.1109%2fTPAMI.2014.2361512&partnerID=40&md5=26aa141e4f7e775276cad2b55ce98065","This paper proposes a deterministic explanation for mutual-information-based image registration (MI registration). The explanation is that MI registration works because it aligns certain image partitions. This notion of aligning partitions is new, and is shown to be related to Schur- and quasi-convexity. The partition-alignment theory of this paper goes beyond explaining mutual- information. It suggests other objective functions for registering images. Some of these newer objective functions are not entropy-based. Simulations with noisy images show that the newer objective functions work well for registration, lending support to the theory. The theory proposed in this paper opens a number of directions for further research in image registration. These directions are also discussed. © 2014 IEEE.","convexity; Image Rregistration; medical image registration; mutual information"
"Perret B., Cousty J., Tankyevych O., Talbot H., Passat N.","Directed connected operators: Asymmetric hierarchies for image filtering and segmentation",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","6", 6942199,"1162","1176",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929224145&doi=10.1109%2fTPAMI.2014.2366145&partnerID=40&md5=8314d706b72abe9348549d3007e5756a","Connected operators provide well-established solutions for digital image processing, typically in conjunction with hierarchical schemes. In graph-based frameworks, such operators basically rely on symmetric adjacency relations between pixels. In this article, we introduce a notion of directed connected operators for hierarchical image processing, by also considering non-symmetric adjacency relations. The induced image representation models are no longer partition hierarchies (i.e., trees), but directed acyclic graphs that generalize standard morphological tree structures such as component trees, binary partition trees or hierarchical watersheds. We describe how to efficiently build and handle these richer data structures, and we illustrate the versatility of the proposed framework in image filtering and image segmentation. © 2014 IEEE.","antiextensive filtering; connected operators; hierarchical image representation; Mathematical morphology; segme"
"Carreira J., Caseiro R., Batista J., Sminchisescu C.","Free-form region description with second-order pooling",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","6", 6915745,"1177","1189",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929223025&doi=10.1109%2fTPAMI.2014.2361137&partnerID=40&md5=fa5b7be5cabd8a0d9dcd6c0a28973dd9","Semantic segmentation and object detection are nowadays dominated by methods operating on regions obtained as a result of a bottom-up grouping process (segmentation) but use feature extractors developed for recognition on fixed-form (e.g. rectangular) patches, with full images as a special case. This is most likely suboptimal. In this paper we focus on feature extraction and description over free-form regions and study the relationship with their fixed-form counterparts. Our main contributions are novel pooling techniques that capture the second-order statistics of local descriptors inside such free-form regions. We introduce second-order generalizations of average and max-pooling that together with appropriate non-linearities, derived from the mathematical structure of their embedding space, lead to state-of-the-art recognition performance in semantic segmentation experiments without any type of local feature coding. In contrast, we show that codebook-based local feature coding is more important when feature extraction is constrained to operate over regions that include both foreground and large portions of the background, as typical in image classification settings, whereas for high-accuracy localization setups, second-order pooling over free-form regions produces results superior to those of the winning systems in the contemporary semantic segmentation challenges, with models that are much faster in both training and testing. © 2014 IEEE.","differential geometry; image descriptors; pooling; Recognition; regression; second-order statistics; segmentation"
"Asthana A., Zafeiriou S., Tzimiropoulos G., Cheng S., Pantic M.","From pixels to response maps: Discriminative image filtering for face alignment in the wild",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","6", 6919301,"1312","1320",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929224181&doi=10.1109%2fTPAMI.2014.2362142&partnerID=40&md5=537b7fffb06164da87da9fae3330e89a","We propose a face alignment framework that relies on the texture model generated by the responses of discriminatively trained part-based filters. Unlike standard texture models built from pixel intensities or responses generated by generic filters (e.g. Gabor), our framework has two important advantages. First, by virtue of discriminative training, invariance to external variations (like identity, pose, illumination and expression) is achieved. Second, we show that the responses generated by discriminatively trained filters (or patch-experts) are sparse and can be modeled using a very small number of parameters. As a result, the optimization methods based on the proposed texture model can better cope with unseen variations. We illustrate this point by formulating both part-based and holistic approaches for generic face alignment and show that our framework outperforms the state-of-the-art on multiple ""wild"" databases. The code and dataset annotations are available for research purposes from http://ibug.doc.ic.ac.uk/resources. © 2014 IEEE.","active appearance models; constrained local models; Face alignment; facial landmark detection"
"Zhang Q., Li B.","Relative hidden Markov models for video-based evaluation of motion skills in surgical training",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","6", 6915721,"1206","1218",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929192781&doi=10.1109%2fTPAMI.2014.2361121&partnerID=40&md5=cd3539ef9ce8c054053b96aed5fa680b","A proper temporal model is essential to analysis tasks involving sequential data. In computer-assisted surgical training, which is the focus of this study, obtaining accurate temporal models is a key step towards automated skill-rating. Conventional learning approaches can have only limited success in this domain due to insufficient amount of data with accurate labels. We propose a novel formulation termed Relative Hidden Markov Model and develop algorithms for obtaining a solution under this formulation. The method requires only relative ranking between input pairs, which are readily available from training sessions in the target application, hence alleviating the requirement on data labeling. The proposed algorithm learns a model from the training data so that the attribute under consideration is linked to the likelihood of the input, hence supporting comparing new sequences. For evaluation, synthetic data are first used to assess the performance of the approach, and then we experiment with real videos from a widely-adopted surgical training platform. Experimental results suggest that the proposed approach provides a promising solution to video-based motion skill evaluation. To further illustrate the potential of generalizing the method to other applications of temporal analysis, we also report experiments on using our model on speech-based emotion recognition. © 2014 IEEE.","emotion recognition; Relative hidden markov model; relative learning; surgical skill; temporal model"
"Yang Q., Tang J., Ahuja N.","Efficient and robust specular highlight removal",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","6", 6915741,"1304","1311",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929152652&doi=10.1109%2fTPAMI.2014.2360402&partnerID=40&md5=69f522ba690f19ef24d338b64f9d2ec5","A robust and effective specular highlight removal method is proposed in this paper. It is based on a key observation - the maximum fraction of the diffuse colour component in diffuse local patches in colour images changes smoothly. The specular pixels can thus be treated as noise in this case. This property allows the specular highlights to be removed in an image denoising fashion: an edge-preserving low-pass filter (e.g., the bilateral filter) can be used to smooth the maximum fraction of the colour components of the original image to remove the noise contributed by the specular pixels. Recent developments in fast bilateral filtering techniques enable the proposed method to run over 200 × faster than state-of-the-art techniques on a standard CPU and differentiates it from previous work. © 2014 IEEE.","bilateral filter; highlight; Specular reflection separation"
"Quadrianto N., Ghahramani Z.","A very simple safe-Bayesian random forest",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","6", 6920043,"1297","1303",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929179482&doi=10.1109%2fTPAMI.2014.2362751&partnerID=40&md5=eb5d60f09a356a80c9d283adb3566bdb","Random forests works by averaging several predictions of de-correlated trees. We show a conceptually radical approach to generate a random forest: random sampling of many trees from a prior distribution, and subsequently performing a weighted ensemble of predictive probabilities. Our approach uses priors that allow sampling of decision trees even before looking at the data, and a power likelihood that explores the space spanned by combination of decision trees. While each tree performs Bayesian inference to compute its predictions, our aggregation procedure uses the power likelihood rather than the likelihood and is therefore strictly speaking not Bayesian. Nonetheless, we refer to it as a Bayesian random forest but with a built-in safety. The safeness comes as it has good predictive performance even if the underlying probabilistic model is wrong. We demonstrate empirically that our Safe-Bayesian random forest outperforms MCMC or SMC based Bayesian decision trees in term of speed and accuracy, and achieves competitive performance to entropy or Gini optimised random forest, yet is very simple to construct. © 2014 IEEE.","Bayesian methods; decision trees; random forest"
"Babenko A., Lempitsky V.","The inverted multi-index",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","6", 6915715,"1247","1260",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929191645&doi=10.1109%2fTPAMI.2014.2361319&partnerID=40&md5=fe52df158c884b409cab5c73fff6af5d","A new data structure for efficient similarity search in very large datasets of high-dimensional vectors is introduced. This structure called the inverted multi-index generalizes the inverted index idea by replacing the standard quantization within inverted indices with product quantization. For very similar retrieval complexity and pre-processing time, inverted multi-indices achieve a much denser subdivision of the search space compared to inverted indices, while retaining their memory efficiency. Our experiments with large datasets of SIFT and GIST vectors demonstrate that because of the denser subdivision, inverted multi-indices are able to return much shorter candidate lists with higher recall. Augmented with a suitable reranking procedure, multi-indices were able to significantly improve the speed of approximate nearest neighbor search on the dataset of 1 billion SIFT vectors compared to the best previously published systems, while achieving better recall and incurring only few percent of memory overhead. © 2014 IEEE.","Image retrieval; Index; nearest neighbor search; product quantization"
"Rodriguez-Vaamonde S., Torresani L., Fitzgibbon A.W.","What can pictures tell us about web pages? Improving document search using images",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","6", 6945905,"1274","1285",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929206960&doi=10.1109%2fTPAMI.2014.2366761&partnerID=40&md5=51ec2bc6455bb8c505d23b5de075de05","Traditional Web search engines do not use the images in the HTML pages to find relevant documents for a given query. Instead, they typically operate by computing a measure of agreement between the keywords provided by the user and only the text portion of each page. In this paper we study whether the content of the pictures appearing in a Web page can be used to enrich the semantic description of an HTML document and consequently boost the performance of a keyword-based search engine. We present a Web-scalable system that exploits a pure text-based search engine to find an initial set of candidate documents for a given query. Then, the candidate set is reranked using visual information extracted from the images contained in the pages. The resulting system retains the computational efficiency of traditional text-based search engines with only a small additional storage cost needed to encode the visual information. We test our approach on one of the TREC Million Query Track benchmarks where we show that the exploitation of visual content yields improvement in accuracies for two distinct text-based search engines, including the system with the best reported performance on this benchmark. We further validate our approach by collecting document relevance judgements on our search results using Amazon Mechanical Turk. The results of this experiment confirm the improvement in accuracy produced by our image-based reranker over a pure text-based system. © 2014 IEEE.","document ranking; multimedia search; search engines; Web Pages"
"Li K., Wang J., Wang H., Dai Q.","Structuring lecture videos by automatic projection screen localization and analysis",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","6", 6915738,"1233","1246",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929159927&doi=10.1109%2fTPAMI.2014.2361133&partnerID=40&md5=b2739bdc89255d1e835ae7a81d0f08b3","We present a fully automatic system for extracting the semantic structure of a typical academic presentation video, which captures the whole presentation stage with abundant camera motions such as panning, tilting, and zooming. Our system automatically detects and tracks both the projection screen and the presenter whenever they are visible in the video. By analyzing the image content of the tracked screen region, our system is able to detect slide progressions and extract a high-quality, non-occluded, geometrically-compensated image for each slide, resulting in a list of representative images that reconstruct the main presentation structure. Afterwards, our system recognizes text content and extracts keywords from the slides, which can be used for keyword-based video retrieval and browsing. Experimental results show that our system is able to generate more stable and accurate screen localization results than commonly-used object tracking methods. Our system also extracts more accurate presentation structures than general video summarization methods, for this specific type of video. © 2014 IEEE.","Lecture video; presentation video; projection screen localization; video structuring; video summarization"
"Oh T.-H., Lee J.-Y., Tai Y.-W., Kweon I.S.","Robust high dynamic range imaging by rank minimization",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","6", 6915885,"1219","1232",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929179529&doi=10.1109%2fTPAMI.2014.2361338&partnerID=40&md5=c2af123feb1589a4aa619bca339d66cf","This paper introduces a new high dynamic range (HDR) imaging algorithm which utilizes rank minimization. Assuming a camera responses linearly to scene radiance, the input low dynamic range (LDR) images captured with different exposure time exhibit a linear dependency and form a rank-1 matrix when stacking intensity of each corresponding pixel together. In practice, misalignments caused by camera motion, presences of moving objects, saturations and image noise break the rank-1 structure of the LDR images. To address these problems, we present a rank minimization algorithm which simultaneously aligns LDR images and detects outliers for robust HDR generation. We evaluate the performances of our algorithm systematically using synthetic examples and qualitatively compare our results with results from the state-of-the-art HDR algorithms using challenging real world examples. © 2014 IEEE.","alignment; High dynamic range image; matrix completion; multi-exposure fusion; rank minimization; RPCA"
"Yarlagadda P., Ommer B.","Beyond the sum of parts: Voting with groups of dependent entities",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","6", 6926849,"1134","1147",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929179483&doi=10.1109%2fTPAMI.2014.2363456&partnerID=40&md5=105ccd3185475955287038fa4f2abda6","The high complexity of multi-scale, category-level object detection in cluttered scenes is efficiently handled by Hough voting methods. However, the main shortcoming of the approach is that mutually dependent local observations are independently casting their votes for intrinsically global object properties such as object scale. Object hypotheses are then assumed to be a mere sum of their part votes. Popular representation schemes are, however, based on a dense sampling of semi-local image features, which are consequently mutually dependent. We take advantage of part dependencies and incorporate them into probabilistic Hough voting by deriving an objective function that connects three intimately related problems: i) grouping mutually dependent parts, ii) solving the correspondence problem conjointly for dependent parts, and iii) finding concerted object hypotheses using extended groups rather than based on local observations alone. Early commitments are avoided by not restricting parts to only a single vote for a locally best correspondence and we learn a weighting of parts during training to reflect their differing relevance for an object. Experiments successfully demonstrate the benefit of incorporating part dependencies through grouping into Hough voting. The joint optimization of groupings, correspondences, and votes not only improves the detection accuracy over standard Hough voting and a sliding window baseline, but it also reduces the computational complexity by significantly decreasing the number of candidate hypotheses. © 2014 IEEE.","grouping; hough voting; Object detection; recognition; visual learning"
"Han H., Otto C., Liu X., Jain A.K.","Demographic estimation from face images: Human vs. machine performance",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","6", 6920084,"1148","1161",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929208117&doi=10.1109%2fTPAMI.2014.2362759&partnerID=40&md5=9486d9ea2336bc781fd9c35a7b733bcc","Demographic estimation entails automatic estimation of age, gender and race of a person from his face image, which has many potential applications ranging from forensics to social media. Automatic demographic estimation, particularly age estimation, remains a challenging problem because persons belonging to the same demographic group can be vastly different in their facial appearances due to intrinsic and extrinsic factors. In this paper, we present a generic framework for automatic demographic (age, gender and race) estimation. Given a face image, we first extract demographic informative features via a boosting algorithm, and then employ a hierarchical approach consisting of between-group classification, and within-group regression. Quality assessment is also developed to identify low-quality face images that are difficult to obtain reliable demographic estimates. Experimental results on a diverse set of face image databases, FG-NET (1K images), FERET (3K images), MORPH II (75K images), PCSO (100K images), and a subset of LFW (4K images), show that the proposed approach has superior performance compared to the state of the art. Finally, we use crowdsourcing to study the human perception ability of estimating demographics from face images. A side-by-side comparison of the demographic estimates from crowdsourced data and the proposed algorithm provides a number of insights into this challenging problem. © 2014 IEEE.","crowdsourcing; Demographic estimation; demographic informative feature; hierarchical approach; human vs. machine; quality assessment"
"Kamyshanska H., Memisevic R.","The potential energy of an autoencoder",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","6", 6918504,"1261","1273",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929192782&doi=10.1109%2fTPAMI.2014.2362140&partnerID=40&md5=94dde0de2a728ff8ea0925d849c3aee5","Autoencoders are popular feature learning models, that are conceptually simple, easy to train and allow for efficient inference. Recent work has shown how certain autoencoders can be associated with an energy landscape, akin to negative log-probability in a probabilistic model, which measures how well the autoencoder can represent regions in the input space. The energy landscape has been commonly inferred heuristically, by using a training criterion that relates the autoencoder to a probabilistic model such as a Restricted Boltzmann Machine (RBM). In this paper we show how most common autoencoders are naturally associated with an energy function, independent of the training procedure, and that the energy landscape can be inferred analytically by integrating the reconstruction function of the autoencoder. For autoencoders with sigmoid hidden units, the energy function is identical to the free energy of an RBM, which helps shed light onto the relationship between these two types of model. We also show that the autoencoder energy function allows us to explain common regularization procedures, such as contractive training, from the perspective of dynamical systems. As a practical application of the energy function, a generative classifier based on class-specific autoencoders is presented. © 2014 IEEE.","Autoencoders; generative classification; representation learning; unsupervised learning"
"Hu W., Zhu S.-C.","Learning 3D object templates by quantizing geometry and appearance spaces",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","6",,"1190","1205",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929191604&doi=10.1109%2fTPAMI.2014.2362141&partnerID=40&md5=2bfa47f9fb5dcd1cad1f3f8159a45775","While 3D object-centered shape-based models are appealing in comparison with 2D viewer-centered appearance-based models for their lower model complexities and potentially better view generalizabilities, the learning and inference of 3D models has been much less studied in the recent literature due to two factors: i) the enormous complexities of 3D shapes in geometric space; and ii) the gap between 3D shapes and their appearances in images. This paper aims at tackling the two problems by studying an And-Or Tree (AoT) representation that consists of two parts: i) a geometry-AoT quantizing the geometry space, i.e. the possible compositions of 3D volumetric parts and 2D surfaces within the volumes; and ii) an appearance-AoT quantizing the appearance space, i.e. the appearance variations of those shapes in different views. In this AoT, an And-node decomposes an entity into constituent parts, and an Or-node represents alternative ways of decompositions. Thus it can express a combinatorial number of geometry and appearance configurations through small dictionaries of 3D shape primitives and 2D image primitives. In the quantized space, the problem of learning a 3D object template is transformed to a structure search problem which can be efficiently solved in a dynamic programming algorithm by maximizing the information gain. We focus on learning 3D car templates from the AoT and collect a new car dataset featuring more diverse views. The learned car templates integrate both the shape-based model and the appearance-based model to combine the benefits of both. In experiments, we show three aspects: 1) the AoT is more efficient than the frequently used octree method in space representation; 2) the learned 3D car template matches the state-of-the art performances on car detection and pose estimation in a public multi-view car dataset; and 3) in our new dataset, the learned 3D template solves the joint task of simultaneous object detection, pose/view estimation, and part localization. It can generalize over unseen views and performs better than the version 5 of the DPM model in terms of object detection and semantic part localization. © 2014 IEEE.","3D object models; And-Or Tree; Hierarchical models; object detection; pose estimation; structure learning"
"Rudovic O., Pavlovic V., Pantic M.","Context-sensitive dynamic ordinal regression for intensity estimation of facial action units",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","5", 6894202,"944","958",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927139162&doi=10.1109%2fTPAMI.2014.2356192&partnerID=40&md5=2c6576bce7264b960ff87b77fd44981b","Modeling intensity of facial action units from spontaneously displayed facial expressions is challenging mainly because of high variability in subject-specific facial expressiveness, head-movements, illumination changes, etc. These factors make the target problem highly context-sensitive. However, existing methods usually ignore this context-sensitivity of the target problem. We propose a novel Conditional Ordinal Random Field (CORF) model for context-sensitive modeling of the facial action unit intensity, where the W5+ (who, when , what, where, why and how) definition of the context is used. While the proposed model is general enough to handle all six context questions, in this paper we focus on the context questions: who (the observed subject), how (the changes in facial expressions), and when (the timing of facial expressions and their intensity). The context questions who and how are modeled by means of the newly introduced context-dependent covariate effects, and the context question when is modeled in terms of temporal correlation between the ordinal outputs, i.e., intensity levels of action units. We also introduce a weighted softmax-margin learning of CRFs from data with skewed distribution of the intensity levels, which is commonly encountered in spontaneous facial data. The proposed model is evaluated on intensity estimation of pain and facial action units using two recently published datasets (UNBC Shoulder Pain and DISFA) of spontaneously displayed facial expressions. Our experiments show that the proposed model performs significantly better on the target tasks compared to the state-of-the-art approaches. Furthermore, compared to traditional learning of CRFs, we show that the proposed weighted learning results in more robust parameter estimation from the imbalanced intensity data. © 2014 IEEE.","action unit intensity; conditional random fields; context modeling; facial expression analysis; FACS; ordinal regression; spontaneous facial behavior"
"Arzeno N.M., Vikalo H.","Semi-supervised affinity propagation with soft instance-level constraints",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","5", 6905814,"1041","1052",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926435914&doi=10.1109%2fTPAMI.2014.2359454&partnerID=40&md5=e77b42c90958b2403e93d9adb438e4f3","Soft-constraint semi-supervised affinity propagation (SCSSAP) adds supervision to the affinity propagation (AP) clustering algorithm without strictly enforcing instance-level constraints. Constraint violations lead to an adjustment of the AP similarity matrix at every iteration of the proposed algorithm and to addition of a penalty to the objective function. This formulation is particularly advantageous in the presence of noisy labels or noisy constraints since the penalty parameter of SCSSAP can be tuned to express our confidence in instance-level constraints. When the constraints are noiseless, SCSSAP outperforms unsupervised AP and performs at least as well as the previously proposed semi-supervised AP and constrained expectation maximization. In the presence of label and constraint noise, SCSSAP results in a more accurate clustering than either of the aforementioned established algorithms. Finally, we present an extension of SCSSAP which incorporates metric learning in the optimization objective and can further improve the performance of clustering. © 2014 IEEE.","affinity propagation; Clustering algorithms; graph algorithms; noisy pairwise constraints; semi-supervised learning"
"Uematsu K., Lee Y.","Statistical optimality in multipartite ranking and ordinal regression",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","5", 6910252,"1080","1094",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926483620&doi=10.1109%2fTPAMI.2014.2360397&partnerID=40&md5=59feefd5dfa669c0805ba1403a88a781","Statistical optimality in multipartite ranking is investigated as an extension of bipartite ranking. We consider the optimality of ranking algorithms through minimization of the theoretical risk which combines pairwise ranking errors of ordinal categories with differential ranking costs. The extension shows that for a certain class of convex loss functions including exponential loss, the optimal ranking function can be represented as a ratio of weighted conditional probability of upper categories to lower categories, where the weights are given by the misranking costs. This result also bridges traditional ranking methods such as proportional odds model in statistics with various ranking algorithms in machine learning. Further, the analysis of multipartite ranking with different costs provides a new perspective on non-smooth listwise ranking measures such as the discounted cumulative gain and preference learning. We illustrate our findings with simulation study and real data analysis. © 2014 IEEE.","Bayes optimality; consistency; convex risk; multipartite ranking; ordinal regression"
"Yang Y., Sundaramoorthi G.","Shape tracking with occlusions via coarse-to-fine region-based sobolev descent",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","5", 6910296,"1053","1066",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926435930&doi=10.1109%2fTPAMI.2014.2360380&partnerID=40&md5=73a398858cd418c1916fe29e76e26409","We present a method to track the shape of an object from video. The method uses a joint shape and appearance model of the object, which is propagated to match shape and radiance in subsequent frames, determining object shape. Self-occlusions and dis-occlusions of the object from camera and object motion pose difficulties to joint shape and appearance models in tracking. They are unable to adapt to new shape and appearance information, leading to inaccurate shape detection. In this work, we model self-occlusions and dis-occlusions in a joint shape and appearance tracking framework. Self-occlusions and the warp to propagate the model are coupled, thus we formulate a joint optimization problem. We derive a coarse-to-fine optimization method, advantageous in tracking, that initially perturbs the model by coarse perturbations before transitioning to finer-scale perturbations seamlessly. This coarse-to-fine behavior is automatically induced by gradient descent on a novel infinite-dimensional Riemannian manifold that we introduce. The manifold consists of planar parameterized regions, and the metric that we introduce is a novel Sobolev metric. Experiments on video exhibiting occlusions/dis-occlusions, complex radiance and background show that occlusion/dis-occlusion modeling leads to superior shape accuracy. © 2014 IEEE.","deformable templates; Object segmentation from video; object tracking; occlusions; optical flow; shape metrics"
"Jiang X., Lai J.","Sparse and dense hybrid representation via dictionary decomposition for face recognition",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","5", 6905839,"1067","1079",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926509500&doi=10.1109%2fTPAMI.2014.2359453&partnerID=40&md5=69a3896e49ca06a373367d9bfd7b52dd","Sparse representation provides an effective tool for classification under the conditions that every class has sufficient representative training samples and the training data are uncorrupted. These conditions may not hold true in many practical applications. Face identification is an example where we have a large number of identities but sufficient representative and uncorrupted training images cannot be guaranteed for every identity. A violation of the two conditions leads to a poor performance of the sparse representation-based classification (SRC). This paper addresses this critic issue by analyzing the merits and limitations of SRC. A sparse-and dense-hybrid representation (SDR) framework is proposed in this paper to alleviate the problems of SRC. We further propose a procedure of supervised low-rank (SLR) dictionary decomposition to facilitate the proposed SDR framework. In addition, the problem of the corrupted training data is also alleviated by the proposed SLR dictionary decomposition. The application of the proposed SDR-SLR approach in face recognition verifies its effectiveness and advancement to the field. Extensive experiments on benchmark face databases demonstrate that it consistently outperforms the state-of-the-art sparse representation based approaches and the performance gains are significant in most cases. © 2014 IEEE.","classification; dictionary learning; face recognition; low-rank matrix recovery; Sparse representation"
"Yamaguchi K., Kiapour M.H., Ortiz L.E., Berg T.L.","Retrieving similar styles to parse clothing",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","5", 6888484,"1028","1040",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927126960&doi=10.1109%2fTPAMI.2014.2353624&partnerID=40&md5=f5c13276dc1a991139a1d6dd956e9893","Clothing recognition is a societally and commercially important yet extremely challenging problem due to large variations in clothing appearance, layering, style, and body shape and pose. In this paper, we tackle the clothing parsing problem using a retrieval-based approach. For a query image, we find similar styles from a large database of tagged fashion images and use these examples to recognize clothing items in the query. Our approach combines parsing from: pre-trained global clothing models, local clothing models learned on the fly from retrieved examples, and transferred parse-masks (Paper Doll item transfer) from retrieved examples. We evaluate our approach extensively and show significant improvements over previous state-of-the-art for both localization (clothing parsing given weak supervision in the form of tags) and detection (general clothing parsing). Our experimental results also indicate that the general pose estimation problem can benefit from clothing parsing. © 2014 IEEE.","Clothing parsing; clothing recognition; image parsing; pose estimation; semantic segmentation"
"Jia Z., Gallagher A.C., Saxena A., Chen T.","3D reasoning from blocks to stability",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","5", 6905840,"905","918",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926483618&doi=10.1109%2fTPAMI.2014.2359435&partnerID=40&md5=739edce9d4a1914b84bd7ac640d243eb","Objects occupy physical space and obey physical laws. To truly understand a scene, we must reason about the space that objects in it occupy, and how each objects is supported stably by each other. In other words, we seek to understand which objects would, if moved, cause other objects to fall. This 3D volumetric reasoning is important for many scene understanding tasks, ranging from segmentation of objects to perception of a rich 3D, physically well-founded, interpretations of the scene. In this paper, we propose a new algorithm to parse a single RGB-D image with 3D block units while jointly reasoning about the segments, volumes, supporting relationships, and object stability. Our algorithm is based on the intuition that a good 3D representation of the scene is one that fits the depth data well, and is a stable, self-supporting arrangement of objects (i.e., one that does not topple). We design an energy function for representing the quality of the block representation based on these properties. Our algorithm fits 3D blocks to the depth values corresponding to image segments, and iteratively optimizes the energy function. Our proposed algorithm is the first to consider stability of objects in complex arrangements for reasoning about the underlying structure of the scene. Experimental results show that our stability-reasoning framework improves RGB-D segmentation and scene volumetric representation. © 2014 IEEE.","computer vision; scene understanding; Segmentation"
"Kolmogorov V.","A new look at reweighted message passing",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","5", 6926846,"919","930",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926429477&doi=10.1109%2fTPAMI.2014.2363465&partnerID=40&md5=e3d44a612e5260d3631c7a8a7c5851e7","We propose a new family of message passing techniques for MAP estimation in graphical models which we call Sequential Reweighted Message Passing (SRMP). Special cases include well-known techniques such as Min-Sum Diffusion (MSD) and a faster Sequential Tree-Reweighted Message Passing (TRW-S). Importantly, our derivation is simpler than the original derivation of TRW-S, and does not involve a decomposition into trees. This allows easy generalizations. The new family of algorithms can be viewed as a generalization of TRW-S from pairwise to higher-order graphical models. We test SRMP on several real-world problems with promising results. © 2014 IEEE.","graphical models; MAP estimation; message passing algorithms"
"Xue J.-H., Hall P.","Why does rebalancing class-unbalanced data improve AUC for linear discriminant analysis?",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","5", 6906278,"1109","1112",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926431649&doi=10.1109%2fTPAMI.2014.2359660&partnerID=40&md5=8d266cb4f7662251a3dedf7495793047","Many established classifiers fail to identify the minority class when it is much smaller than the majority class. To tackle this problem, researchers often first rebalance the class sizes in the training dataset, through oversampling the minority class or undersampling the majority class, and then use the rebalanced data to train the classifiers. This leads to interesting empirical patterns. In particular, using the rebalanced training data can often improve the area under the receiver operating characteristic curve (AUC) for the original, unbalanced test data. The AUC is a widely-used quantitative measure of classification performance, but the property that it increases with rebalancing has, as yet, no theoretical explanation. In this note, using Gaussian-based linear discriminant analysis (LDA) as the classifier, we demonstrate that, at least for LDA, there is an intrinsic, positive relationship between the rebalancing of class sizes and the improvement of AUC. We show that the largest improvement of AUC is achieved, asymptotically, when the two classes are fully rebalanced to be of equal sizes. © 2014 IEEE.","AUC; class imbalance; class rebalancing; linear discriminant analysis; oversampling; ROC; undersampling"
"Song H.O., Girshick R., Zickler S., Geyer C., Felzenszwalb P., Darrell T.","Generalized sparselet models for real-time multiclass object recognition",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","5", 6891251,"1001","1012",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927166207&doi=10.1109%2fTPAMI.2014.2353631&partnerID=40&md5=88849368fd53a27e05a0351e5d8153ab","The problem of real-time multiclass object recognition is of great practical importance in object recognition. In this paper, we describe a framework that simultaneously utilizes shared representation, reconstruction sparsity, and parallelism to enable real-time multiclass object detection with deformable part models at 5Hz on a laptop computer with almost no decrease in task performance. Our framework is trained in the standard structured output prediction formulation and is generically applicable for speeding up object recognition systems where the computational bottleneck is in multiclass, multi-convolutional inference. We experimentally demonstrate the efficiency and task performance of our method on PASCAL VOC, subset of ImageNet, Caltech101 and Caltech256 dataset. © 2014 IEEE.","deformable part models; Object detection; real-time vision; sparse coding"
"Wu Z., Li Y., Radke R.J.","Viewpoint invariant human re-identification in camera networks using pose priors and subject-discriminative features",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","5", 6910272,"1095","1108",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926442890&doi=10.1109%2fTPAMI.2014.2360373&partnerID=40&md5=d3c9e82026d262f1c5d65ab738a55dd5","Human re-identification across cameras with non-overlapping fields of view is one of the most important and difficult problems in video surveillance and analysis. However, current algorithms are likely to fail in real-world scenarios for several reasons. For example, surveillance cameras are typically mounted high above the ground plane, causing serious perspective changes. Also, most algorithms approach matching across images using the same descriptors, regardless of camera viewpoint or human pose. Here, we introduce a re-identification algorithm that addresses both problems. We build a model for human appearance as a function of pose, using training data gathered from a calibrated camera. We then apply this 'pose prior' in online re-identification to make matching and identification more robust to viewpoint. We further integrate person-specific features learned over the course of tracking to improve the algorithm's performance. We evaluate the performance of the proposed algorithm and compare it to several state-of-the-art algorithms, demonstrating superior performance on standard benchmarking datasets as well as a challenging new airport surveillance scenario. © 2014 IEEE.","Camera Networks; Human Re-Identification; Viewpoint invariance"
"Wu T., Zhu S.-C.","Learning near-optimal cost-sensitive decision policy for object detection",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","5", 6906257,"1013","1027",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926435911&doi=10.1109%2fTPAMI.2014.2359653&partnerID=40&md5=7d2f6af220d1e8c7fea3cafe3e326db8","Many popular object detectors, such as AdaBoost, SVM and deformable part-based models (DPM), compute additive scoring functions at a large number of windows in an image pyramid, thus computational efficiency is an important consideration in real time applications besides accuracy. In this paper, a decision policy refers to a sequence of two-sided thresholds to execute early reject and early accept based on the cumulative scores at each step. We formulate an empirical risk function as the weighted sum of the cost of computation and the loss of false alarm and missing detection. Then a policy is said to be cost-sensitive and optimal if it minimizes the risk function. While the risk function is complex due to high-order correlations among the two-sided thresholds, we find that its upper bound can be optimized by dynamic programming efficiently. We show that the upper bound is very tight empirically and thus the resulting policy is said to be near-optimal. In experiments, we show that the decision policy outperforms state-of-the-art cascade methods significantly, in several popular detection tasks and benchmarks, in terms of computational efficiency with similar accuracy of detection. © 2014 IEEE.","cost-sensitive computing; Decision policy; dynamic programming; object detection; risk minimization"
"Chen N., Zhu J., Xia F., Zhang B.","Discriminative relational topic models",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","5", 6914609,"973","986",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926487953&doi=10.1109%2fTPAMI.2014.2361129&partnerID=40&md5=c157b3d60f969c21e3329a60bf59bd59","Relational topic models (RTMs) provide a probabilistic generative process to describe both the link structure and document contents for document networks, and they have shown promise on predicting network structures and discovering latent topic representations. However, existing RTMs have limitations in both the restricted model expressiveness and incapability of dealing with imbalanced network data. To expand the scope and improve the inference accuracy of RTMs, this paper presents three extensions: 1) unlike the common link likelihood with a diagonal weight matrix that allows the-same-topic interactions only, we generalize it to use a full weight matrix that captures all pairwise topic interactions and is applicable to asymmetric networks; 2) instead of doing standard Bayesian inference, we perform regularized Bayesian inference (RegBayes) with a regularization parameter to deal with the imbalanced link structure issue in real networks and improve the discriminative ability of learned latent representations; and 3) instead of doing variational approximation with strict mean-field assumptions, we present collapsed Gibbs sampling algorithms for the generalized relational topic models by exploring data augmentation without making restricting assumptions. Under the generic RegBayes framework, we carefully investigate two popular discriminative loss functions, namely, the logistic log-loss and the max-margin hinge loss. Experimental results on several real network datasets demonstrate the significance of these extensions on improving prediction performance. © 2014 IEEE.","data augmentation; regularized Bayesian inference; relational topic models; statistical network analysis"
"Lin L., Wang X., Yang W., Lai J.-H.","Discriminatively trained and-Or graph models for object shape detection",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","5", 6908013,"959","972",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926429586&doi=10.1109%2fTPAMI.2014.2359888&partnerID=40&md5=97eba9e083049c28af140672d19af694","In this paper, we investigate a novel reconfigurable part-based model, namely And-Or graph model, to recognize object shapes in images. Our proposed model consists of four layers: leaf-nodes at the bottom are local classifiers for detecting contour fragments; or-nodes above the leaf-nodes function as the switches to activate their child leaf-nodes, making the model reconfigurable during inference; and-nodes in a higher layer capture holistic shape deformations; one root-node on the top, which is also an or-node, activates one of its child and-nodes to deal with large global variations (e.g. different poses and views). We propose a novel structural optimization algorithm to discriminatively train the And-Or model from weakly annotated data. This algorithm iteratively determines the model structures (e.g. the nodes and their layouts) along with the parameter learning. On several challenging datasets, our model demonstrates the effectiveness to perform robust shape-based object detection against background clutter and outperforms the other state-of-the-art approaches. We also release a new shape database with annotations, which includes more than 1500 challenging shape instances, for recognition and detection. © 2014 IEEE.","And-Or Graph; grammar model; Object detection; structural optimization"
"Perrier R., Arnaud E., Sturm P., Ortner M.","Estimation of an observation satellite's attitude using multimodal pushbroom cameras",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","5", 6911994,"987","1000",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926435932&doi=10.1109%2fTPAMI.2014.2360394&partnerID=40&md5=6f604f44cb026a58595fb4469b1e31e0","Pushbroom cameras are widely used for earth observation applications. This sensor acquires 1D images over time and uses the straight motion of the satellite to sweep out a region of space and build a 2D image. The stability of the satellite is critical during the pushbroom acquisition process. Therefore its attitude is assumed to be constant over time. However, the recent manufacture of smaller and lighter satellites to reduce launching cost has weakened this assumption. Small oscillations of the satellite's attitude can result in noticeable warps in images, and geolocation information is lost as the satellite does not capture what it ought to. Current solutions use inertial sensors to control the attitude and correct the images, but they are costly and of limited precision. As the warped images do contain information about attitude variations, we suggest using image registration to estimate them. We exploit the geometry of the focal plane and the stationary nature of the disturbances to recover undistorted images. We embed the estimation in a Bayesian framework where image registration, a prior on attitude variations and a radiometric correction model are fused to retrieve the motion of the satellite. We illustrate the performance of our algorithm on four satellite datasets. © 2014 IEEE.","hyperparameter learning; Maximum A Posteriori estimator; multimodal image registration; pushbroom cameras; satellite attitude"
"Kim J., Grauman K.","Boundary preserving dense local regions",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","5", 6912979,"931","943",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926444039&doi=10.1109%2fTPAMI.2014.2360689&partnerID=40&md5=f6d53301c997d4f28308b3d83eaec987","We propose a dense local region detector to extract features suitable for image matching and object recognition tasks. Whereas traditional local interest operators rely on repeatable structures that often cross object boundaries (e.g., corners, scale-space blobs), our sampling strategy is driven by segmentation, and thus preserves object boundaries and shape. At the same time, whereas existing region-based representations are sensitive to segmentation parameters and object deformations, our novel approach to robustly sample dense sites and determine their connectivity offers better repeatability. In extensive experiments, we find that the proposed region detector provides significantly better repeatability and localization accuracy for object matching compared to an array of existing feature detectors. In addition, we show our regions lead to excellent results on two benchmark tasks that require good feature matching: weakly supervised foreground discovery and nearest neighbor-based object recognition. © 2014 IEEE.","Distance transform; Feature matching; Local feature; Object recognition; Segmentation; Shapes"
"Ma Z., Teschendorff A.E., Leijon A., Qiao Y., Zhang H., Guo J.","Variational bayesian matrix factorization for bounded support data",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","4",,"876","889",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924746875&doi=10.1109%2fTPAMI.2014.2353639&partnerID=40&md5=7acfe68c054bd7a0e9ed4c77cda09249","A novel Bayesian matrix factorization method for bounded support data is presented. Each entry in the observation matrix is assumed to be beta distributed. As the beta distribution has two parameters, two parameter matrices can be obtained, which matrices contain only nonnegative values. In order to provide low-rank matrix factorization, the nonnegative matrix factorization (NMF) technique is applied. Furthermore, each entry in the factorized matrices, i.e., the basis and excitation matrices, is assigned with gamma prior. Therefore, we name this method as beta-gamma NMF (BG-NMF). Due to the integral expression of the gamma function, estimation of the posterior distribution in the BG-NMF model can not be presented by an analytically tractable solution. With the variational inference framework and the relative convexity property of the log-inverse-beta function, we propose a new lower-bound to approximate the objective function. With this new lower-bound, we derive an analytically tractable solution to approximately calculate the posterior distributions. Each of the approximated posterior distributions is also gamma distributed, which retains the conjugacy of the Bayesian estimation. In addition, a sparse BG-NMF can be obtained by including a sparseness constraint to the gamma prior. Evaluations with synthetic data and real life data demonstrate the good performance of the proposed method. © 2015 IEEE.","Bayesian estimation; bioinformatics; bounded support data; collaborative filtering; extended factorized approximation; Nonnegative matrix factorization; relative convexity; variational inference"
"Liu M., Hartley R., Salzmann M.","Mirror surface reconstruction from a single image",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","4", 6888493,"760","773",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924619623&doi=10.1109%2fTPAMI.2014.2353622&partnerID=40&md5=694d79806c270963081ea6256da5c654","This paper tackles the problem of reconstructing the shape of a smooth mirror surface from a single image. In particular, we consider the case where the camera is observing the reflection of a static reference target in the unknown mirror. We first study the reconstruction problem given dense correspondences between 3D points on the reference target and image locations. In such conditions, our differential geometry analysis provides a theoretical proof that the shape of the mirror surface can be recovered if the pose of the reference target is known. We then relax our assumptions by considering the case where only sparse correspondences are available. In this scenario, we formulate reconstruction as an optimization problem, which can be solved using a nonlinear least-squares method. We demonstrate the effectiveness of our method on both synthetic and real images. We then provide a theoretical analysis of the potential degenerate cases with and without prior knowledge of the pose of the reference target. Finally we show that our theory can be similarly applied to the reconstruction of the surface of transparent object. © 2015 IEEE.","partial differential equation; reconstruction; single image; Smooth mirror surface; transparent surface reconstruction"
"Zhu J.-Y., Wu J., Xu Y., Chang E., Tu Z.","Unsupervised object class discovery via saliency-guided multiple class learning",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","4", 6888473,"862","875",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924750243&doi=10.1109%2fTPAMI.2014.2353617&partnerID=40&md5=eb2ad7345a4db1f504b781b68845bb5b","In this paper, we tackle the problem of common object (multiple classes) discovery from a set of input images, where we assume the presence of one object class in each image. This problem is, loosely speaking, unsupervised since we do not know a priori about the object type, location, and scale in each image. We observe that the general task of object class discovery in a fully unsupervised manner is intrinsically ambiguous; here we adopt saliency detection to propose candidate image windows/patches to turn an unsupervised learning problem into a weakly-supervised learning problem. In the paper, we propose an algorithm for simultaneously localizing objects and discovering object classes via bottom-up (saliency-guided) multiple class learning (bMCL). Our contributions are three-fold: (1) we adopt saliency detection to convert unsupervised learning into multiple instance learning, formulated as bottom-up multiple class learning (bMCL); (2) we propose an integrated framework that simultaneously performs object localization, object class discovery, and object detector training; (3) we demonstrate that our framework yields significant improvements over existing methods for multi-class object discovery and possess evident advantages over competing methods in computer vision. In addition, although saliency detection has recently attracted much attention, its practical usage for high-level vision tasks has yet to be justified. Our method validates the usefulness of saliency detection to output 'noisy input' for a top-down method to extract common patterns. © 2015 IEEE.","multiple instance learning; object detection; saliency; Unsupervised object discovery; weakly supervised learning"
"Hu W., Li W., Zhang X., Maybank S.","Single and Multiple Object Tracking Using a Multi-Feature Joint Sparse Representation",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","4", 6891248,"816","833",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924666775&doi=10.1109%2fTPAMI.2014.2353628&partnerID=40&md5=706c515f11ede048ef6295c883488490","In this paper, we propose a tracking algorithm based on a multi-feature joint sparse representation. The templates for the sparse representation can include pixel values, textures, and edges. In the multi-feature joint optimization, noise or occlusion is dealt with using a set of trivial templates. A sparse weight constraint is introduced to dynamically select the relevant templates from the full set of templates. A variance ratio measure is adopted to adaptively adjust the weights of different features. The multi-feature template set is updated adaptively. We further propose an algorithm for tracking multi-objects with occlusion handling based on the multi-feature joint sparse reconstruction. The observation model based on sparse reconstruction automatically focuses on the visible parts of an occluded object by using the information in the trivial templates. The multi-object tracking is simplified into a joint Bayesian inference. The experimental results show the superiority of our algorithm over several state-of-the-art tracking algorithms. © 1979-2012 IEEE.","multi-feature joint sparse representation; tracking multi-objects under occlusions; Visual object tracking"
"Zhang S., Yang M., Cour T., Yu K., Metaxas D.N.","Query Specific Rank Fusion for Image Retrieval",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","4",,"803","815",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924598451&doi=10.1109%2fTPAMI.2014.2346201&partnerID=40&md5=747537948d147e26d50dd539784e7d0f","Recently two lines of image retrieval algorithms demonstrate excellent scalability: 1) local features indexed by a vocabulary tree, and 2) holistic features indexed by compact hashing codes. Although both of them are able to search visually similar imageseffectively, their retrieval precision may vary dramatically among queries. Therefore, combining these two types of methods is expected to further enhance the retrieval precision. However, the feature characteristics and the algorithmic procedures of these methods are dramatically different, which is very challenging for the feature-level fusion. This motivates us to investigate how to fuse the ordered retrieval sets, i.e., the ranks of images, given by multiple retrieval methods, to boost the retrieval precision without sacrificing theirscalability. In this paper, we model retrieval ranks as graphs of candidate images and propose a graph-based query specific fusion approach, where multiple graphs are merged and reranked by conducting a link analysis on a fused graph. The retrieval quality of an individual method is measured on-the-fly by assessing the consistency of the top candidates' nearest neighborhoods. Hence, it iscapable of adaptively integrating the strengths of the retrieval methods using local or holistic features for different query images. This proposed method does not need any supervision, has few parameters, and is easy to implement. Extensive and thorough experiments have been conducted on four public datasets, i.e., the UKbench, Corel-5K, Holidays and the large-scale San Francisco Landmarks datasets. Our proposed method has achieved very competitive performance, including state-of-the-art results on several data sets, e.g., the N-S score 3.83 for UKbench. © 1979-2012 IEEE.","graph-based fusion; hashing; Large-scale image retrieval; query specific fusion; vocabulary tree"
"Zarrabeitia L.A., Qureshi F.Z., Aruliah D.A.","Stereo reconstruction of droplet flight trajectories",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","4",,"847","861",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924565533&doi=10.1109%2fTPAMI.2014.2353638&partnerID=40&md5=7de3bdcb5fa7a98ecd3248e084024488","We developed a new method for extracting 3D flight trajectories of droplets using high-speed stereo capture. We noticed that traditional multi-camera tracking techniques fare poorly on our problem, in part due to the fact that all droplets have very similar shapes, sizes and appearances. Our method uses local motion models to track individual droplets in each frame. 2D tracks are used to learn a global, non-linear motion model, which in turn can be used to estimate the 3D locations of individual droplets even when these are not visible in any camera. We have evaluated the proposed method on both synthetic and real data and our method is able to reconstruct 3D flight trajectories of hundreds of droplets. The proposed technique solves for both the 3D trajectory of a droplet and its motion model concomitantly, and we have found it to be superior to 3D reconstruction via triangulation. Furthermore, the learned global motion model allows us to relax the simultaneity assumptions of stereo camera systems. Our results suggest that, even when full stereo information is available, our unsynchronized reconstruction using the global motion model can significantly improve the 3D estimation accuracy. © 2015 IEEE.","multi-target tracking; multi-view geometry; nonlinear motion; parameter estimation; Stereo reconstruction"
"Flusser J., Suk T., Boldyš J., Zitová B.","Projection Operators and Moment Invariants to Image Blurring",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","4", 6891258,"786","802",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924690353&doi=10.1109%2fTPAMI.2014.2353644&partnerID=40&md5=ad9a9dc5817e1599d0116428ca98a4c6","In this paper we introduce a new theory of blur invariants. Blur invariants are image features which preserve their values if the image is convolved by a point-spread function (PSF) of a certain class. We present the invariants to convolution with an arbitrary N-fold symmetric PSF, both in Fourier and image domain. We introduce a notion of a primordial image as a canonical form of all blur-equivalent images. It is defined in spectral domain by means of projection operators. We prove that the moments of the primordial image are invariant to blur and we derive recursive formulae for their direct computation without actually constructing the primordial image. We further prove they form a complete set of invariants and show how to extent their invariance also to translation, rotation and scaling. We illustrate by simulated and real-data experiments their invariance and recognition power. Potential applications of this method are wherever one wants to recognize objects on blurred images. © 1979-2012 IEEE.","blur invariants; Blurred image; image moments; moment invariants; N-fold rotation symmetry; object recognition; projection operators"
"Hayat M., Bennamoun M., An S.","Deep reconstruction models for image set classification",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","4", 6888522,"713","727",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924588005&doi=10.1109%2fTPAMI.2014.2353635&partnerID=40&md5=85d4988730b59d88e125416dc26cb8a5","Image set classification finds its applications in a number of real-life scenarios such as classification from surveillance videos, multi-view camera networks and personal albums. Compared with single image based classification, it offers more promises and has therefore attracted significant research attention in recent years. Unlike many existing methods which assume images of a set to lie on a certain geometric surface, this paper introduces a deep learning framework which makes no such prior assumptions and can automatically discover the underlying geometric structure. Specifically, a Template Deep Reconstruction Model (TDRM) is defined whose parameters are initialized by performing unsupervised pre-training in a layer-wise fashion using Gaussian Restricted Boltzmann Machines (GRBMs). The initialized TDRM is then separately trained for images of each class and class-specific DRMs are learnt. Based on the minimum reconstruction errors from the learnt class-specific models, three different voting strategies are devised for classification. Extensive experiments are performed to demonstrate the efficacy of the proposed framework for the tasks of face and object recognition from image sets. Experimental results show that the proposed method consistently outperforms the existing state of the art methods. © 2015 IEEE.","auto-encoders; deep learning; Image set classification; object recognition; video based face recognition"
"Aftab K., Hartley R., Trumpf J.","Generalized weiszfeld algorithms for Lq optimization",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","4",,"728","745",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924546298&doi=10.1109%2fTPAMI.2014.2353625&partnerID=40&md5=d86b338ed69b1796920ecd209fb3aa40","In many computer vision applications, a desired model of some type is computed by minimizing a cost function based on several measurements. Typically, one may compute the model that minimizes the L2 cost, that is the sum of squares of measurement errors with respect to the model. However, the Lq solution which minimizes the sum of the qth power of errors usually gives more robust results in the presence of outliers for some values of q , for example, q = 1. The Weiszfeld algorithm is a classic algorithm for finding the geometric L1 mean of a set of points in Euclidean space. It is provably optimal and requires neither differentiation, nor line search. The Weiszfeld algorithm has also been generalized to find the L1 mean of a set of points on a Riemannian manifold of non-negative curvature. This paper shows that the Weiszfeld approach may be extended to a wide variety of problems to find an Lq mean for 1 ≤ q &lt; 2 , while maintaining simplicity and provable convergence. We apply this problem to both single-rotation averaging (under which the algorithm provably finds the global Lq optimum) and multiple rotation averaging (for which no such proof exists). Experimental results of Lq optimization for rotations show the improved reliability and robustness compared to L2 optimization. © 2015 IEEE.","Lq mean; rotation averaging; Weiszfeld algorithm"
"Bazzani L., Zanotto M., Cristani M., Murino V.","Joint Individual-Group Modeling for Tracking",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","4", 6891328,"746","759",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924678376&doi=10.1109%2fTPAMI.2014.2353641&partnerID=40&md5=baf2b09e18827c8386595835cbab1dfd","We present a novel probabilistic framework that jointly models individuals and groups for tracking. Managing groups is challenging, primarily because of their nonlinear dynamics and complex layout which lead to repeated splitting and merging events. The proposed approach assumes a tight relation of mutual support between the modeling of individuals and groups, promoting the idea that groups are better modeled if individuals are considered and vice versa. This concept is translated in a mathematical model using a decentralized particle filtering framework which deals with a joint individual-group state space. The model factorizes the joint space into two dependent subspaces, where individuals and groups share the knowledge of the joint individual-group distribution. The assignment of people to the different groups (and thus group initialization, split and merge) is implemented by two alternative strategies: using classifiers trained beforehand on statistics of group configurations, and through online learning of a Dirichlet process mixture model, assuming that no training data is available before tracking. These strategies lead to two different methods that can be used on top of any person detector (simulated using the ground truth in our experiments). We provide convincing results on two recent challenging tracking benchmarks. © 1979-2012 IEEE.","decentralized particle filtering; Dirichlet process mixture model; Group modeling; joint individual-group tracking"
"Mumtaz A., Coviello E., Lanckriet G.R.G., Chan A.B.","A scalable and accurate descriptor for dynamic textures using bag of system trees",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","4", 6905854,"697","712",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924709787&doi=10.1109%2fTPAMI.2014.2359432&partnerID=40&md5=57501ce1f5408fbeb82cf72380c8f4ea","The bag-of-systems (BoS) representation is a descriptor of motion in a video, where dynamic texture (DT) codewords represent the typical motion patterns in spatio-temporal patches extracted from the video. The efficacy of the BoS descriptor depends on the richness of the codebook, which depends on the number of codewords in the codebook. However, for even modest sized codebooks, mapping videos onto the codebook results in a heavy computational load. In this paper we propose the BoS Tree, which constructs a bottom-up hierarchy of codewords that enables efficient mapping of videos to the BoS codebook. By leveraging the tree structure to efficiently index the codewords, the BoS Tree allows for fast look-ups in the codebook and enables the practical use of larger, richer codebooks. We demonstrate the effectiveness of BoS Trees on classification of four video datasets, as well as on annotation of a video dataset and a music dataset. Finally, we show that, although the fast look-ups of BoS Tree result in different descriptors than BoS for the same video, the overall distance (and kernel) matrices are highly correlated resulting in similar classification performance. © 2014 IEEE.","bag of systems; dynamic texture recognition; Dynamic textures; efficient indexing; large codebooks; music annotation; video annotation"
"Tschiatschek S., Pernkopf F.","On Bayesian Network Classifiers with Reduced Precision Parameters",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","4", 6888528,"774","785",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924678377&doi=10.1109%2fTPAMI.2014.2353620&partnerID=40&md5=54932110fd753f34523ff8057792ce85","Bayesian network classifier (BNCs) are typically implemented on nowadays desktop computers. However, many real world applications require classifier implementation on embedded or low power systems. Aspects for this purpose have not been studied rigorously. We partly close this gap by analyzing reduced precision implementations of BNCs. In detail, we investigate the quantization of the parameters of BNCs with discrete valued nodes including the implications on the classification rate (CR). We derive worst-case and probabilistic bounds on the CR for different bit-widths. These bounds are evaluated on several benchmark datasets. Furthermore, we compare the classification performance and the robustness of BNCs with generatively and discriminatively optimized parameters, i.e. parameters optimized for high data likelihood and parameters optimized for classification, with respect to parameter quantization. Generatively optimized parameters are more robust for very low bit-widths, i.e. less classifications change because of quantization. However, classification performance is better for discriminatively optimized parameters for all but very low bit-widths. Additionally, we perform analysis for margin-optimized tree augmented network (TAN) structures which outperform generatively optimized TAN structures in terms of CR and robustness. © 1979-2012 IEEE.","Bayesian network classifiers; custom precision; discriminative learning; quantization"
"Prusa D., Werner T.","Universality of the local marginal polytope",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","4", 6888502,"898","904",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924567243&doi=10.1109%2fTPAMI.2014.2353626&partnerID=40&md5=c0e8a8116fcb6a87f5b77a07a659fa34","We show that solving the LP relaxation of the min-sum labeling problem (also known as MAP inference problem in graphical models, discrete energy minimization, or valued constraint satisfaction) is not easier than solving any linear program. Precisely, every polytope is linear-time representable by a local marginal polytope and every LP can be reduced in linear time to a linear optimization (allowing infinite costs) over a local marginal polytope. The reduction can be done (though with a higher time complexity) even if the local marginal polytope is restricted to have a planar structure. © 2015 IEEE.","discrete energy minimization; Graphical model; linear programming relaxation; local marginal polytope; Markov random field; valued constraint satisfaction"
"Yang Q.","Stereo matching using tree filtering",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","4", 6888475,"834","846",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924611154&doi=10.1109%2fTPAMI.2014.2353642&partnerID=40&md5=6bd8aeedd56b1cf028a927d3a4f60a00","Matching cost aggregation is one of the oldest and still popular methods for stereo correspondence. While effective and efficient, cost aggregation methods typically aggregate the matching cost by summing/averaging over a user-specified, local support region. This is obviously only locally-optimal, and the computational complexity of the full-kernel implementation usually depends on the region size. In this paper, the cost aggregation problem is re-examined and a non-local solution is proposed. The matching cost values are aggregated adaptively based on pixel similarity on a tree structure derived from the stereo image pair to preserve depth edges. The nodes of this tree are all the image pixels, and the edges are all the edges between the nearest neighboring pixels. The similarity between any two pixels is decided by their shortest distance on the tree. The proposed method is non-local as every node receives supports from all other nodes on the tree. The proposed method can be naturally extended to the time domain for enforcing temporal coherence. Unlike previous methods, the non-local property guarantees that the depth edges will be preserved when the temporal coherency between all the video frames are considered. A non-local weighted median filter is also proposed based on the non-local cost aggregation algorithm. It has been demonstrated to outperform all local weighted median filters on disparity/depth upsampling and refinement. © 2015 IEEE.","bilateral filtering; edge-preserving smoothing; minimum spanning tree; Stereo matching"
"Yeung S.-K., Wu T.-P., Tang C.-K., Chan T.F., Osher S.J.","Normal estimation of a transparent object using a video",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","4",,"890","897",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924567244&doi=10.1109%2fTPAMI.2014.2346195&partnerID=40&md5=b6a0a5c5e07e60c6484abd1cb9a245db","Reconstructing transparent objects is a challenging problem. While producing reasonable results for quite complex objects, existing approaches require custom calibration or somewhat expensive labor to achieve high precision. When an overall shape preserving salient and fine details is sufficient, we show in this paper a significant step toward solving the problem when the object's silhouette is available and simple user interaction is allowed, by using a video of a transparent object shot under varying illumination. Specifically, we estimate the normal map of the exterior surface of a given solid transparent object, from which the surface depth can be integrated. Our technical contribution lies in relating this normal estimation problem to one of graph-cut segmentation. Unlike conventional formulations, however, our graph is dual-layered, since we can see a transparent object's foreground as well as the background behind it. Quantitative and qualitative evaluation are performed to verify the efficacy of this practical solution. © 2015 IEEE.","graph-cuts; normal estimation; segmentation; Transparent object"
"Ferrer M.A., Diaz-Cabrera M., Morales A.","Static signature synthesis: A neuromotor inspired approach for biometrics",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","3", 6867369,"667","680",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923052487&doi=10.1109%2fTPAMI.2014.2343981&partnerID=40&md5=6855562e73814af9242f6a1fdcf67967","In this paper we propose a new method for generating synthetic handwritten signature images for biometric applications. The procedures we introduce imitate the mechanism of motor equivalence which divides human handwriting into two steps: the working out of an effector independent action plan and its execution via the corresponding neuromuscular path. The action plan is represented as a trajectory on a spatial grid. This contains both the signature text and its flourish, if there is one. The neuromuscular path is simulated by applying a kinematic Kaiser filter to the trajectory plan. The length of the filter depends on the pen speed which is generated using a scalar version of the sigma lognormal model. An ink deposition model, applied pixel by pixel to the pen trajectory, provides realistic static signature images. The lexical and morphological properties of the synthesized signatures as well as the range of the synthesis parameters have been estimated from real databases of real signatures such as the MCYT Off-line and the GPDS960GraySignature corpuses. The performance experiments show that by tuning only four parameters it is possible to generate synthetic identities with different stability and forgers with different skills. Therefore it is possible to create datasets of synthetic signatures with a performance similar to databases of real signatures. Moreover, we can customize the created dataset to produce skilled forgeries or simple forgeries which are easier to detect, depending on what the researcher needs. Perceptual evaluation gives an average confusion of 44.06 percent between real and synthetic signatures which shows the realism of the synthetic ones. The utility of the synthesized signatures is demonstrated by studying the influence of the pen type and number of users on an automatic signature verifier. © 2014 IEEE.","Biometric recognition; ink deposition model; kinematic theory of human movements; motor equivalence theory; off-line signature verification; synthetic generation"
"Wu W., Chen Z., Gao X., Li Y., Brown E.N., Gao S.","Probabilistic common spatial patterns for multichannel EEG analysis",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","3", 6832647,"639","653",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923043542&doi=10.1109%2fTPAMI.2014.2330598&partnerID=40&md5=bb03e2f4d703c3d189b2b2c3dc770492","Common spatial patterns (CSP) is a well-known spatial filtering algorithm for multichannel electroencephalogram (EEG) analysis. In this paper, we cast the CSP algorithm in a probabilistic modeling setting. Specifically, probabilistic CSP (P-CSP) is proposed as a generic EEG spatio-temporal modeling framework that subsumes the CSP and regularized CSP algorithms. The proposed framework enables us to resolve the overfitting issue of CSP in a principled manner. We derive statistical inference algorithms that can alleviate the issue of local optima. In particular, an efficient algorithm based on eigendecomposition is developed for maximum a posteriori (MAP) estimation in the case of isotropic noise. For more general cases, a variational algorithm is developed for group-wise sparse Bayesian learning for the P-CSP model and for automatically determining the model size. The two proposed algorithms are validated on a simulated data set. Their practical efficacy is also demonstrated by successful applications to single-trial classifications of three motor imagery EEG data sets and by the spatio-temporal pattern analysis of one EEG data set recorded in a Stroop color naming task. © 2014 IEEE.","Brain-computer interface; Common spatial patterns; Electroencephalogram; Fukunaga-Koontz transform; Sparse bayesian learning; Variational bayes"
"Serradell E., Pinheiro M.A., Sznitman R., Kybic J., Moreno-Noguer F., Fua P.","Non-rigid graph registration using active testing search",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","3", 6866165,"625","638",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923013613&doi=10.1109%2fTPAMI.2014.2343235&partnerID=40&md5=c1280e5e24c5075fe1bfdbc2903b0180","We present a new approach for matching sets of branching curvilinear structures that form graphs embedded in ℝ2 or ℝ3 and may be subject to deformations. Unlike earlier methods, ours does not rely on local appearance similarity nor does require a good initial alignment. Furthermore, it can cope with non-linear deformations, topological differences, and partial graphs. To handle arbitrary non-linear deformations, we use Gaussian process regressions to represent the geometrical mapping relating the two graphs. In the absence of appearance information, we iteratively establish correspondences between points, update the mapping accordingly, and use it to estimate where to find the most likely correspondences that will be used in the next step. To make the computation tractable for large graphs, the set of new potential matches considered at each iteration is not selected at random as with many RANSAC-based algorithms. Instead, we introduce a so-called Active Testing Search strategy that performs a priority search to favor the most likely matches and speed-up the process. We demonstrate the effectiveness of our approach first on synthetic cases and then on angiography data, retinal fundus images, and microscopy image stacks acquired at very different resolutions. © 1979-2012 IEEE.","active testing search; Graph matching; non-rigid registration"
"Si X., Feng J., Zhou J., Luo Y.","Detection and rectification of distorted fingerprints",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","3", 7029762,"555","568",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923063476&doi=10.1109%2fTPAMI.2014.2345403&partnerID=40&md5=aef29d0791352036ac7e0c83c4c15d57","Elastic distortion of fingerprints is one of the major causes for false non-match. While this problem affects all fingerprintrecognition applications, it is especially dangerous in negative recognition applications, such as watchlist and deduplicationapplications. In such applications, malicious users may purposely distort their fingerprints to evade identification. In this paper, weproposed novel algorithms to detect and rectify skin distortion based on a single fingerprint image. Distortion detection is viewed as a two-class classification problem, for which the registered ridge orientation map and period map of a fingerprint are used as the feature vector and a SVM classifier is trained to perform the classification task. Distortion rectification (or equivalently distortion field estimation) is viewed as a regression problem, where the input is a distorted fingerprint and the output is the distortion field. To solve this problem, a database (called reference database) of various distorted reference fingerprints and corresponding distortion fields is built in the offline stage, and then in the online stage, the nearest neighbor of the input fingerprint is found in the reference database and the corresponding distortion field is used to transform the input fingerprint into a normal one. Promising results have been obtained on three databases containing many distorted fingerprints, namely FVC2004 DB1, Tsinghua Distorted Fingerprint database, and the NIST SD27 latent fingerprint database. © 1979-2012 IEEE.","distortion; Fingerprint; nearest neighbor regression; PCA; registration"
"Trzcinski T., Christoudias M., Lepetit V.","Learning image descriptors with boosting",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","3", 6867341,"597","610",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923059540&doi=10.1109%2fTPAMI.2014.2343961&partnerID=40&md5=cddec44f5428ba7c7b372f1bcf7d9565","We propose a novel and general framework to learn compact but highly discriminative floating-point and binary local feature descriptors. By leveraging the boosting-trick we first show how to efficiently train a compact floating-point descriptor that is very robust to illumination and viewpoint changes. We then present the main contribution of this paper - a binary extension of the framework that demonstrates the real advantage of our approach and allows us to compress the descriptor even further. Each bit of the resulting binary descriptor, which we call BinBoost, is computed with a boosted binary hash function, and we show how to efficiently optimize the hash functions so that they are complementary, which is key to compactness and robustness. As we do not put any constraints on the weak learner configuration underlying each hash function, our general framework allows us to optimize the sampling patterns of recently proposed hand-crafted descriptors and significantly improve their performance. Moreover, our boosting scheme can easily adapt to new applications and generalize to other types of image data, such as faces, while providing state-of-the-art results at a fraction of the matching time and memory footprint. © 1979-2012 IEEE.","binary embedding; boosting; Learning feature descriptors"
"Kong A.W.K.","A statistical analysis of iriscode and its security implications",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","3", 6867381,"513","528",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922983609&doi=10.1109%2fTPAMI.2014.2343959&partnerID=40&md5=3849ff5a8db045cd3f00dab8739560e4","IrisCode has been used to gather iris data for 430 million people. Because of the huge impact of IrisCode, it is vital that it is completely understood. This paper first studies the relationship between bit probabilities and a mean of iris images (The mean of iris images is defined as the average of independent iris images.) and then uses the Chi-square statistic, the correlation coefficient and a resampling algorithm to detect statistical dependence between bits. The results show that the statistical dependence forms a graph with a sparse and structural adjacency matrix. A comparison of this graph with a graph whose edges are defined by the inner product of the Gabor filters that produce IrisCodes shows that partial statistical dependence is induced by the filters and propagates through the graph. Using this statistical information, the security risk associated with two patented template protection schemes that have been deployed in commercial systems for producing application-specific IrisCodes is analyzed. To retain high identification speed, they use the same key to lock all IrisCodes in a database. The belief has been that if the key is not compromised, the IrisCodes are secure. This study shows that even without the key, application-specific IrisCodes can be unlocked and that the key can be obtained through the statistical dependence detected. © 1979-2012 IEEE.","Biometrics; Daugman algorithm; iris recognition; statistical dependence; template protection"
"Zhu Y., Lucey S.","Convolutional sparse coding for trajectory reconstruction",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","3", 6687214,"529","540",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922991765&doi=10.1109%2fTPAMI.2013.2295311&partnerID=40&md5=0ef476f4f2d973eb2d7d0921f3fdc6a4","Trajectory basis Non-Rigid Structure from Motion (NRSfM) refers to the process of reconstructing the 3D trajectory of each point of a non-rigid object from just their 2D projected trajectories. Reconstruction relies on two factors: (i) the condition of the composed camera &amp; trajectory basis matrix, and (ii) whether the trajectory basis has enough degrees of freedom to model the 3D point trajectory. These two factors are inherently conflicting. Employing a trajectory basis with small capacity has the positive characteristic of reducing the likelihood of an ill-conditioned system (when composed with the camera) during reconstruction. However, this has the negative characteristic of increasing the likelihood that the basis will not be able to fully model the object's ""true"" 3D point trajectories. In this paper we draw upon a well known result centering around the Reduced Isometry Property (RIP) condition for sparse signal reconstruction. RIP allow us to relax the requirement that the full trajectory basis composed with the camera matrix must be well conditioned. Further, we propose a strategy for learning an over-complete basis using convolutional sparse coding from naturally occurring point trajectory corpora to increase the likelihood that the RIP condition holds for a broad class of point trajectories and camera motions. Finally, we propose an ℓ1 inspired objective for trajectory reconstruction that is able to ""adaptively"" select the smallest sub-matrix from an over-complete trajectory basis that balances (i) and (ii). We present more practical 3D reconstruction results compared to current state of the art in trajectory basis NRSfM. © 2013 IEEE.","convolutional sparse coding; Nonrigid structure from motion; reconstructability; ℓ0 norm; ℓ1 norm"
"Liu H., Latecki L.J., Yan S.","Dense subgraph partition of positive hypergraphs",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","3", 6873327,"541","554",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923062574&doi=10.1109%2fTPAMI.2014.2346173&partnerID=40&md5=cbbbf1a6c33a183340178185ff33a67d","In this paper, we present a novel partition framework, called dense subgraph partition (DSP), to automatically, precisely and efficiently decompose a positive hypergraph into dense subgraphs. A positive hypergraph is a graph or hypergraph whose edges, except self-loops, have positive weights. We first define the concepts of core subgraph, conditional core subgraph, and disjoint partition of a conditional core subgraph, then define DSP based on them. The result of DSP is an ordered list of dense subgraphs with decreasing densities, which uncovers all underlying clusters, as well as outliers. A divide-and-conquer algorithm, called min-partition evolution, is proposed to efficiently compute the partition. DSP has many appealing properties. First, it is a nonparametric partition and it reveals all meaningful clusters in a bottom-up way. Second, it has an exact and efficient solution, called min-partition evolution algorithm. The min-partition evolution algorithm is a divide-and-conquer algorithm, thus time-efficient and memory-friendly, and suitable for parallel processing. Third, it is a unified partition framework for a broad range of graphs and hypergraphs. We also establish its relationship with the densest k-subgraph problem (Dk S), an NP-hard but fundamental problem in graph theory, and prove that DSP gives precise solutions to Dk S for all k in a graph-dependent set, called critical k-set. To our best knowledge, this is a strong result which has not been reported before. Moreover, as our experimental results show, for sparse graphs, especially web graphs, the size of critical k -set is close to the number of vertices in the graph. We test the proposed partition framework on various tasks, and the experimental results clearly illustrate its advantages. © 1979-2012 IEEE.","dense subgraph; densest k-subgraph; Graph partition; image matching; mode seeking"
"Lezama J., Morel J.-M., Randall G., Grompone Von Gioi R.","A contrario 2D point alignment detection",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","3", 6871384,"499","512",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923079855&doi=10.1109%2fTPAMI.2014.2345389&partnerID=40&md5=226082f5d3c9d4eb88150f2541f07584","In spite of many interesting attempts, the problem of automatically finding alignments in a 2D set of points seems to be still open. The difficulty of the problem is illustrated here by very simple examples. We then propose an elaborate solution. We show that a correct alignment detection depends on not less than four interlaced criteria, namely the amount of masking in texture, the relative bilateral local density of the alignment, its internal regularity, and finally a redundancy reduction step. Extending tools of the a contrario detection theory, we show that all of these detection criteria can be naturally embedded in a single probabilistic a contrario model with a single user parameter, the number of false alarms. Our contribution to the a contrario theory is the use of sophisticated conditional events on random point sets, for which expectation we nevertheless find easy bounds. By these bounds the mathematical consistency of our detection model receives a simple proof. Our final algorithm also includes a new formulation of the exclusion principle in Gestalt theory to avoid redundant detections. Aiming at reproducibility, a source code and an online demo open to any data point set are provided. The method is carefully compared to three state-of-the-art algorithms and an application to real data is discussed. Limitations of the final method are also illustrated and explained. © 2014 IEEE.","A contrario methods; Clustering; Point alignment detection; Poisson point process"
"Delgado-Friedrichs O., Robins V., Sheppard A.","Skeletonization and partitioning of digital images using discrete morse theory",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","3", 6873268,"654","666",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923039971&doi=10.1109%2fTPAMI.2014.2346172&partnerID=40&md5=16701d8889c4578754e5865d5f96b81c","We show how discrete Morse theory provides a rigorous and unifying foundation for defining skeletons and partitions of grayscale digital images. We model a grayscale image as a cubical complex with a real-valued function defined on its vertices (the voxel values). This function is extended to a discrete gradient vector field using the algorithm presented in Robins, Wood, Sheppard TPAMI 33:1646 (2011). In the current paper we define basins (the building blocks of a partition) and segments of the skeleton using the stable and unstable sets associated with critical cells. The natural connection between Morse theory and homology allows us to prove the topological validity of these constructions; for example, that the skeleton is homotopic to the initial object. We simplify the basins and skeletons via Morse-theoretic cancellation of critical cells in the discrete gradient vector field using a strategy informed by persistent homology. Simple working Python code for our algorithms for efficient vector field traversal is included. Example data are taken from micro-CT images of porous materials, an application area where accurate topological models of pore connectivity are vital for fluid-flow modelling. © 1979-2012 IEEE.","Curve skeleton; discrete Morse theory; medial axis transform; persistent homology; surface skeleton; watershed transform"
"Park C., Woehl T.J., Evans J.E., Browning N.D.","Minimum cost multi-way data association for optimizing multitarget tracking of interacting objects",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","3", 6873308,"611","624",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923015059&doi=10.1109%2fTPAMI.2014.2346202&partnerID=40&md5=120717167605b013ee05c2fb31f870c8","This paper presents a general formulation for a minimum cost data association problem which associates data features via one-to-one, m-to-one and one-to-n links with minimum total cost of the links. A motivating example is a problem of tracking multiple interacting nanoparticles imaged on video frames, where particles can aggregate into one particle or a particle can be split into multiple particles. Many existing multitarget tracking methods are capable of tracking non-interacting targets or tracking interacting targets of restricted degrees of interactions. The proposed formulation solves a multitarget tracking problem for general degrees of inter-object interactions. The formulation is in the form of a binary integer programming problem. We propose a polynomial time solution approach that can obtain a good relaxation solution of the binary integer programming, so the approach can be applied for multitarget tracking problems of a moderate size (for hundreds of targets over tens of time frames). The resulting solution is always integral and obtains a better duality gap than the simple linear relaxation solution of the corresponding problem. The proposed method was validated through applications to simulated multitarget tracking problems and a real multitarget tracking problem. © 1979-2012 IEEE.","binary integer programming; Data association; decomposition; lagrange dual relaxation"
"Kumar A., Kwong C.","Towards contactless, low-cost and accurate 3D fingerprint identification",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","3", 6857403,"681","696",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923027404&doi=10.1109%2fTPAMI.2014.2339818&partnerID=40&md5=a7af194a06a47fd0156a0f4c90889c16","Human identification using fingerprint impressions has been widely studied and employed for more than 2000 years. Despite new advancements in the 3D imaging technologies, widely accepted representation of 3D fingerprint features and matching methodology is yet to emerge. This paper investigates 3D representation of widely employed 2D minutiae features by recovering and incorporating (i) minutiae height z and (ii) its 3D orientation φ information and illustrates an effective matching strategy for matching popular minutiae features extended in 3D space. One of the obstacles of the emerging 3D fingerprint identification systems to replace the conventional 2D fingerprint system lies in their bulk and high cost, which is mainly contributed from the usage of structured lighting system or multiple cameras. This paper attempts to addresses such key limitations of the current 3D fingerprint technologies bydeveloping the single camera-based 3D fingerprint identification system. We develop a generalized 3D minutiae matching model and recover extended 3D fingerprint features from the reconstructed 3D fingerprints. 2D fingerprint images acquired for the 3D fingerprint reconstruction can themselves be employed for the performance improvement and have been illustrated in the work detailed in this paper. This paper also attempts to answer one of the most fundamental questions on the availability of inherent discriminableinformation from 3D fingerprints. The experimental results are presented on a database of 240 clients 3D fingerprints, which is made publicly available to further research efforts in this area, and illustrate the discriminant power of 3D minutiae representation andmatching to achieve performance improvement. © 1979-2012 IEEE.","3d fingerprint individuality; 3d fingerprint matching; 3d minutiae; Biometrics; contactless fingerprint identification; photometric stereo"
"Cheng M.-M., Mitra N.J., Huang X., Torr P.H.S., Hu S.-M.","Global contrast based salient region detection",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","3", 6871397,"569","582",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923094805&doi=10.1109%2fTPAMI.2014.2345401&partnerID=40&md5=0c311706310c498255ffb0a387157b6d","Automatic estimation of salient object regions across images, without any prior assumption or knowledge of the contents of the corresponding scenes, enhances many computer vision and computer graphics applications. We introduce a regional contrast based salient object detection algorithm, which simultaneously evaluates global contrast differences and spatial weighted coherence scores. The proposed algorithm is simple, efficient, naturally multi-scale, and produces full-resolution, high-quality saliency maps. These saliency maps are further used to initialize a novel iterative version of GrabCut, namely SaliencyCut, for high quality unsupervised salient object segmentation. We extensively evaluated our algorithm using traditional salient object detection datasets, as well as a more challenging Internet image dataset. Our experimental results demonstrate that our algorithm consistently outperforms 15 existing salient object detection and segmentation methods, yielding higher precision and better recall rates. We also show that our algorithm can be used to efficiently extract salient object masks from Internet images, enabling effective sketch-based image retrieval (SBIR) via simple shape comparisons. Despite such noisy internet images, where the saliency regions are ambiguous, our saliency guided image retrieval achieves a superior retrieval rate compared with state-of-the-art SBIR methods, and additionally provides important target object region information. © 2014 IEEE.","image retrieval; saliency map; Salient object detection; unsupervised segmentation; visual attention"
"Blomstedt P., Tang J., Xiong J., Granlund C., Corander J.","A bayesian predictive model for clustering data of mixed discrete and continuous type",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","3", 6810157,"489","498",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923064465&doi=10.1109%2fTPAMI.2014.2359431&partnerID=40&md5=7792f9686e761216e79feeb624ab61f2","Advantages of model-based clustering methods over heuristic alternatives have been widely demonstrated in the literature. Most model-based clustering algorithms assume that the data are either discrete or continuous, possibly allowing both types to be present in separate features. In this paper, we introduce a model-based approach for clustering feature vectors of mixed type, allowing each feature to simultaneously take on both categorical and real values. Such data may be encountered, for instance, in chemical and biological analyses, in the analysis of survey data, as well as in image analysis. Our model is formulated within a Bayesian predictive framework, where clustering solutions correspond to random partitions of the data. Using conjugate analysis, the posterior probability for each possible partition can be determined analytically, enabling the utilization of efficient computational search strategies for finding the posterior optimal partition. The derived model is illustrated using several synthetic and real datasets. © 1979-2012 IEEE.","Bayes methods; mixed distributions; predictive models; unsupervised learning"
"Henriques J.F., Caseiro R., Martins P., Batista J.","High-speed tracking with kernelized correlation filters",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","3", 6870486,"583","596",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922907906&doi=10.1109%2fTPAMI.2014.2345390&partnerID=40&md5=e20c5cac25b1fbf1cc1094852a9389ae","The core component of most modern trackers is a discriminative classifier, tasked with distinguishing between the target and the surrounding environment. To cope with natural image changes, this classifier is typically trained with translated and scaled sample patches. Such sets of samples are riddled with redundancies - any overlapping pixels are constrained to be the same. Based on this simple observation, we propose an analytic model for datasets of thousands of translated patches. By showing that the resulting data matrix is circulant, we can diagonalize it with the discrete Fourier transform, reducing both storage and computation by several orders of magnitude. Interestingly, for linear regression our formulation is equivalent to a correlation filter, used by some of the fastest competitive trackers. For kernel regression, however, we derive a new kernelized correlation filter (KCF), that unlike other kernel algorithms has the exact same complexity as its linear counterpart. Building on it, we also propose a fast multi-channel extension of linear correlation filters, via a linear kernel, which we call dual correlation filter (DCF). Both KCF and DCF outperform top-ranking trackers such as Struck or TLD on a 50 videos benchmark, despite running at hundreds of frames-per-second, and being implemented in a few lines of code (Algorithm 1). To encourage further developments, our tracking framework was made open-source. © 1979-2012 IEEE.","circulant matrices; correlation filters; discrete Fourier transform; kernel methods; ridge regression; Visual tracking"
"Adams R.P., Fox E.B., Sudderth E.B., Teh Y.W.","Guest editors' introduction to the special issue on bayesian nonparametrics",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 7004120,"209","211",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920945317&doi=10.1109%2fTPAMI.2014.2380478&partnerID=40&md5=342c2672279bfbaab1ec139365b933c2","The articles in this special issue discuss the applications supported by Bayesian nonparametric modeling. These probabilistic models defined over infinite-dimensional parameter spaces. For Gaussian process models of regression and classification functions, the parameter space consists of a set of continuous functions. For the Dirichlet process mixture models used in density estimation and clustering, the parameter space is dense in the space of probability measures. Bayesian nonparametric models provide a flexible framework for modeling complex data and a promising alternative to classical model selection methods. Due to recent computational advances, these approaches have received increasing attention in machine learning, statistics, probability, and related application domains. © 1979-2012 IEEE.",
"Polatkan G., Zhou M., Carin L., Blei D., Daubechies I.","A bayesian nonparametric approach to image super-resolution",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6809161,"346","358",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920996879&doi=10.1109%2fTPAMI.2014.2321404&partnerID=40&md5=83b0f791d61f1aaa1dd14bf7a61227bf","Super-resolution methods form high-resolution images from low-resolution images. In this paper, we develop a new Bayesian nonparametric model for super-resolution. Our method uses a beta-Bernoulli process to learn a set of recurring visual patterns, called dictionary elements, from the data. Because it is nonparametric, the number of elements found is also determined from the data. We test the results on both benchmark and natural images, comparing with several other models from the research literature. We perform large-scale human evaluation experiments to assess the visual quality of the results. In a first implementation, we use Gibbs sampling to approximate the posterior. However, this algorithm is not feasible for large-scale data. To circumvent this, we then develop an online variational Bayes (VB) algorithm. This algorithm finds high quality dictionaries in a fraction of the time needed by the Gibbs sampler. © 1979-2012 IEEE.",
"Gershman S.J., Frazier P.I., Blei D.M.","Distance dependent infinite latent feature models",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6809186,"334","345",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920972649&doi=10.1109%2fTPAMI.2014.2321387&partnerID=40&md5=3246ab16697bb6a296c06c3813b042be","Latent feature models are widely used to decompose data into a small number of components. Bayesian nonparametric variants of these models, which use the Indian buffet process (IBP) as a prior over latent features, allow the number of features to be determined from the data. We present a generalization of the IBP, the distance dependent Indian buffet process (dd-IBP), for modeling non-exchangeable data. It relies on distances defined between data points, biasing nearby data to share more features. The choice of distance measure allows for many kinds of dependencies, including temporal and spatial. Further, the original IBP is a special case of the dd-IBP. We develop the dd-IBP and theoretically characterize its feature-sharing properties. We derive a Markov chain Monte Carlo sampler for a linear Gaussian model with a dd-IBP prior and study its performance on real-world non-exchangeable data. © 1979-2012 IEEE.",
"Hensman J., Rattray M., Lawrence N.D.","Fast nonparametric clustering of structured time-series",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6802369,"383","393",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920982448&doi=10.1109%2fTPAMI.2014.2318711&partnerID=40&md5=8d6554e5d7303f9f4dce136855244b6e","In this publication, we combine two Bayesian nonparametric models: the Gaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP model is to introduce a variation on the GP prior which enables us to model structured time-series data, i.e., data containing groups where we wish to model inter- and intra-group variability. Our innovation in the DP model is an implementation of a new fast collapsed variational inference procedure which enables us to optimize our variational approximation significantly faster than standard VB approaches. In a biological time series application we show how our model better captures salient features of the data, leading to better consistency with existing biological classifications, while the associated inference algorithm provides a significant speed-up over EM-based variational inference. © 1979-2012 IEEE.",
"Doshi-Velez F., Pfau D., Wood F., Roy N.","Bayesian nonparametric methods for partially-observable reinforcement learning",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6616533,"394","407",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920971066&doi=10.1109%2fTPAMI.2013.191&partnerID=40&md5=36866e025219973e29aa30be2e398168","Making intelligent decisions from incomplete information is critical in many applications: for example, robots must choose actions based on imperfect sensors, and speech-based interfaces must infer a user's needs from noisy microphone inputs. What makes these tasks hard is that often we do not have a natural representation with which to model the domain and use for choosing actions; we must learn about the domain's properties while simultaneously performing the task. Learning a representation also involves trade-offs between modeling the data that we have seen previously and being able to make predictions about new data. This article explores learning representations of stochastic systems using Bayesian nonparametric statistics. Bayesian nonparametric methods allow the sophistication of a representation to scale gracefully with the complexity in the data. Our main contribution is a careful empirical evaluation of how representations learned using Bayesian nonparametric methods compare to other standard learning approaches, especially in support of planning and control. We show that the Bayesian aspects of the methods result in achieving state-of-the-art performance in decision making with relatively few samples, while the nonparametric aspects often result in fewer computations. These results hold across a variety of different techniques for choosing actions given a representation. © 1979-2012 IEEE.","HDP-HMM; POMDP; Reinforcement Learning"
"Dai A.M., Storkey A.J.","The supervised hierarchical dirichlet process",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6784083,"243","255",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921025724&doi=10.1109%2fTPAMI.2014.2315802&partnerID=40&md5=43101db4c6a4cd6c99731872d90ff9d0","We propose the supervised hierarchical Dirichlet process (sHDP), a nonparametric generative model for the joint distribution of a group of observations and a response variable directly associated with that whole group. We compare the sHDP with another leading method for regression on grouped data, the supervised latent Dirichlet allocation (sLDA) model. We evaluate our method on two real-world classification problems and two real-world regression problems. Bayesian nonparametric regression models based on the Dirichlet process, such as the Dirichlet process-generalised linear models (DP-GLM) have previously been explored; these models allow flexibility in modelling nonlinear relationships. However, until now, hierarchical Dirichlet process (HDP) mixtures have not seen significant use in supervised problems with grouped data since a straightforward application of the HDP on the grouped data results in learnt clusters that are not predictive of the responses. The sHDP solves this problem by allowing for clusters to be learnt jointly from the group structure and from the label assigned to each group. © 2015 IEEE.","Bayesian nonparametrics; hierarchical Dirichlet process; latent Dirichlet allocation; topic modelling"
"Broderick T., Mackey L., Paisley J., Jordan M.I.","Combinatorial clustering and the beta negative binomial process",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6802382,"290","306",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920964358&doi=10.1109%2fTPAMI.2014.2318721&partnerID=40&md5=068326891d8c6fc33efae25a02e62988","We develop a Bayesian nonparametric approach to a general family of latent class problems in which individuals can belong simultaneously to multiple classes and where each class can be exhibited multiple times by an individual. We introduce a combinatorial stochastic process known as the negative binomial process (NBP ) as an infinite-dimensional prior appropriate for such problems. We show that the NBP is conjugate to the beta process, and we characterize the posterior distribution under the beta-negative binomial process (BNBP) and hierarchical models based on the BNBP (the HBNBP ). We study the asymptotic properties of the BNBP and develop a three-parameter extension of the BNBP that exhibits power-law behavior. We derive MCMC algorithms for posterior inference under the HBNBP, and we present experiments using these algorithms in the domains of image segmentation, object recognition, and document analysis. © 1979-2012 IEEE.",
"Orbanz P., Roy D.M.","Bayesian models of graphs, arrays and other exchangeable random structures",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6847223,"437","461",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920964398&doi=10.1109%2fTPAMI.2014.2334607&partnerID=40&md5=81269e2722435da0724af7dfe1830211","The natural habitat of most Bayesian methods is data represented by exchangeable sequences of observations, for which de Finetti's theorem provides the theoretical foundation. Dirichlet process clustering, Gaussian process regression, and many other parametric and nonparametric Bayesian models fall within the remit of this framework; many problems arising in modern data analysis do not. This article provides an introduction to Bayesian models of graphs, matrices, and other data that can be modeled by random structures. We describe results in probability theory that generalize de Finetti's theorem to such data and discuss their relevance to nonparametric Bayesian modeling. With the basic ideas in place, we survey example models available in the literature; applications of such models include collaborative filtering, link prediction, and graph and network analysis. We also highlight connections to recent developments in graph theory and probability, and sketch the more general mathematical foundation of Bayesian methods for other types of data beyond sequences and arrays. © 1979-2012 IEEE.",
"Xu Z., Yan F., Qi Y.","Bayesian nonparametric models for multiway data analysis",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6629993,"475","487",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920982450&doi=10.1109%2fTPAMI.2013.201&partnerID=40&md5=04dd231820a225399b789720497e70f7","Tensor decomposition is a powerful computational tool for multiway data analysis. Many popular tensor decomposition approaches - such as the Tucker decomposition and CANDECOMP/PARAFAC (CP) - amount to multi-linear factorization. They are insufficient to model (i) complex interactions between data entities, (ii) various data types (e.g., missing data and binary data), and (iii) noisy observations and outliers. To address these issues, we propose tensor-variate latent nonparametric Bayesian models for multiway data analysis. We name these models InfTucker. These new models essentially conduct Tucker decomposition in an infinite feature space. Unlike classical tensor decomposition models, our new approaches handle both continuous and binary data in a probabilistic framework. Unlike previous Bayesian models on matrices and tensors, our models are based on latent Gaussian or $t$ processes with nonlinear covariance functions. Moreover, on network data, our models reduce to nonparametric stochastic blockmodels and can be used to discover latent groups and predict missing interactions. To learn the models efficiently from data, we develop a variational inference technique and explore properties of the Kronecker product for computational efficiency. Compared with a classical variational implementation, this technique reduces both time and space complexities by several orders of magnitude. On real multiway and network data, our new models achieved significantly higher prediction accuracy than state-of-art tensor decomposition methods and blockmodels. © 1979-2012 IEEE.","Algorithms for data and knowledge management; Machine learning"
"Deisenroth M.P., Fox D., Rasmussen C.E.","Gaussian processes for data-efficient learning in robotics and control",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6654139,"408","423",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920973635&doi=10.1109%2fTPAMI.2013.218&partnerID=40&md5=55fbfbee22b68ece0b495f09779d9510","Autonomous learning has been a promising direction in control and robotics for more than a decade since data-driven learning allows to reduce the amount of engineering knowledge, which is otherwise required. However, autonomous reinforcement learning (RL) approaches typically require many interactions with the system to learn controllers, which is a practical limitation in real systems, such as robots, where many interactions can be impractical and time consuming. To address this problem, current learning approaches typically require task-specific knowledge in form of expert demonstrations, realistic simulators, pre-shaped policies, or specific knowledge about the underlying dynamics. In this paper, we follow a different approach and speed up learning by extracting more information from data. In particular, we learn a probabilistic, non-parametric Gaussian process transition model of the system. By explicitly incorporating model uncertainty into long-term planning and controller learning our approach reduces the effects of model errors, a key problem in model-based learning. Compared to state-of-the art RL our model-based policy search method achieves an unprecedented speed of learning. We demonstrate its applicability to autonomous learning in real robot and control tasks. © 1979-2012 IEEE.","Bayesian inference; Control; Gaussian processes; Policy search; Reinforcement learning; Robotics"
"Archambeau C., Lakshminarayanan B., Bouchard G.","Latent IBP compound dirichlet allocation",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6780626,"321","333",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920993124&doi=10.1109%2fTPAMI.2014.2313122&partnerID=40&md5=e8a504093dac45293bc5935485cfea22","We introduce the four-parameter IBP compound Dirichlet process (ICDP), a stochastic process that generates sparse non-negative vectors with potentially an unbounded number of entries. If we repeatedly sample from the ICDP we can generate sparse matrices with an infinite number of columns and power-law characteristics. We apply the four-parameter ICDP to sparse nonparametric topic modelling to account for the very large number of topics present in large text corpora and the power-law distribution of the vocabulary of natural languages. The model, which we call latent IBP compound Dirichlet allocation (LIDA), allows for power-law distributions, both, in the number of topics summarising the documents and in the number of words defining each topic. It can be interpreted as a sparse variant of the hierarchical Pitman-Yor process when applied to topic modelling. We derive an efficient and simple collapsed Gibbs sampler closely related to the collapsed Gibbs sampler of latent Dirichlet allocation (LDA), making the model applicable in a wide range of domains. Our nonparametric Bayesian topic model compares favourably to the widely used hierarchical Dirichlet process and its heavy tailed version, the hierarchical Pitman-Yor process, on benchmark corpora. Experiments demonstrate that accounting for the power-distribution of real data is beneficial and that sparsity provides more interpretable results. © 1979-2012 IEEE.","bag-of-words representation; Bayesian nonparametrics; clustering; Gibbs sampling; power-law distribution; sparse modelling; topic modelling"
"Chen C., Buntine W., Ding N., Xie L., Du L.","Differential topic models",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6777293,"230","242",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920936716&doi=10.1109%2fTPAMI.2014.2313127&partnerID=40&md5=9569696529d350bebc61187bf4ddc666","In applications we may want to compare different document collections: they could have shared content but also different and unique aspects in particular collections. This task has been called comparative text mining or cross-collection modeling. We present a differential topic model for this application that models both topic differences and similarities. For this we use hierarchical Bayesian nonparametric models. Moreover, we found it was important to properly model power-law phenomena in topic-word distributions and thus we used the full Pitman-Yor process rather than just a Dirichlet process. Furthermore, we propose the transformed Pitman-Yor process (TPYP) to incorporate prior knowledge such as vocabulary variations in different collections into the model. To deal with the non-conjugate issue between model prior and likelihood in the TPYP, we thus propose an efficient sampling algorithm using a data augmentation technique based on the multinomial theorem. Experimental results show the model discovers interesting aspects of different collections. We also show the proposed MCMC based algorithm achieves a dramatically reduced test perplexity compared to some existing topic models. Finally, we show our model outperforms the state-of-the-art for document classification/ideology prediction on a number of text collections. © 1979-2012 IEEE.",
"Zhou M., Carin L.","Negative binomial process count and mixture modeling",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6636308,"307","320",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920998554&doi=10.1109%2fTPAMI.2013.211&partnerID=40&md5=09c8f4a55fef8f81377ad92f0c9ae356","The seemingly disjoint problems of count and mixture modeling are united under the negative binomial (NB) process. A gamma process is employed to model the rate measure of a Poisson process, whose normalization provides a random probability measure for mixture modeling and whose marginalization leads to an NB process for count modeling. A draw from the NB process consists of a Poisson distributed finite number of distinct atoms, each of which is associated with a logarithmic distributed number of data samples. We reveal relationships between various count- and mixture-modeling distributions and construct a Poisson-logarithmic bivariate distribution that connects the NB and Chinese restaurant table distributions. Fundamental properties of the models are developed, and we derive efficient Bayesian inference. It is shown that with augmentation and normalization, the NB process and gamma-NB process can be reduced to the Dirichlet process and hierarchical Dirichlet process, respectively. These relationships highlight theoretical, structural, and computational advantages of the NB process. A variety of NB processes, including the beta-geometric, beta-NB, marked-beta-NB, marked-gamma-NB and zero-inflated-NB processes, with distinct sharing mechanisms, are also constructed. These models are applied to topic modeling, with connections made to existing algorithms under Poisson factor analysis. Example results show the importance of inferring both the NB dispersion and probability parameters. © 1979-2012 IEEE.","Bayesian Nonparametrics; Beta Process; Chinese Restaurant Process; Completely Random Measures; Count Modeling; Dirichlet Process; Gamma Process; Hierarchical Dirichlet Process; Mixed-Membership Modeling; Mixture Modeling; Negative Binomial Process; Normalized Random Measures; Poisson Factor Analysis; Poisson Process; Topic Modeling"
"Knowles D.A., Ghahramani Z.","Pitman yor diffusion trees for bayesian hierarchical clustering",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6777276,"271","289",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920997639&doi=10.1109%2fTPAMI.2014.2313115&partnerID=40&md5=fa826792a00f0cde7263286620294d7a","In this paper we introduce the Pitman Yor Diffusion Tree (PYDT), a Bayesian non-parametric prior over tree structures which generalises the Dirichlet Diffusion Tree [30] and removes the restriction to binary branching structure. The generative process is described and shown to result in an exchangeable distribution over data points. We prove some theoretical properties of the model including showing its construction as the continuum limit of a nested Chinese restaurant process model. We then present two alternative MCMC samplers which allow us to model uncertainty over tree structures, and a computationally efficient greedy Bayesian EM search algorithm. Both algorithms use message passing on the tree structure. The utility of the model and algorithms is demonstrated on synthetic and real world data, both continuous and binary. © 1979-2012 IEEE.",
"Paisley J., Wang C., Blei D.M., Jordan M.I.","Nested hierarchical dirichlet processes",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6802355,"256","270",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920971064&doi=10.1109%2fTPAMI.2014.2318728&partnerID=40&md5=f51d1e6e039b9bf77a9f0db7e302c6ff","We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. The nHDP generalizes the nested Chinese restaurant process (nCRP) to allow each word to follow its own path to a topic node according to a per-document distribution over the paths on a shared tree. This alleviates the rigid, single-path formulation assumed by the nCRP, allowing documents to easily express complex thematic borrowings. We derive a stochastic variational inference algorithm for the model, which enables efficient inference for massive collections of text documents. We demonstrate our algorithm on 1.8 million documents from The New York Times and 2.7 million documents from Wikipedia. © 1979-2012 IEEE.",
"Foti N.J., Williamson S.A.","A survey of non-exchangeable priors for bayesian nonparametric models",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6654119,"359","371",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920982447&doi=10.1109%2fTPAMI.2013.224&partnerID=40&md5=74acd989b1637a391a87acdb2af003ee","Dependent nonparametric processes extend distributions over measures, such as the Dirichlet process and the beta process, to give distributions over collections of measures, typically indexed by values in some covariate space. Such models are appropriate priors when exchangeability assumptions do not hold, and instead we want our model to vary fluidly with some set of covariates. Since the concept of dependent nonparametric processes was formalized by MacEachern, there have been a number of models proposed and used in the statistics and machine learning literatures. Many of these models exhibit underlying similarities, an understanding of which, we hope, will help in selecting an appropriate prior, developing new models, and leveraging inference techniques. © 1979-2012 IEEE.","Introductory and Survey; Stochastic processes"
"Xu Z., Maceachern S., Xu X.","Modeling non-gaussian time series with nonparametric bayesian model",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6654132,"372","382",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920937591&doi=10.1109%2fTPAMI.2013.222&partnerID=40&md5=6850b80a55653df9558db11fd2b13ec4","We present a class of Bayesian copula models whose major components are the marginal (limiting) distribution of a stationary time series and the internal dynamics of the series. We argue that these are the two features with which an analyst is typically most familiar, and hence that these are natural components with which to work. For the marginal distribution, we use a nonparametric Bayesian prior distribution along with a cdf-inverse cdf transformation to obtain large support. For the internal dynamics, we rely on the traditionally successful techniques of normal-theory time series. Coupling the two components gives us a family of (Gaussian) copula transformed autoregressive models. The models provide coherent adjustments of time scales and are compatible with many extensions, including changes in volatility of the series. We describe basic properties of the models, show their ability to recover non-Gaussian marginal distributions, and use a GARCH modification of the basic model to analyze stock index return series. The models are found to provide better fit and improved short-range and long-range predictions than Gaussian competitors. The models are extensible to a large variety of fields, including continuous time models, spatial models, models for multiple series, models driven by external covariate streams, and non-stationary models. © 1979-2012 IEEE.","Autoregressive process; Copula model; GARCH; Probability integral transformation"
"De Blasi P., Favaro S., Lijoi A., Mena R.H., Prunster I., Ruggiero M.","Are gibbs-type priors the most natural generalization of the dirichlet process?",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6654160,"212","229",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920992178&doi=10.1109%2fTPAMI.2013.217&partnerID=40&md5=5e011167f94914529626e936337d4033","Discrete random probability measures and the exchangeable random partitions they induce are key tools for addressing a variety of estimation and prediction problems in Bayesian inference. Here we focus on the family of Gibbs-type priors, a recent elegant generalization of the Dirichlet and the Pitman-Yor process priors. These random probability measures share properties that are appealing both from a theoretical and an applied point of view: (i) they admit an intuitive predictive characterization justifying their use in terms of a precise assumption on the learning mechanism; (ii) they stand out in terms of mathematical tractability; (iii) they include several interesting special cases besides the Dirichlet and the Pitman-Yor processes. The goal of our paper is to provide a systematic and unified treatment of Gibbs-type priors and highlight their implications for Bayesian nonparametric inference. We deal with their distributional properties, the resulting estimators, frequentist asymptotic validation and the construction of time-dependent versions. Applications, mainly concerning mixture models and species sampling, serve to convey the main ideas. The intuition inherent to this class of priors and the neat results they lead to make one wonder whether it actually represents the most natural generalization of the Dirichlet process. © 1979-2012 IEEE.","Nonparametric statistics; Stochastic processes"
"Gilboa E., Saatci Y., Cunningham J.P.","Scaling multidimensional inference for structured gaussian processes",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6616550,"424","436",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920973636&doi=10.1109%2fTPAMI.2013.192&partnerID=40&md5=bcb435eb7dc7742ed086b546d2f440e9","Exact Gaussian process (GP) regression has O(N3) runtime for data size N, making it intractable for large N. Many algorithms for improving GP scaling approximate the covariance with lower rank matrices. Other work has exploited structure inherent in particular covariance functions, including GPs with implied Markov structure, and inputs on a lattice (both enable O(N) or O(N log N) runtime). However, these GP advances have not been well extended to the multidimensional input setting, despite the preponderance of multidimensional applications. This paper introduces and tests three novel extensions of structured GPs to multidimensional inputs, for models with additive and multiplicative kernels. First we present a new method for inference in additive GPs, showing a novel connection between the classic backfitting method and the Bayesian framework. We extend this model using two advances: a variant of projection pursuit regression, and a Laplace approximation for non-Gaussian observations. Lastly, for multiplicative kernel structure, we present a novel method for GPs with inputs on a multidimensional grid. We illustrate the power of these three advances on several data sets, achieving performance equal to or very close to the naive GP at orders of magnitude less cost. © 1979-2012 IEEE.","backfitting; Gaussian processes; Kronecker matrices; projection-pursuit regression"
"Palla K., Knowles D.A., Ghahramani Z.","Relational learning and network modelling using infinite latent attribute models",2014,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","2", 6815978,"462","474",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920996880&doi=10.1109%2fTPAMI.2014.2324586&partnerID=40&md5=845832d6ce164acbb034a187a093aa53","Latent variable models for network data extract a summary of the relational structure underlying an observed network. The simplest possible models subdivide nodes of the network into clusters; the probability of a link between any two nodes then depends only on their cluster assignment. Currently available models can be classified by whether clusters are disjoint or are allowed to overlap. These models can explain a 'flat' clustering structure. Hierarchical Bayesian models provide a natural approach to capture more complex dependencies. We propose a model in which objects are characterised by a latent feature vector. Each feature is itself partitioned into disjoint groups (subclusters), corresponding to a second layer of hierarchy. In experimental comparisons, the model achieves significantly improved predictive performance on social and biological link prediction tasks. The results indicate that models with a single layer hierarchy over-simplify real networks. © 1979-2012 IEEE.",
"Kang H., Hebert M., Efros A.A., Kanade T.","Data-driven objectness",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","1", 6783808,"189","195",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916898733&doi=10.1109%2fTPAMI.2014.2315811&partnerID=40&md5=c6992de91fd8f6d3eda729f2798e8c04","We propose a data-driven approach to estimate the likelihood that an image segment corresponds to a scene object (its ""objectness"") by comparing it to a large collection of example object regions. We demonstrate that when the application domain is known, for example, in our case activity of daily living (ADL), we can capture the regularity of the domain specific objects using millions of exemplar object regions. Our approach to estimating the objectness of an image region proceeds in two steps: 1) finding the exemplar regions that are the most similar to the input image segment; 2) calculating the objectness of the image segment by combining segment properties, mutual consistency across the nearest exemplar regions, and the prior probability of each exemplar region. In previous work, parametric objectness models were built from a small number of manually annotated objects regions, instead, our data-driven approach uses 5 million object regions along with their metadata information. Results on multiple data sets demonstrates our data-driven approach compared to the existing model based techniques. We also show the application of our approach in improving the performance of object discovery algorithms. © 2014 IEEE.","Activity of daily living (ADL); Data-driven; Object discovery; Objectness; Product images; Segment selection"
"Ben-Shahar O., Ben-Yosef G.","Tangent bundle elastica and computer vision",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","1", 6866207,"161","174",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916917794&doi=10.1109%2fTPAMI.2014.2343214&partnerID=40&md5=9ccd96ebf78c1bb4a5336872b58e8275","Visual curve completion, an early visual process that completes the occluded parts between observed boundary fragments (a.k.a. inducers), is a major problem in perceptual organization and a critical step toward higher level visual tasks in both biological and machine vision. Most computational contributions to solving this problem suggest desired perceptual properties that the completed contour should satisfy in the image plane, and then seek the mathematical curves that provide them. Alternatively, few studies (including by the authors) have suggested to frame the problem not in the image plane but rather in the unit tangent bundle R2 x S1, the space that abstracts the primary visual cortex, where curve completion allegedly occurs. Combining both schools, here we propose and develop a biologically plausible theory of elastica in the tangent bundle that provides not only perceptually superior completion results but also a rigorous computational prediction that inducer curvatures greatly affects the shape of the completed curve, as indeed indicated by human perception. © 2014 IEEE.","Curve completion; Elastica; Tangent bundle; Visual completion"
"Chen Q., Song Z., Dong J., Huang Z., Hua Y., Yan S.","Contextualizing object detection and classification",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","1", 6866901,"13","27",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916886558&doi=10.1109%2fTPAMI.2014.2343217&partnerID=40&md5=9ea92e1c53dd348a4085e10ce6e3d753","We investigate how to iteratively and mutually boost object classification and detection performance by taking the outputs from one task as the context of the other one. While context models have been quite popular, previous works mainly concentrate on co-occurrence relationship within classes and few of them focus on contextualization from a top-down perspective, i.e. high-level task context. In this paper, our system adopts a new method for adaptive context modeling and iterative boosting. First, the contextualized support vector machine (Context-SVM) is proposed, where the context takes the role of dynamically adjusting the classification score based on the sample ambiguity, and thus the context-adaptive classifier is achieved. Then, an iterative training procedure is presented. In each step, Context-SVM, associated with the output context from one task (object classification or detection), is instantiated to boost the performance for the other task, whose augmented outputs are then further used to improve the former task by Context-SVM. The proposed solution is evaluated on the object classification and detection tasks of PASCAL Visual Object Classes Challenge (VOC) 2007, 2010 and SUN09 data sets, and achieves the state-of-the-art performance. © 2014 IEEE.","Context modeling; Object classification; Object detection"
"Zhang M.-L., Wu L.","Lift: Multi-label learning with label-specific features",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","1", 6857380,"107","120",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916933802&doi=10.1109%2fTPAMI.2014.2339815&partnerID=40&md5=a352867ecc5e37ae904a65278e23187c","Multi-label learning deals with the problem where each example is represented by a single instance (feature vector) while associated with a set of class labels. Existing approaches learn from multi-label data by manipulating with identical feature set, i.e. the very instance representation of each example is employed in the discrimination processes of all class labels. However, this popular strategy might be suboptimal as each label is supposed to possess specific characteristics of its own. In this paper, another strategy to learn from multi-label data is studied, where label-specific features are exploited to benefit the discrimination of different class labels. Accordingly, an intuitive yet effective algorithm named LIFT, i.e. multi-label learning with Label specIfic FeaTures, is proposed. LIFT firstly constructs features specific to each label by conducting clustering analysis on its positive and negative instances, and then performs training and testing by querying the clustering results. Comprehensive experiments on a total of 17 benchmark data sets clearly validate the superiority of LIFT against other well-established multi-label learning algorithms as well as the effectiveness of label-specific features. © 2014 IEEE.","Label correlations; Label-specific features; Machine learning; Multi-label learning"
"Nock R., Bel Haj Ali W., D'Ambrosio R., Nielsen F., Barlaud M.","Gentle nearest neighbors boosting over proper scoring rules",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","1", 6747340,"80","93",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916910914&doi=10.1109%2fTPAMI.2014.2307877&partnerID=40&md5=ae82279178b11ba41c02c286f374afc6","Tailoring nearest neighbors algorithms to boosting is an important problem. Recent papers study an approach, UNN, which provably minimizes particular convex surrogates under weak assumptions. However, numerical issues make it necessary to experimentally tweak parts of the UNN algorithm, at the possible expense of the algorithm's convergence and performance. In this paper, we propose a lightweight Newton-Raphson alternative optimizing proper scoring rules from a very broad set, and establish formal convergence rates under the boosting framework that compete with those known for UNN. To the best of our knowledge, no such boosting-compliant convergence rates were previously known in the popular Gentle Adaboost's lineage. We provide experiments on a dozen domains, including Caltech and SUN computer vision databases, comparing our approach to major families including support vector machines, (Ada)boosting and stochastic gradient descent. They support three major conclusions: (i) GNNB significantly outperforms UNN, in terms of convergence rate and quality of the outputs, (ii) GNNB performs on par with or better than computationally intensive large margin approaches, (iii) on large domains that rule out those latter approaches for computational reasons, GNNB provides a simple and competitive contender to stochastic gradient descent. Experiments include a divide-and-conquer improvement of GNNB exploiting the link with proper scoring rules optimization. © 2014 IEEE.","Boosting; Nearest neighbors; Proper scoring rules"
"Xiong Y., Chakrabarti A., Basri R., Gortler S.J., Jacobs D.W., Zickler T.","From shading to local shape",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","1", 6866216,"67","79",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916910912&doi=10.1109%2fTPAMI.2014.2343211&partnerID=40&md5=7131f2c01b2126c2c67d0198c7a9c619","We develop a framework for extracting a concise representation of the shape information available from diffuse shading in a small image patch. This produces a mid-level scene descriptor, comprised of local shape distributions that are inferred separately at every image patch across multiple scales. The framework is based on a quadratic representation of local shape that, in the absence of noise, has guarantees on recovering accurate local shape and lighting. And when noise is present, the inferred local shape distributions provide useful shape information without over-committing to any particular image explanation. These local shape distributions naturally encode the fact that some smooth diffuse regions are more informative than others, and they enable efficient and robust reconstruction of object-scale shape. Experimental results show that this approach to surface reconstruction compares well against the state-of-art on both synthetic images and captured photographs. © 2014 IEEE.","3D reconstruction; Local shape descriptors; Shape from shading; Statistical models"
"Sironi A., Tekin B., Rigamonti R., Lepetit V., Fua P.","Learning separable filters",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","1", 6866160,"94","106",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916918697&doi=10.1109%2fTPAMI.2014.2343229&partnerID=40&md5=e7f9bca9b73353dabb92c8adaaac5474","Learning filters to produce sparse image representations in terms of overcomplete dictionaries has emerged as a powerful way to create image features for many different purposes. Unfortunately, these filters are usually both numerous and non-separable, making their use computationally expensive. In this paper, we show that such filters can be computed as linear combinations of a smaller number of separable ones, thus greatly reducing the computational complexity at no cost in terms of performance. This makes filter learning approaches practical even for large images or 3D volumes, and we show that we significantly outperform state-of-the-art methods on the curvilinear structure extraction task, in terms of both accuracy and speed. Moreover, our approach is general and can be used on generic convolutional filter banks to reduce the complexity of the feature extraction step. © 2014 IEEE.","Convolutional neural networks; Convolutional sparse coding; Features extraction; Filter learning; Image denoising; Segmentation of linear structures; Separable convolution; Tensor decomposition"
"Houle M.E., Nett M.","Rank-based similarity search: Reducing the dimensional dependence",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","1", 6866199,"136","150",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916881667&doi=10.1109%2fTPAMI.2014.2343223&partnerID=40&md5=fe86b34ee7c4284df2ff8d64c3abb45b","This paper introduces a data structure for k-NN search, the Rank Cover Tree (RCT), whose pruning tests rely solely on the comparison of similarity values; other properties of the underlying space, such as the triangle inequality, are not employed. Objects are selected according to their ranks with respect to the query object, allowing much tighter control on the overall execution costs. A formal theoretical analysis shows that with very high probability, the RCT returns a correct query result in time that depends very competitively on a measure of the intrinsic dimensionality of the data set. The experimental results for the RCT show that non-metric pruning strategies for similarity search can be practical even when the representational dimension of the data is extremely high. They also show that the RCT is capable of meeting or exceeding the level of performance of state-of-the-art methods that make use of metric pruning or other selection tests involving numerical constraints on distance values. © 2014 IEEE.","Intrinsic dimensionality; Nearest neighbor search; Rank-based search"
"Zhang X.-L.","Convex discriminative multitask clustering",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","1", 6866178,"28","40",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916931592&doi=10.1109%2fTPAMI.2014.2343221&partnerID=40&md5=bae38bdf70838ff7c33120c2c85a33d4","Multitask clustering tries to improve the clustering performance of multiple tasks simultaneously by taking their relationship into account. Most existing multitask clustering algorithms fall into the type of generative clustering, and none are formulated as convex optimization problems. In this paper, we propose two convex Discriminative Multitask Clustering (DMTC) objectives to address the problems. The first one aims to learn a shared feature representation, which can be seen as a technical combination of the convex multitask feature learning and the convex Multiclass Maximum Margin Clustering (M3C). The second one aims to learn the task relationship, which can be seen as a combination of the convex multitask relationship learning and M3C. The objectives of the two algorithms are solved in a uniform procedure by the efficient cutting-plane algorithm and further unified in the Bayesian framework. Experimental results on a toy problem and two benchmark data sets demonstrate the effectiveness of the proposed algorithms. © 2014 IEEE.","Convex optimization; Cutting-plane algorithm; Discriminative clustering; Unsupervised multitask learning"
"Johnsson K., Soneson C., Fontes M.","Low bias local intrinsic dimension estimation from expected simplex skewness",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","1", 6866171,"196","202",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916930372&doi=10.1109%2fTPAMI.2014.2343220&partnerID=40&md5=6d3cbd1f25957b5489d7ecb8bd52518a","In exploratory high-dimensional data analysis, local intrinsic dimension estimation can sometimes be used in order to discriminate between data sets sampled from different low-dimensional structures. Global intrinsic dimension estimators can in many cases be adapted to local estimation, but this leads to problems with high negative bias or high variance. We introduce a method that exploits the curse/blessing of dimensionality and produces local intrinsic dimension estimators that have very low bias, even in cases where the intrinsic dimension is higher than the number of data points, in combination with relatively low variance. We show that our estimators have a very good ability to classify local data sets by their dimension compared to other local intrinsic dimension estimators; furthermore we provide examples showing the usefulness of local intrinsic dimension estimation in general and our method in particular for stratification of real data sets. © 2014 IEEE.","Intrinsic dimension estimation; Manifold learning"
"Li Y.-F., Zhou Z.-H.","Towards making unlabeled data never hurt",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","1", 6710159,"175","188",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916917798&doi=10.1109%2fTPAMI.2014.2299812&partnerID=40&md5=5a59b55f22913c80201b4c596126712e","It is usually expected that learning performance can be improved by exploiting unlabeled data, particularly when the number of labeled data is limited. However, it has been reported that, in some cases existing semi-supervised learning approaches perform even worse than supervised ones which only use labeled data. For this reason, it is desirable to develop safe semi-supervised learning approaches that will not significantly reduce learning performance when unlabeled data are used. This paper focuses on improving the safeness of semi-supervised support vector machines (S3VMs). First, the S3VM-us approach is proposed. It employs a conservative strategy and uses only the unlabeled instances that are very likely to be helpful, while avoiding the use of highly risky ones. This approach improves safeness but its performance improvement using unlabeled data is often much smaller than S3VMs. In order to develop a safe and well-performing approach, we examine the fundamental assumption of S3VMs, i.e., low-density separation. Based on the observation that multiple good candidate low-density separators may be identified from training data, safe semi-supervised support vector machines (S4VMs) are here proposed. This approach uses multiple low-density separators to approximate the ground-truth decision boundary and maximizes the improvement in performance of inductive SVMs for any candidate separator. Under the assumption employed by S3VMs, it is here shown that S4VMs are provably safe and that the performance improvement using unlabeled data can be maximized. An out-of-sample extension of S4VMs is also presented. This extension allows S4VMs to make predictions on unseen instances. Our empirical study on a broad range of data shows that the overall performance of S4VMs is highly competitive with S3VMs, whereas in contrast to S3VMs which hurt performance significantly in many cases, S4VMs rarely perform worse than inductive SVMs. © 2014 IEEE.","S3VMs; S4VMs; Safe; Semi-supervised learning; Unlabeled data"
"Shi Q., Reid M., Caetano T., Van Den Hengel A., Wang Z.","A hybrid loss for multiclass and structured prediction",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","1", 6740814,"2","12",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916910915&doi=10.1109%2fTPAMI.2014.2306414&partnerID=40&md5=8b95f22289f30233d708a22627d40113","We propose a novel hybrid loss for multiclass and structured prediction problems that is a convex combination of a log loss for Conditional Random Fields (CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs). We provide a sufficient condition for when the hybrid loss is Fisher consistent for classification. This condition depends on a measure of dominance between labels - specifically, the gap between the probabilities of the best label and the second best label. We also prove Fisher consistency is necessary for parametric consistency when learning models such as CRFs. We demonstrate empirically that the hybrid loss typically performs least as well as - and often better than - both of its constituent losses on a variety of tasks, such as human action recognition. In doing so we also provide an empirical comparison of the efficacy of probabilistic and margin based approaches to multiclass and structured prediction. © 2014 IEEE.","Conditional random fields; Fisher consistency; Hybrid loss; Structured learning; Support vector machines"
"Xiao M., Guo Y.","Feature space independent semi-supervised domain adaptation via kernel matching",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","1", 6866177,"54","66",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916884703&doi=10.1109%2fTPAMI.2014.2343216&partnerID=40&md5=e3af67a7763d9dde8eccbefcbfc9f740","Domain adaptation methods aim to learn a good prediction model in a label-scarce target domain by leveraging labeled patterns from a related source domain where there is a large amount of labeled data. However, in many practical domain adaptation learning scenarios, the feature distribution in the source domain is different from that in the target domain. In the extreme, the two distributions could differ completely when the feature representation of the source domain is totally different from that of the target domain. To address the problems of substantial feature distribution divergence across domains and heterogeneous feature representations of different domains, we propose a novel feature space independent semi-supervised kernel matching method for domain adaptation in this work. Our approach learns a prediction function on the labeled source data while mapping the target data points to similar source data points by matching the target kernel matrix to a submatrix of the source kernel matrix based on a Hilbert Schmidt Independence Criterion. We formulate this simultaneous learning and mapping process as a non-convex integer optimization problem and present a local minimization procedure for its relaxed continuous form. We evaluate the proposed kernel matching method using both cross domain sentiment classification tasks of Amazon product reviews and cross language text classification tasks of Reuters multilingual newswire stories. Our empirical results demonstrate that the proposed kernel matching method consistently and significantly outperforms comparison methods on both cross domain classification problems with homogeneous feature spaces and cross domain classification problems with heterogeneous feature spaces. © 2014 IEEE.","Domain adaptation; Heterogeneous feature spaces; Kernel matching"
"Cabral R., De La Torre F., Costeira J.P., Bernardino A.","Matrix completion for weakly-supervised multi-label image classification",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","1", 6866218,"121","135",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916877392&doi=10.1109%2fTPAMI.2014.2343234&partnerID=40&md5=2d16da928e0127fbf66073fc4f711187","In the last few years, image classification has become an incredibly active research topic, with widespread applications. Most methods for visual recognition are fully supervised, as they make use of bounding boxes or pixelwise segmentations to locate objects of interest. However, this type of manual labeling is time consuming, error prone and it has been shown that manual segmentations are not necessarily the optimal spatial enclosure for object classifiers. This paper proposes a weakly-supervised system for multi-label image classification. In this setting, training images are annotated with a set of keywords describing their contents, but the visual concepts are not explicitly segmented in the images. We formulate the weakly-supervised image classification as a low-rank matrix completion problem. Compared to previous work, our proposed framework has three advantages: (1) Unlike existing solutions based on multiple-instance learning methods, our model is convex. We propose two alternative algorithms for matrix completion specifically tailored to visual data, and prove their convergence. (2) Unlike existing discriminative methods, our algorithm is robust to labeling errors, background noise and partial occlusions. (3) Our method can potentially be used for semantic segmentation. Experimental validation on several data sets shows that our method outperforms state-of-the-art classification algorithms, while effectively capturing each class appearance. © 2014 IEEE.","Multi-label image classification; Nuclear norm; Rank minimization; Segmentation; Weakly-supervised learning"
"Hong B.-W., Soatto S.","Shape matching using multiscale integral invariants",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","1", 6862873,"151","160",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916887228&doi=10.1109%2fTPAMI.2014.2342215&partnerID=40&md5=784767f9fa02de9066f912dd25dfd31d","We present a shape descriptor based on integral kernels. Shape is represented in an implicit formand it is characterized by a series of isotropic kernels that provide desirable invariance properties. The shape features are characterized at multiple scales which form a signature that is a compact description of shape over a range of scales. The shape signature is designed to be invariant with respect to group transformations which include translation, rotation, scaling, and reflection. In addition, the integral kernels that characterize local shape geometry enable the shape signature to be robust with respect to undesirable perturbations while retaining discriminative power. Use of our shape signature is demonstrated for shape matching based on a number of synthetic and real examples. © 2014 IEEE.","Integral invariant; Scale invariant; Shape descriptor; Shape matching; Wasserstein distance"
"Žitnik M., Zupan B.","Data fusion by matrix factorization",2015,"IEEE Transactions on Pattern Analysis and Machine Intelligence","37","1", 6867358,"41","53",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916887227&doi=10.1109%2fTPAMI.2014.2343973&partnerID=40&md5=50dcc26481ed0f3fe85d02d881282d9c","For most problems in science and engineering we can obtain data sets that describe the observed system from various perspectives and record the behavior of its individual components. Heterogeneous data sets can be collectively mined by data fusion. Fusion can focus on a specific target relation and exploit directly associated data together with contextual data and data about system's constraints. In the paper we describe a data fusion approach with penalized matrix tri-factorization (DFMF) that simultaneously factorizes data matrices to reveal hidden associations. The approach can directly consider any data that can be expressed in a matrix, including those from feature-based representations, ontologies, associations and networks. We demonstrate the utility of DFMF for gene function prediction task with eleven different data sources and for prediction of pharmacologic actions by fusing six data sources. Our data fusion algorithm compares favorably to alternative data integration approaches and achieves higher accuracy than can be obtained from any single data source alone. © 2014 IEEE.","Bioinformatics; Cheminformatics; Data fusion; Data mining; Intermediate data integration; Matrix factorization"
